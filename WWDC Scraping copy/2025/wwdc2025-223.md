# Wwdc2025 223

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Summary

Transcript

Code

Explore enhancements to your spatial business appDiscover how the latest enhancements and APIs in visionOS 26 expand access and extend enterprise capabilities announced last year. Learn how these all-new features make it easy to build model training workflows, enhance video feeds, and enable you to align coordinate systems over a local network to develop collaborative experiences in your in-house app.Chapters0:04 -Introduction1:37 -Streamline development4:54 -Enhance user experience13:37 -Visualize the environment24:01 -Next stepsResourcesBuilding spatial experiences for business apps with enterprise APIs for visionOSImplementing object tracking in your visionOS appHD VideoSD VideoRelated VideosWWDC25Share visionOS experiences with nearby peopleWWDC24Create enhanced spatial computing experiences with ARKitExplore object tracking for visionOSIntroducing enterprise APIs for visionOS

Discover how the latest enhancements and APIs in visionOS 26 expand access and extend enterprise capabilities announced last year. Learn how these all-new features make it easy to build model training workflows, enhance video feeds, and enable you to align coordinate systems over a local network to develop collaborative experiences in your in-house app.

0:04 -Introduction

1:37 -Streamline development

4:54 -Enhance user experience

13:37 -Visualize the environment

24:01 -Next steps

Building spatial experiences for business apps with enterprise APIs for visionOS

Implementing object tracking in your visionOS app

HD VideoSD Video

HD Video

SD Video

Share visionOS experiences with nearby people

Create enhanced spatial computing experiences with ARKit

Explore object tracking for visionOS

Introducing enterprise APIs for visionOS

Search this video…Hi, my name is Alex Powers,and I’m an engineer on the Enterprise team for visionOS.It’s great to be back at WWDC.Last year, we introduced the first set of Enterprise APIs for visionOS.Since then, we’ve been working hardto bring you even more enterprise capabilities.Before exploring the new features,let me review the fundamental requirements for the Enterprise APIs.Because they offer broad utility and deeper device access,accessing these APIs requires a managed entitlementalong with a license file tied to your developer account.And designed for proprietary, in-house apps,developed by your organization for your employees,or for custom apps you build for another business to distribute internally.With those considerations in mind,I’m excited to take you through the new Enterprise APIsand some major improvements to the existing APIs.I’ll start with changes that will streamline your developmentand make it easier for you to access enterprise capabilities.Next, I’ll show you a way to enhance the user experienceby providing new ways to interact with windows,share content with people nearby, and protect sensitive information.And finally, I'll explore new capabilities to visualize your environment.Let me begin by showing you some wayswe’re making it easier to access enterprise capabilitiesand streamline your development.Starting with wider API access,we’ve made some changes this year to give you wider accessto several APIs we introduced last year.We previously introduced the abilityfor an app to access external video from USB Video Class devicesthrough the Vision Pro Developer Strap.The API allows your app to leverageUVC-compatible webcams for enhanced video conferencing,specialized imaging devices for remote diagnostics,or industrial inspection cameras for quality control.We’ve also made access to Neural Engine availablefor advanced on-device machine learning.I’m happy to say that with the latest visionOS,these APIs are now available for all developers.You’ll be able to access UVC video and Neural Enginewithout an enterprise license or an entitlement.Last year, we introduced object tracking for visionOS,enabling powerful experiences where your app can recognizeand track specific real world objects.This year, we're adding the ability to train directly from the command line.This means you can now automate the model training process,integrate it into your existing pipelines,and manage your object tracking assets more efficiently,all without needing to manually use the CreateML appfor each individual object.This tool gives you all the same controls as the CreateML app.I hope this will unlock new workflowsand make iterating on your object tracking features faster and more scalable.We're also making enterprise license management simpler.You can now access your license filesdirectly within your Apple Developer account.Renewals are automatically pushed to your apps over the air,and we’ve created the Vision Entitlement Services framework.This framework makes it straightforward to check if your applicationis properly licensed and approved for specific features.Using Vision Entitlement Services,you can determine if your app can access specific enterprise API,like main camera access.See your license status and its expiration date,and for apps using the Increased Performance Headroom entitlement,you can verify this move before intensive tasksto ensure the best performance.As an example, let me show youhow you would determine if your app is configured properlyto access the main camera.First, import the framework.Then use the shared Enterprise License Details singletonto first confirm that the license is valid,and then confirm that the license is approved for mainCameraAccess.So that’s how the latest visionOS expands API accessand makes developing models and managing your enterprise apps easier.Now let me walk through some new ways for enhancing user experiencesby building more intuitive, collaborative, and secure spatial applications.First, we’re introducing a way to make window interactionsin spatial environments more natural,especially when moving while wearing Vision Pro.We call it Window Follow Mode.When enabled, it ensures content remains accessibleand relative to your position.To enable this behavior, you need the window-body-follow entitlement.This entitlement is requested and managed as a licensed entitlement.Once the entitlement is granted and included in your app,this behavior will be enabled for any window on the device in all applications.Standard windows in visionOS remain fixed in space where you place them.But imagine you have a dashboard,a set of instructions, or reference materialthat you need to glance at frequentlywhile performing a task that requires you to move.Window Follow Mode allows you to choose a windowand have it move with you as you move from place to place.Let's see Window Follow Mode in action.Here I am focused on a project at my workbench.At the same time, my manipulator arm is executing a task.I want to monitor the manipulator status,but without constantly interrupting my main task.To enable Window Follow Mode for the status window,I click and hold the window close control. I choose Start Follow Mode.And there we go.The status window will follow me as I move back to my work area.So that’s Window Follow Mode.One great way to enhance your user experience.But spatial computing is truly at its bestwhen enabling shared, collaborative experiences.And that’s precisely what shared coordinate spaces enable.This feature allows people who are physically togetherto share their spatial experiences with each other.Everyone can naturally interact with and discussthe app’s content as if it were physically present.We provide high-level APIs using SharePlay that automatically handlethe discovery, connection, and session managementfor shared coordinate spaces.We have a whole session on this called“Share visionOS experiences with nearby people.”While SharePlay offers fantastic ease of use out of the box,we understand that some scenarios demand more control.You might need to integrate with your own custom networking infrastructure.Or maybe your enterprise requirementsmean you have to handle device communication directly.For these use cases, we’re introducing a new ARKit APIfor establishing shared coordinate spaces specificallyfor enterprise customers.It’s called the SharedCoordinateSpaceProvider.This API allows multiple participants to align their coordinate systems.This is achieved by exchanging specific ARKit-generated dataover your chosen local network transport.Each participant continuously shares this data with the others.This continuous sharing creates a common coordinate system,enabling shared world anchors to appear consistently for everyone.With that, let me run through how you would use this APIto build a custom shared experience.Using SharedCoordinateSpaceProvideris straightforward if you worked with ARKit data providers before.Similar to World Tracking or Hand Tracking,you instantiate it and run it on your active ARKitSession.Once running, the SharedCoordinateSpaceProvidergenerates the necessary alignment informationencapsulated in CoordinateSpaceData objects.You retrieve this data using a pull-based API,the provider’s nextCoordinateSpaceData() function.My application is responsible for transmitting this CoordinateSpaceDatato the other participants to establish a shared coordinate space.This gives you full control.You can use any networking layer that you want.Conversely, when your app receives CoordinateSpaceDatafrom another participant over the network,you provide it to the local SharedCoordinateSpaceProviderby calling its push() method.Each piece of incoming data is tagged with the sender’s unique participantID.Finally, the provider helps you manage the session lifecycle.It offers an eventUpdates async sequence to inform you about important changes,such as when a participant has left the shared space.Let me walk through an example of how this works in code.I start by creating a SharedCoordinateSpaceProviderand running it on my ARKitSession.When data arrives from another participant on my network,I update the local provider's understanding using push data.To get the data my device needs to share,I call the nextCoordinateSpaceData() function.This gives me the CoordinateSpaceData object representing my local state,ready to be broadcast over my network.Finally, this logic forms the heart of my custom shared space management,bridging my networking layer with ARKit’s coordinate alignment.So that’s ARKit’s Shared Coordinate API for enterprise developers,a great way to add collaboration to in-house apps.My final user experience enhancement is all about data privacy and security.Many enterprise apps handle sensitive information, financial data,patient records, proprietary designs or confidential communications.And while incredibly useful, capabilities such as SharePlay,screen captures and recordings,or even Screen Mirroring can inadvertently expose this sensitive data.So today, there’s a new API that gives you controlover what can get captured and shared with others.And it’s the new contentCaptureProtected view modifier for SwiftUI.It’s supported in apps with the protected content entitlement.You simply add it to any user interface elementor even entire RealityKit scenes.When content is marked as protected,the system will automatically obscure it in any screen captures,recordings, mirrored or shared views.However, the content remains perfectly visible to the user wearing the device.Here’s an example of a common enterprise use case.I have an app that serves as a central repositoryfor my company documents, accessible to all employees.However, certain documents within the systemcontain sensitive information and shouldn’t be shared widely.I’m sharing these documents with my team in the other office.Here, the team can see our meeting notes and the plan for next year.Both of these documents are visible to me and shared with the team.Here, you can see the quarterly report has a lock icon.This report shouldn’t be sharedand so my team can’t see it on the remote display.Now that you’ve seen protected content in action,let's see how to implement it.In this example, I have a document view that contains a child viewthat I’ve called SensitiveDataView.It has information that needs to be seen only on Vision Pro.To protect it, I append the view modifier, contentCaptureProtected, and I’m done.The system will now obscure the feedwhenever any attempt is made to share this content.You can also integrate this content protection with authentication flowslike Optic ID or corporate single sign-on.So that’s how to protect app 2D and 3D content.It can be protected with the same simple modifier.Those features enhance the experience within the digital space.Now, I’ll focus on some features designed to help visualize the environmentand bridge the physical and digital worlds.First, we’re expanding camera access on Vision Pro.Vision Pro uses its sophisticated camera systemto capture the wearer’s environmentwith the forward cameras providing the passthrough experience.Last year, we shipped an API to provide access tothe device's left main camera video feed.And this year, we’ve expanded the API to provide direct accessto the individual left or right cameras,or access both for stereo processing and analysis.If you’re already familiar,it’s the CameraFrameProvider API in ARKit.And now, camera feed support is available inboth the Immersive Space and Shared Space environments,allowing your app to function alongside other apps and windows.So that’s how the latest visionOS makes camera access even more flexible.Now let me show you a new way to visualize details in your surroundings.Professionals often need to monitor specific details in their work area.For example, technicians need to read small gauges on complex machinery,or inspectors might need to examine components in poorly lit areas.To address this, we’re introducing a powerful new featurethat allows people wearing Vision Proto select a specific area in their real-world viewand provide a dedicated video feed of that area in its own window.This feed can be magnified or enhanced, making critical details clear.There’s a new SwiftUI view in VisionKit called CameraRegionView.You simply position this window visually over the area you want to enhance.Then, the CameraRegionView uses its own positionto provide the appropriate region and space for the virtual camera.If you require more fine-grained control,you can use the new CameraRegionProvider API in ARKit.This gives you direct access and is useful if you’re already using ARKit,familiar with anchors, or have more specific UI needs.Here’s a demo of how it works using an example status app that I’ve created.Here, you can see that I’m back with my project.This time, I’d like to monitor the pressure in the system while I work.I’ll open the inspector window of my status appand place it in front of the gauge.As you can see, the video feed of the gauge has appeared in my status app.Now I can return to work and keep an eye on the pressure while I work.Now let me show you how I added a Camera Region to my appin just a few lines of code using SwiftUI and the VisionKit API.First, I import VisionKit.I define a standard SwiftUI view.I’ve called it InspectorView.This will contain the camera region.The core of this view is CameraRegionView.I’m initializing it withthe IsContrastAndVibrancyEnhancementEnabled parameter,passing true to enable stabilization with contrast and vibrancy enhancement.As I mentioned, this view needs to live in its own windowbecause it uses the window's positionto determine what part of the passthrough is processed.For that, let’s look at the App struct.Here’s my app struct.I have a main WindowGroup for my primary app content.I’ll make a second WindowGroup for the InspectorView.That’s enough to add a camera region to my app.But for more complex applications, CameraRegionView supports a closure.So I’m going to change my code to use this closureto analyze the camera images, and later,I may add a feature to save the images to a file.I’ll modify the CameraRegionView to accept a closure,allowing me to process each camera frame as it arrives.First, I add my cameraFeedDelivery class that I’ve made to capture camera framesand deliver them to the rest of my app.My closure will use the pixelBuffer from the CameraRegionView.Here, I’ll check for errors and pass the pixelBufferto my cameraFeedDelivery class.My closure returns nil, which indicatesthat I’ve not modified the pixelBuffer.I could also use this closure for custom processing.If I modify the pixelBuffer and return it,then the CameraRegionView would render the adjusted camera image.So with just a few lines of code, I’ve added camera regions to my app.In my example, I enabled contrast and vibrancy enhancement.But the Camera Region APIs provides two built-in processing capabilities.First is image stabilization.This ensures that content remains anchored and stableduring natural head movements.And second is contrast and vibrancy enhancement,which includes stabilization and optimizes for brightnessand color representation.Now let’s look at ARKit’s API for camera regions.Perhaps your application would like a camera region associatedwith a particular 3D object.Or you'd like to place a camera regionafter recognizing a specific object in the environment.If your application needs this level of fine-grained controlover anchors and 3D objects, this API provides the low-level primitives,and you'll need to define the anchors.In ARKit, your anchor defines a virtual windowinto the real world by specifying its transformand physical size in meters.This window defines an area wherewhere you’ll see the direct, stabilized view of the passthrough camera feed.You can think of it like placing a virtual cameraright there in your physical space.This virtual camera doesn’t need to be attached to a visionOS window.It can produce a feed of any location within view of Vision Pro’s cameras.Now let's take a closer look at the API.ARKit offers a new type of data provider called the CameraRegionProvider.Integrating camera regions follows a familiar ARKit pattern.I start by running a data provider on my ARKitSession,just like I would for other ARKit features.With the provider up and running,my next step is to pinpoint the area for a camera region.I do this by creating a CameraRegionAnchor and adding it to my provider.Think of these anchors as specifying the exact regionsin the real world that I want for the virtual camera.As ARKit runs, the provider sends updates to these anchors.Each update comes with a new pixelBuffer.This buffer contains the stabilized view for that specific spatial region.So let’s dive into how I create one of these anchors.Creating a CameraRegionAnchor is straightforward.I define its position and orientation in the worldusing a standard 6-degree-of-freedom transform.Then I specify its physical size, its width, and height in meters.Together, these parameters define the real-world windowfor the camera region.I also need to tell ARKit if I want the windowcontrast enhanced or just stabilized.Then I add it to the CameraRegionProvider.After adding the anchor, I call anchorUpdates(forID:)and pass the anchor ID of the newAnchor.The camera feed now appears exactly at the location specified by the anchor,and my code can handle the pixelBuffers provided with each update.So that’s Camera Regions in ARKit,an incredibly useful tool for keeping track of specific areasin your environment.But before I leave the topic,there are some points I’d like you to keep in mind.The pass-through content in the CameraRegionView,like any SwiftUI view, can be zoomed or panned using standard techniques.If you implement these transformations,ensure they are also applied to any camera framesyou save or transmit remotely.It’s important to understand that the enhancement algorithmdynamically adjusts its frame rate to deliver the best possible image quality.Choosing stabilization over contrast enhancementwill result in a higher frame rate,as stabilization requires less processing power.And while Camera Regions in ARKit are powerful and allow regions of any size,it’s important to be mindful of resource usage.Larger camera regions will naturally have a greater impact on memory and processing.And finally, I strongly recommend you evaluate your overall resource useas you design your experience.Particularly when working with large enhanced regions.As a guideline, aim for CameraRegionAnchorsto display passthrough content using about one-sixthor less of the overall visible area.So those are the topics designed to bridge your physical and digital worlds,and the last of a long list of enterprise-ready enhancementswe’ve added this year.From making core functionality like UVC accessand object tracking more flexible, to introducing Window Follow Mode,App-Protected Content, and Camera Regions.I'm sure you'll find a myriad of waysto put these new capabilities to work in your app.And with that, let me wrap up with some final guidance.First, be mindful of environmental safety.Ensure users are in a suitable locationto perform tasks safely while wearing Vision Pro,especially when interacting with real-world equipment.Remember that with enhanced access,particularly to cameras and sensors, comes increased responsibility.Be transparent with users about what data is being accessed and why.Design your applications to collect only the necessary informationfor the task at hand, respecting user privacy in the workplace.Ensure your application and use case meet the eligibility requirements.These are intended for proprietary in-house apps developed for your own employees,or for custom B2B apps built for another business and distributed privately.And with those items confirmed,if eligible, only request the enterprise entitlementsyou genuinely need for your application’s specific functionality.And finally, please share your feedback with us.We rely on your input not only regarding these specific APIs,but also about the future capabilitiesyou need to build amazing enterprise applications on visionOS.Thank you for watching and have a great WWDC!

Hi, my name is Alex Powers,and I’m an engineer on the Enterprise team for visionOS.It’s great to be back at WWDC.

Last year, we introduced the first set of Enterprise APIs for visionOS.Since then, we’ve been working hardto bring you even more enterprise capabilities.

Before exploring the new features,let me review the fundamental requirements for the Enterprise APIs.

Because they offer broad utility and deeper device access,accessing these APIs requires a managed entitlementalong with a license file tied to your developer account.And designed for proprietary, in-house apps,developed by your organization for your employees,or for custom apps you build for another business to distribute internally.

With those considerations in mind,I’m excited to take you through the new Enterprise APIsand some major improvements to the existing APIs.

I’ll start with changes that will streamline your developmentand make it easier for you to access enterprise capabilities.Next, I’ll show you a way to enhance the user experienceby providing new ways to interact with windows,share content with people nearby, and protect sensitive information.And finally, I'll explore new capabilities to visualize your environment.

Let me begin by showing you some wayswe’re making it easier to access enterprise capabilitiesand streamline your development.

Starting with wider API access,we’ve made some changes this year to give you wider accessto several APIs we introduced last year.

We previously introduced the abilityfor an app to access external video from USB Video Class devicesthrough the Vision Pro Developer Strap.The API allows your app to leverageUVC-compatible webcams for enhanced video conferencing,specialized imaging devices for remote diagnostics,or industrial inspection cameras for quality control.

We’ve also made access to Neural Engine availablefor advanced on-device machine learning.I’m happy to say that with the latest visionOS,these APIs are now available for all developers.You’ll be able to access UVC video and Neural Enginewithout an enterprise license or an entitlement.

Last year, we introduced object tracking for visionOS,enabling powerful experiences where your app can recognizeand track specific real world objects.This year, we're adding the ability to train directly from the command line.

This means you can now automate the model training process,integrate it into your existing pipelines,and manage your object tracking assets more efficiently,all without needing to manually use the CreateML appfor each individual object.This tool gives you all the same controls as the CreateML app.I hope this will unlock new workflowsand make iterating on your object tracking features faster and more scalable.

We're also making enterprise license management simpler.

You can now access your license filesdirectly within your Apple Developer account.Renewals are automatically pushed to your apps over the air,and we’ve created the Vision Entitlement Services framework.This framework makes it straightforward to check if your applicationis properly licensed and approved for specific features.

Using Vision Entitlement Services,you can determine if your app can access specific enterprise API,like main camera access.See your license status and its expiration date,and for apps using the Increased Performance Headroom entitlement,you can verify this move before intensive tasksto ensure the best performance.

As an example, let me show youhow you would determine if your app is configured properlyto access the main camera.

First, import the framework.Then use the shared Enterprise License Details singletonto first confirm that the license is valid,and then confirm that the license is approved for mainCameraAccess.

So that’s how the latest visionOS expands API accessand makes developing models and managing your enterprise apps easier.

Now let me walk through some new ways for enhancing user experiencesby building more intuitive, collaborative, and secure spatial applications.First, we’re introducing a way to make window interactionsin spatial environments more natural,especially when moving while wearing Vision Pro.

We call it Window Follow Mode.

When enabled, it ensures content remains accessibleand relative to your position.To enable this behavior, you need the window-body-follow entitlement.This entitlement is requested and managed as a licensed entitlement.

Once the entitlement is granted and included in your app,this behavior will be enabled for any window on the device in all applications.

Standard windows in visionOS remain fixed in space where you place them.But imagine you have a dashboard,a set of instructions, or reference materialthat you need to glance at frequentlywhile performing a task that requires you to move.

Window Follow Mode allows you to choose a windowand have it move with you as you move from place to place.

Let's see Window Follow Mode in action.

Here I am focused on a project at my workbench.At the same time, my manipulator arm is executing a task.I want to monitor the manipulator status,but without constantly interrupting my main task.To enable Window Follow Mode for the status window,I click and hold the window close control. I choose Start Follow Mode.

And there we go.The status window will follow me as I move back to my work area.

So that’s Window Follow Mode.One great way to enhance your user experience.But spatial computing is truly at its bestwhen enabling shared, collaborative experiences.

And that’s precisely what shared coordinate spaces enable.This feature allows people who are physically togetherto share their spatial experiences with each other.

Everyone can naturally interact with and discussthe app’s content as if it were physically present.We provide high-level APIs using SharePlay that automatically handlethe discovery, connection, and session managementfor shared coordinate spaces.

We have a whole session on this called“Share visionOS experiences with nearby people.”While SharePlay offers fantastic ease of use out of the box,we understand that some scenarios demand more control.You might need to integrate with your own custom networking infrastructure.Or maybe your enterprise requirementsmean you have to handle device communication directly.

For these use cases, we’re introducing a new ARKit APIfor establishing shared coordinate spaces specificallyfor enterprise customers.It’s called the SharedCoordinateSpaceProvider.This API allows multiple participants to align their coordinate systems.This is achieved by exchanging specific ARKit-generated dataover your chosen local network transport.Each participant continuously shares this data with the others.This continuous sharing creates a common coordinate system,enabling shared world anchors to appear consistently for everyone.

With that, let me run through how you would use this APIto build a custom shared experience.

Using SharedCoordinateSpaceProvideris straightforward if you worked with ARKit data providers before.

Similar to World Tracking or Hand Tracking,you instantiate it and run it on your active ARKitSession.Once running, the SharedCoordinateSpaceProvidergenerates the necessary alignment informationencapsulated in CoordinateSpaceData objects.You retrieve this data using a pull-based API,the provider’s nextCoordinateSpaceData() function.

My application is responsible for transmitting this CoordinateSpaceDatato the other participants to establish a shared coordinate space.This gives you full control.You can use any networking layer that you want.

Conversely, when your app receives CoordinateSpaceDatafrom another participant over the network,you provide it to the local SharedCoordinateSpaceProviderby calling its push() method.Each piece of incoming data is tagged with the sender’s unique participantID.Finally, the provider helps you manage the session lifecycle.It offers an eventUpdates async sequence to inform you about important changes,such as when a participant has left the shared space.

Let me walk through an example of how this works in code.

I start by creating a SharedCoordinateSpaceProviderand running it on my ARKitSession.When data arrives from another participant on my network,I update the local provider's understanding using push data.

To get the data my device needs to share,I call the nextCoordinateSpaceData() function.This gives me the CoordinateSpaceData object representing my local state,ready to be broadcast over my network.

Finally, this logic forms the heart of my custom shared space management,bridging my networking layer with ARKit’s coordinate alignment.

So that’s ARKit’s Shared Coordinate API for enterprise developers,a great way to add collaboration to in-house apps.

My final user experience enhancement is all about data privacy and security.Many enterprise apps handle sensitive information, financial data,patient records, proprietary designs or confidential communications.And while incredibly useful, capabilities such as SharePlay,screen captures and recordings,or even Screen Mirroring can inadvertently expose this sensitive data.

So today, there’s a new API that gives you controlover what can get captured and shared with others.

And it’s the new contentCaptureProtected view modifier for SwiftUI.It’s supported in apps with the protected content entitlement.You simply add it to any user interface elementor even entire RealityKit scenes.

When content is marked as protected,the system will automatically obscure it in any screen captures,recordings, mirrored or shared views.However, the content remains perfectly visible to the user wearing the device.Here’s an example of a common enterprise use case.

I have an app that serves as a central repositoryfor my company documents, accessible to all employees.However, certain documents within the systemcontain sensitive information and shouldn’t be shared widely.

I’m sharing these documents with my team in the other office.Here, the team can see our meeting notes and the plan for next year.Both of these documents are visible to me and shared with the team.Here, you can see the quarterly report has a lock icon.

This report shouldn’t be sharedand so my team can’t see it on the remote display.

Now that you’ve seen protected content in action,let's see how to implement it.

In this example, I have a document view that contains a child viewthat I’ve called SensitiveDataView.It has information that needs to be seen only on Vision Pro.To protect it, I append the view modifier, contentCaptureProtected, and I’m done.The system will now obscure the feedwhenever any attempt is made to share this content.You can also integrate this content protection with authentication flowslike Optic ID or corporate single sign-on.

So that’s how to protect app 2D and 3D content.It can be protected with the same simple modifier.

Those features enhance the experience within the digital space.Now, I’ll focus on some features designed to help visualize the environmentand bridge the physical and digital worlds.

First, we’re expanding camera access on Vision Pro.

Vision Pro uses its sophisticated camera systemto capture the wearer’s environmentwith the forward cameras providing the passthrough experience.

Last year, we shipped an API to provide access tothe device's left main camera video feed.And this year, we’ve expanded the API to provide direct accessto the individual left or right cameras,or access both for stereo processing and analysis.If you’re already familiar,it’s the CameraFrameProvider API in ARKit.

And now, camera feed support is available inboth the Immersive Space and Shared Space environments,allowing your app to function alongside other apps and windows.

So that’s how the latest visionOS makes camera access even more flexible.

Now let me show you a new way to visualize details in your surroundings.

Professionals often need to monitor specific details in their work area.For example, technicians need to read small gauges on complex machinery,or inspectors might need to examine components in poorly lit areas.

To address this, we’re introducing a powerful new featurethat allows people wearing Vision Proto select a specific area in their real-world viewand provide a dedicated video feed of that area in its own window.

This feed can be magnified or enhanced, making critical details clear.

There’s a new SwiftUI view in VisionKit called CameraRegionView.

You simply position this window visually over the area you want to enhance.Then, the CameraRegionView uses its own positionto provide the appropriate region and space for the virtual camera.

If you require more fine-grained control,you can use the new CameraRegionProvider API in ARKit.

This gives you direct access and is useful if you’re already using ARKit,familiar with anchors, or have more specific UI needs.

Here’s a demo of how it works using an example status app that I’ve created.

Here, you can see that I’m back with my project.This time, I’d like to monitor the pressure in the system while I work.

I’ll open the inspector window of my status appand place it in front of the gauge.

As you can see, the video feed of the gauge has appeared in my status app.Now I can return to work and keep an eye on the pressure while I work.

Now let me show you how I added a Camera Region to my appin just a few lines of code using SwiftUI and the VisionKit API.

First, I import VisionKit.

I define a standard SwiftUI view.I’ve called it InspectorView.This will contain the camera region.The core of this view is CameraRegionView.I’m initializing it withthe IsContrastAndVibrancyEnhancementEnabled parameter,passing true to enable stabilization with contrast and vibrancy enhancement.

As I mentioned, this view needs to live in its own windowbecause it uses the window's positionto determine what part of the passthrough is processed.For that, let’s look at the App struct.

Here’s my app struct.I have a main WindowGroup for my primary app content.I’ll make a second WindowGroup for the InspectorView.

That’s enough to add a camera region to my app.But for more complex applications, CameraRegionView supports a closure.So I’m going to change my code to use this closureto analyze the camera images, and later,I may add a feature to save the images to a file.

I’ll modify the CameraRegionView to accept a closure,allowing me to process each camera frame as it arrives.

First, I add my cameraFeedDelivery class that I’ve made to capture camera framesand deliver them to the rest of my app.

My closure will use the pixelBuffer from the CameraRegionView.

Here, I’ll check for errors and pass the pixelBufferto my cameraFeedDelivery class.My closure returns nil, which indicatesthat I’ve not modified the pixelBuffer.I could also use this closure for custom processing.If I modify the pixelBuffer and return it,then the CameraRegionView would render the adjusted camera image.

So with just a few lines of code, I’ve added camera regions to my app.In my example, I enabled contrast and vibrancy enhancement.But the Camera Region APIs provides two built-in processing capabilities.First is image stabilization.This ensures that content remains anchored and stableduring natural head movements.And second is contrast and vibrancy enhancement,which includes stabilization and optimizes for brightnessand color representation.

Now let’s look at ARKit’s API for camera regions.Perhaps your application would like a camera region associatedwith a particular 3D object.Or you'd like to place a camera regionafter recognizing a specific object in the environment.If your application needs this level of fine-grained controlover anchors and 3D objects, this API provides the low-level primitives,and you'll need to define the anchors.

In ARKit, your anchor defines a virtual windowinto the real world by specifying its transformand physical size in meters.This window defines an area wherewhere you’ll see the direct, stabilized view of the passthrough camera feed.

You can think of it like placing a virtual cameraright there in your physical space.This virtual camera doesn’t need to be attached to a visionOS window.It can produce a feed of any location within view of Vision Pro’s cameras.

Now let's take a closer look at the API.

ARKit offers a new type of data provider called the CameraRegionProvider.Integrating camera regions follows a familiar ARKit pattern.

I start by running a data provider on my ARKitSession,just like I would for other ARKit features.With the provider up and running,my next step is to pinpoint the area for a camera region.

I do this by creating a CameraRegionAnchor and adding it to my provider.Think of these anchors as specifying the exact regionsin the real world that I want for the virtual camera.As ARKit runs, the provider sends updates to these anchors.Each update comes with a new pixelBuffer.This buffer contains the stabilized view for that specific spatial region.

So let’s dive into how I create one of these anchors.

Creating a CameraRegionAnchor is straightforward.I define its position and orientation in the worldusing a standard 6-degree-of-freedom transform.Then I specify its physical size, its width, and height in meters.Together, these parameters define the real-world windowfor the camera region.I also need to tell ARKit if I want the windowcontrast enhanced or just stabilized.Then I add it to the CameraRegionProvider.

After adding the anchor, I call anchorUpdates(forID:)and pass the anchor ID of the newAnchor.The camera feed now appears exactly at the location specified by the anchor,and my code can handle the pixelBuffers provided with each update.

So that’s Camera Regions in ARKit,an incredibly useful tool for keeping track of specific areasin your environment.But before I leave the topic,there are some points I’d like you to keep in mind.

The pass-through content in the CameraRegionView,like any SwiftUI view, can be zoomed or panned using standard techniques.If you implement these transformations,ensure they are also applied to any camera framesyou save or transmit remotely.

It’s important to understand that the enhancement algorithmdynamically adjusts its frame rate to deliver the best possible image quality.Choosing stabilization over contrast enhancementwill result in a higher frame rate,as stabilization requires less processing power.

And while Camera Regions in ARKit are powerful and allow regions of any size,it’s important to be mindful of resource usage.Larger camera regions will naturally have a greater impact on memory and processing.

And finally, I strongly recommend you evaluate your overall resource useas you design your experience.Particularly when working with large enhanced regions.As a guideline, aim for CameraRegionAnchorsto display passthrough content using about one-sixthor less of the overall visible area.

So those are the topics designed to bridge your physical and digital worlds,and the last of a long list of enterprise-ready enhancementswe’ve added this year.From making core functionality like UVC accessand object tracking more flexible, to introducing Window Follow Mode,App-Protected Content, and Camera Regions.I'm sure you'll find a myriad of waysto put these new capabilities to work in your app.

And with that, let me wrap up with some final guidance.

First, be mindful of environmental safety.Ensure users are in a suitable locationto perform tasks safely while wearing Vision Pro,especially when interacting with real-world equipment.

Remember that with enhanced access,particularly to cameras and sensors, comes increased responsibility.Be transparent with users about what data is being accessed and why.Design your applications to collect only the necessary informationfor the task at hand, respecting user privacy in the workplace.

Ensure your application and use case meet the eligibility requirements.These are intended for proprietary in-house apps developed for your own employees,or for custom B2B apps built for another business and distributed privately.And with those items confirmed,if eligible, only request the enterprise entitlementsyou genuinely need for your application’s specific functionality.

And finally, please share your feedback with us.We rely on your input not only regarding these specific APIs,but also about the future capabilitiesyou need to build amazing enterprise applications on visionOS.

Thank you for watching and have a great WWDC!

3:00 -createml on the Mac command line

4:28 -VisionEntitlementServices

10:04 -SharedCoordinateSpaceModel

12:50 -contentCaptureProtected

16:48 -CameraRegionView

21:15 -CameraRegionAnchor

0:04 -IntroductionLast year, Apple introduced the Enterprise APIs, and this year, there are a host of new features and improvements. These APIs, designed for proprietary in-house apps, require managed entitlements and license files. The updates focus on streamlining development, enhancing user experience, and enabling new environment visualization capabilities.1:37 -Streamline developmentThe latest visionOS enhances developer capabilities by expanding API access. Key improvements include wider API availability, object-tracking enhancements, and simplified enterprise license management. 

Several APIs introduced last year, such as those for UVC-compatible webcam access and Neural Engine, are now open to all developers without enterprise licenses. You can now train object-tracking models directly from the command line, automating the process and integrating it into existing pipelines.

License files are now accessible within the Apple Developer account, renewals are automatic, and a new framework enables developers to easily check license status and app approvals for specific features.4:54 -Enhance user experienceSeveral new features in visionOS 26 enhance user experiences in spatial applications. 

One such feature is Window Follow Mode, which allows users to enable a window to move with them as they navigate through a spatial environment. This is particularly useful for tasks that require frequent reference to information while moving. To enable this mode, you need to request and manage a specific licensed entitlement.

Spatial computing is also enhanced for collaboration. Shared coordinate spaces enable people who are physically together to share their spatial experiences. Everyone can interact with and discuss the app's content as if it were physically present. 

You can utilize SharePlay for easy setup, or for more control, a new ARKit API called 'SharedCoordinateSpaceProvider' is available specifically for enterprise customers. This API allows multiple participants to align their coordinate systems and share data over a chosen local network transport, creating a common coordinate system for shared world anchors.

Data privacy and security are also of the utmost importance, especially in enterprise applications that handle sensitive information. A new API called 'contentCaptureProtected' allows you to mark specific user interface elements or entire scenes as protected. The system then automatically obscures this protected content in any screen captures, recordings, mirrored views, or shared sessions, ensuring that sensitive data remains visible only to the user wearing the device.13:37 -Visualize the environmentThe latest visionOS enhances Apple Vision Pro's capabilities to bridge the physical and digital worlds. The camera system is expanded, providing you with greater access to the device's cameras through the ARKit 'CameraFrameProvider' API. This capability allows apps to utilize camera feeds in both immersive and shared space environments.

A new feature enables people to magnify and enhance specific areas of their real-world view. Using VisionKit's 'CameraRegionView', developers can create windows that display dedicated video feeds of selected regions, making critical details clearer. This feature is particularly useful for professionals, such as technicians and inspectors, who need to monitor small gauges or components in poorly lit areas.

You can implement this feature with just a few lines of SwiftUI code, utilizing the 'CameraRegionView' and 'CameraRegionProvider' APIs. These APIs offer built-in processing options like contrast and vibrancy enhancement, and allow for custom image analysis and modification, providing flexibility for various applications. ARKit's Camera Regions feature enhances the pass-through camera feed in Apple Vision Pro, which can be applied to specific areas defined by virtual windows called 'CameraRegionAnchors'. 

You can create these anchors by specifying their position, orientation, and size in the real world. ARKit then provides stabilized and enhanced pixel buffers for these regions. The feature allows for zooming and panning, and dynamically adjusts frame rate based on the chosen enhancement algorithm. Be sure to consider resource usage, with larger camera regions impacting memory and processing.24:01 -Next stepsFor best results in Apple Vision Pro enterprise apps, prioritize safety, respect privacy by only collecting necessary data, ensure apps meet eligibility requirements (in-house or custom B2B), and request only the entitlements needed for specific functionality.

0:04 -Introduction

Last year, Apple introduced the Enterprise APIs, and this year, there are a host of new features and improvements. These APIs, designed for proprietary in-house apps, require managed entitlements and license files. The updates focus on streamlining development, enhancing user experience, and enabling new environment visualization capabilities.

Last year, Apple introduced the Enterprise APIs, and this year, there are a host of new features and improvements. These APIs, designed for proprietary in-house apps, require managed entitlements and license files. The updates focus on streamlining development, enhancing user experience, and enabling new environment visualization capabilities.

1:37 -Streamline development

The latest visionOS enhances developer capabilities by expanding API access. Key improvements include wider API availability, object-tracking enhancements, and simplified enterprise license management. 

Several APIs introduced last year, such as those for UVC-compatible webcam access and Neural Engine, are now open to all developers without enterprise licenses. You can now train object-tracking models directly from the command line, automating the process and integrating it into existing pipelines.

License files are now accessible within the Apple Developer account, renewals are automatic, and a new framework enables developers to easily check license status and app approvals for specific features.

The latest visionOS enhances developer capabilities by expanding API access. Key improvements include wider API availability, object-tracking enhancements, and simplified enterprise license management. 

Several APIs introduced last year, such as those for UVC-compatible webcam access and Neural Engine, are now open to all developers without enterprise licenses. You can now train object-tracking models directly from the command line, automating the process and integrating it into existing pipelines.

License files are now accessible within the Apple Developer account, renewals are automatic, and a new framework enables developers to easily check license status and app approvals for specific features.

4:54 -Enhance user experience

Several new features in visionOS 26 enhance user experiences in spatial applications. 

One such feature is Window Follow Mode, which allows users to enable a window to move with them as they navigate through a spatial environment. This is particularly useful for tasks that require frequent reference to information while moving. To enable this mode, you need to request and manage a specific licensed entitlement.

Spatial computing is also enhanced for collaboration. Shared coordinate spaces enable people who are physically together to share their spatial experiences. Everyone can interact with and discuss the app's content as if it were physically present. 

You can utilize SharePlay for easy setup, or for more control, a new ARKit API called 'SharedCoordinateSpaceProvider' is available specifically for enterprise customers. This API allows multiple participants to align their coordinate systems and share data over a chosen local network transport, creating a common coordinate system for shared world anchors.

Data privacy and security are also of the utmost importance, especially in enterprise applications that handle sensitive information. A new API called 'contentCaptureProtected' allows you to mark specific user interface elements or entire scenes as protected. The system then automatically obscures this protected content in any screen captures, recordings, mirrored views, or shared sessions, ensuring that sensitive data remains visible only to the user wearing the device.

Several new features in visionOS 26 enhance user experiences in spatial applications. 

One such feature is Window Follow Mode, which allows users to enable a window to move with them as they navigate through a spatial environment. This is particularly useful for tasks that require frequent reference to information while moving. To enable this mode, you need to request and manage a specific licensed entitlement.

Spatial computing is also enhanced for collaboration. Shared coordinate spaces enable people who are physically together to share their spatial experiences. Everyone can interact with and discuss the app's content as if it were physically present. 

You can utilize SharePlay for easy setup, or for more control, a new ARKit API called 'SharedCoordinateSpaceProvider' is available specifically for enterprise customers. This API allows multiple participants to align their coordinate systems and share data over a chosen local network transport, creating a common coordinate system for shared world anchors.

Data privacy and security are also of the utmost importance, especially in enterprise applications that handle sensitive information. A new API called 'contentCaptureProtected' allows you to mark specific user interface elements or entire scenes as protected. The system then automatically obscures this protected content in any screen captures, recordings, mirrored views, or shared sessions, ensuring that sensitive data remains visible only to the user wearing the device.

13:37 -Visualize the environment

The latest visionOS enhances Apple Vision Pro's capabilities to bridge the physical and digital worlds. The camera system is expanded, providing you with greater access to the device's cameras through the ARKit 'CameraFrameProvider' API. This capability allows apps to utilize camera feeds in both immersive and shared space environments.

A new feature enables people to magnify and enhance specific areas of their real-world view. Using VisionKit's 'CameraRegionView', developers can create windows that display dedicated video feeds of selected regions, making critical details clearer. This feature is particularly useful for professionals, such as technicians and inspectors, who need to monitor small gauges or components in poorly lit areas.

You can implement this feature with just a few lines of SwiftUI code, utilizing the 'CameraRegionView' and 'CameraRegionProvider' APIs. These APIs offer built-in processing options like contrast and vibrancy enhancement, and allow for custom image analysis and modification, providing flexibility for various applications. ARKit's Camera Regions feature enhances the pass-through camera feed in Apple Vision Pro, which can be applied to specific areas defined by virtual windows called 'CameraRegionAnchors'. 

You can create these anchors by specifying their position, orientation, and size in the real world. ARKit then provides stabilized and enhanced pixel buffers for these regions. The feature allows for zooming and panning, and dynamically adjusts frame rate based on the chosen enhancement algorithm. Be sure to consider resource usage, with larger camera regions impacting memory and processing.

The latest visionOS enhances Apple Vision Pro's capabilities to bridge the physical and digital worlds. The camera system is expanded, providing you with greater access to the device's cameras through the ARKit 'CameraFrameProvider' API. This capability allows apps to utilize camera feeds in both immersive and shared space environments.

A new feature enables people to magnify and enhance specific areas of their real-world view. Using VisionKit's 'CameraRegionView', developers can create windows that display dedicated video feeds of selected regions, making critical details clearer. This feature is particularly useful for professionals, such as technicians and inspectors, who need to monitor small gauges or components in poorly lit areas.

You can implement this feature with just a few lines of SwiftUI code, utilizing the 'CameraRegionView' and 'CameraRegionProvider' APIs. These APIs offer built-in processing options like contrast and vibrancy enhancement, and allow for custom image analysis and modification, providing flexibility for various applications. ARKit's Camera Regions feature enhances the pass-through camera feed in Apple Vision Pro, which can be applied to specific areas defined by virtual windows called 'CameraRegionAnchors'. 

You can create these anchors by specifying their position, orientation, and size in the real world. ARKit then provides stabilized and enhanced pixel buffers for these regions. The feature allows for zooming and panning, and dynamically adjusts frame rate based on the chosen enhancement algorithm. Be sure to consider resource usage, with larger camera regions impacting memory and processing.

24:01 -Next steps

For best results in Apple Vision Pro enterprise apps, prioritize safety, respect privacy by only collecting necessary data, ensure apps meet eligibility requirements (in-house or custom B2B), and request only the entitlements needed for specific functionality.

For best results in Apple Vision Pro enterprise apps, prioritize safety, respect privacy by only collecting necessary data, ensure apps meet eligibility requirements (in-house or custom B2B), and request only the entitlements needed for specific functionality.

## Code Samples

```swift
xcrun createml objecttracker -s my.usdz -o my.referenceobject
```

```swift
import
 VisionEntitlementServices


func
 
checkLicenseStatus
() {
    
// Get the shared license details instance

    
let
 license 
=
 
EnterpriseLicenseDetails
.shared

    
// First, you might check the overall license status

    
guard
 license.licenseStatus 
==
 .valid 
else
 {
        
print
(
"Enterprise license is not valid: 
\(license.licenseStatus)
"
)
        
// Optionally disable enterprise features or alert the user

        
return

    }

    
// Then, check for a specific entitlement before using the feature

    
if
 license.isApproved(for: .mainCameraAccess) {
        
// Safe to proceed with using the main camera API

        
print
(
"Main Camera Access approved. Enabling feature..."
)
        
// ... enable camera functionality ...

    } 
else
 {
        
// Feature not approved for this license

        
print
(
"Main Camera Access not approved."
)
        
// ... keep feature disabled, potentially inform user ...

    }
}
```

```swift
//


//  SharedCoordinateSpaceModel.swift


//



import
 ARKit


class
 
SharedCoordinateSpaceModel
 {
    
let
 arkitSession 
=
 
ARKitSession
()
    
let
 sharedCoordinateSpace 
=
 
SharedCoordinateSpaceProvider
()
    
let
 worldTracking 
=
 
WorldTrackingProvider
()

    
func
 
runARKitSession
() 
async
 {
        
do
 {
            
try
 
await
 arkitSession.run([sharedCoordinateSpace, worldTracking])
        } 
catch
 {
            reportError(
"Error: running session: 
\(error)
"
)
        }
    }

    
// Push data received from other participants

    
func
 
pushCoordinateSpaceData
(
_
 
data
: 
Data
) {
        
if
 
let
 coordinateSpaceData 
=
 
SharedCoordinateSpaceProvider
.
CoordinateSpaceData
(data: data) {
            sharedCoordinateSpace.push(data: coordinateSpaceData)
        }
    }

    
// Poll data to be sent to other participants

    
func
 
pollCoordinateSpaceData
() 
async
 {
        
if
 
let
 coordinateSpaceData 
=
 sharedCoordinateSpace.nextCoordinateSpaceData {
            
// Send my coordinate space data

        }
    }

    
// Be notified when participants connect or disconnect from the shared coordinate space

    
func
 
processEventUpdates
() 
async
 {
        
let
 participants 
=
 [
UUID
]()
        
for
 
await
 event 
in
 sharedCoordinateSpace.eventUpdates {
            
switch
 event {
                
// Participants changed

            
case
 .connectedParticipantIdentifiers(participants: participants):
                
// handle change

                
print
(
"Handle change in participants"
)
            
case
 .sharingEnabled:
                
print
(
"sharing enabled"
)
            
case
 .sharingDisabled:
                
print
(
"sharing disabled"
)
            
@unknown
 
default
:
                
print
(
"handle future events"
)
            }
        }
    }

    
// Be notified when able to add shared world anchors

    
func
 
processSharingAvailabilityUpdates
() 
async
 {
        
for
 
await
 sharingAvailability 
in
 worldTracking.worldAnchorSharingAvailability
            
where
 sharingAvailability 
==
 .available {
            
// Able to add anchor

        }
    }
    
// Add shared world anchor

    
func
 
addWorldAnchor
(
at
 
transform
: simd_float4x4) 
async
 
throws
 {
        
let
 anchor 
=
 
WorldAnchor
(originFromAnchorTransform: transform, sharedWithNearbyParticipants: 
true
)
        
try
 
await
 worldTracking.addAnchor(anchor)
    }

    
// Process shared anchor updates from local session and from other participants

    
func
 
processWorldTrackingUpdates
() 
async
 {
        
for
 
await
 update 
in
 worldTracking.anchorUpdates {
            
switch
 update.event {
            
case
 .added, .updated, .removed:
                
// Handle anchor updates

                
print
(
"Handle updates to shared world anchors"
)
            }
        }
    }
}
```

```swift
// Example implementing contentCaptureProtected



struct
 
SecretDocumentView
: 
View
 {
    
var
 body: 
some
 
View
 {
        
VStack
 {
            
Text
(
"Secrets"
)
                .font(.largeTitle)
                .padding()

            
SensitiveDataView
()
                .contentCaptureProtected()
        }
        .frame(maxWidth: .infinity, maxHeight: .infinity, alignment: .top)
    }
}
```

```swift
//


//  InspectorView.swift


//



import
 SwiftUI

import
 VisionKit


struct
 
InspectorView
: 
View
 {
    
@Environment
(
CameraFeedDelivery
.
self
) 
private
 
var
 cameraFeedDelivery: 
CameraFeedDelivery


    
var
 body: 
some
 
View
 {
        
CameraRegionView
(isContrastAndVibrancyEnhancementEnabled: 
true
) { result 
in

            
var
 pixelBuffer: 
CVReadOnlyPixelBuffer
?
            
switch
 result {
            
case
 .success(
let
 value):
                pixelBuffer 
=
 value.pixelBuffer
            
case
 .failure(
let
 error):
                reportError(
"Failure: 
\(error.localizedDescription)
"
)
                cameraFeedDelivery.stopFeed()
                
return
 
nil

            }

            cameraFeedDelivery.frameUpdate(pixelBuffer: pixelBuffer
!
)
            
return
 
nil

        }
    }
}


@main


struct
 
EnterpriseAssistApp
: 
App
 {
    
var
 body: 
some
 
Scene
 {
        
WindowGroup
 {
            
ContentView
()
        }

        
WindowGroup
(id: 
"InspectorView"
) {
            
InspectorView
()
        }
        .windowResizability(.contentSize)
    }
}
```

```swift
class
 
CameraRegionHandler
 {
    
let
 arkitSession 
=
 
ARKitSession
()
    
var
 cameraRegionProvider: 
CameraRegionProvider
?
    
var
 cameraRegionAnchor: 
CameraRegionAnchor
?

    
func
 
setUpNewAnchor
(
anchor
: simd_float4x4, 
width
: 
Float
, 
height
: 
Float
) 
async
 {
        
let
 anchor 
=
 
CameraRegionAnchor
(originFromAnchorTransform: anchor,
                                        width: width,
                                        height: height,
                                        cameraEnhancement: .stabilization)

        
guard
 
let
 cameraRegionProvider 
=
 
self
.cameraRegionProvider 
else
 {
            reportError(
"Missing CameraRegionProvider"
)
            
return

        }

        
do
 {
            
try
 
await
 cameraRegionProvider.addAnchor(anchor)
        } 
catch
 {
            reportError(
"Error adding anchor: 
\(error)
"
)
        }
        cameraRegionAnchor 
=
 anchor

        
Task
 {
            
let
 updates 
=
 cameraRegionProvider.anchorUpdates(forID: anchor.id)
            
for
 
await
 update 
in
 updates {
                
let
 pixelBuffer 
=
 update.anchor.pixelBuffer
                
// handle pixelBuffer

            }
        }
    }

    
func
 
removeAnchor
() 
async
 {
        
guard
 
let
 cameraRegionProvider 
=
 
self
.cameraRegionProvider 
else
 {
            reportError(
"Missing CameraRegionProvider"
)
            
return

        }

        
if
 
let
 cameraRegionAnchor 
=
 
self
.cameraRegionAnchor {
            
do
 {
                
try
 
await
 cameraRegionProvider.removeAnchor(cameraRegionAnchor)
            } 
catch
 {
                reportError(
"Error removing anchor: 
\(error.localizedDescription)
"
)
                
return

            }
            
self
.cameraRegionAnchor 
=
 
nil

        }
    }
}
```

