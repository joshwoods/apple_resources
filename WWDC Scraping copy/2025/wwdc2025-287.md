# Wwdc2025 287

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Summary

Transcript

Code

What’s new in RealityKitUnleash your creativity with new RealityKit features that can help you build rich 3D content for iOS, iPadOS, macOS, tvOS and visionOS. Learn how you can access ARKit data directly through RealityKit. Explore how you can interact with your 3D content more naturally using the object manipulation feature. Discover some new APIs for scene understanding, environment blending, instancing and much more, all using an interactive sample.Chapters0:00 -Introduction3:19 -Anchoring updates6:52 -ManipulationComponent10:01 -Scene understanding11:18 -EnvironmentBlendingComponent12:26 -MeshInstancesComponent16:28 -Immersive media21:43 -Accessories and moreResourcesPlaying immersive media with RealityKitPresenting images in RealityKitHD VideoSD VideoRelated VideosWWDC25Better together: SwiftUI and RealityKitExplore spatial accessory input on visionOSSupport immersive video playback in visionOS apps

Unleash your creativity with new RealityKit features that can help you build rich 3D content for iOS, iPadOS, macOS, tvOS and visionOS. Learn how you can access ARKit data directly through RealityKit. Explore how you can interact with your 3D content more naturally using the object manipulation feature. Discover some new APIs for scene understanding, environment blending, instancing and much more, all using an interactive sample.

0:00 -Introduction

3:19 -Anchoring updates

6:52 -ManipulationComponent

10:01 -Scene understanding

11:18 -EnvironmentBlendingComponent

12:26 -MeshInstancesComponent

16:28 -Immersive media

21:43 -Accessories and more

Playing immersive media with RealityKit

Presenting images in RealityKit

HD VideoSD Video

HD Video

SD Video

Better together: SwiftUI and RealityKit

Explore spatial accessory input on visionOS

Support immersive video playback in visionOS apps

Search this video…Hi, I’m Laurence, a software engineer on the RealityKit team.Welcome to my session, “What’s new in RealityKit”.In this session, I’ll discuss some of the new RealityKit featuresthat are being released this year.We introduced RealityKit in 2019to enable you to integrate 3D content into your apps,providing realistic rendering and enhancing immersive experiences.Since then we’ve received a lot of feedback from you, which has helped usmake this framework better year after year.RealityKit offers a wide variety of capabilitiesfor your 3D content to blend seamlessly with the real-world environment,enabling you to create immersive apps and games on visionOS.Alongside visionOS, RealityKit also brings many key features of RealityKitto iOS, iPadOS, and macOS.With RealityKit’s cross-platform capabilities, you can write your app onceand bring it to many different platforms with minimal code changes.This year I’m proud to announce that RealityKit is now supportedon the latest tvOS!Now you can bring your existing apps and experiences to AppleTV or create new onesfor the big screen!RealityKit is supported on all generations of AppleTV 4K.This year’s RealityKit update brings a wide array of new functionalities thatmake it easier than ever to create 3D experiences that blend the virtualand real world together.In this session, I’ll take you through some of these features,like the ManipulationComponent, EnvironmentBlendingComponent,MeshInstancesComponent and more.I’ll use some of these features to build a spatial puzzle game.The game begins with a locked chest that’s anchored to a surface in front you.Around this chest there will be several objects that you can interact with.One of these objects will have the key that unlocks the chestattached to the bottom of it.By grabbing and inspecting the objects you can find which one has the key.And once you have the key, you can unlock the chest and see your prize:a tiny fireworks display!First, I’m going to use the new native ARKit support in RealityKit to anchormodels to the space in front of the player and handle anchor lifecycle changes.Then I’ll show how you can use the new ManipulationComponent to add interactionsto 3D entities in a scene.I’ll also use a PhysicsBodyComponentto have them realistically drop down when released.Then I will use the new SceneUnderstanding APIs to allow the game entitiesto collide with our scene understanding mesh.After that I’m going to use the EnvironmentBlendingComponent to allowthe app to better blend with the real world.Next, I will show you how you can use the new MeshInstancesComponentto efficiently draw multiple instances of a 3D modelthat I can use to decorate my scene.Then, I will go over some exciting new immersive media updates.I will also cover other new announcements like spatial accessories,updates to entities and more.I will start by anchoring 3D models to the real world environment using a new APIthat provides access to ARKit data directly through RealityKit.I’ll be using a RealityKit AnchorEntity to position my game on a table.Anchor entities are used to attach virtual content to real world surfaces.This year, we are making AnchorEntities more powerful by exposing the ARKitanchoring data directly.Let me show you how this is done.To gain access to ARKit data directly through RealityKit, you will need to firstcreate a SpatialTrackingSession.The configuration for the session will tell RealityKit to send the newAnchorStateEvents to your app as the AnchorEntities’ states change.Then you can set up an AnchorEntity to filter the propertiesof the anchor you are looking for.In my app I want to look for a table with specific dimensions.Once RealityKit finds the best anchor that matches the properties I set on myAnchorEntity, it will fire an AnchorStateEvent.That AnchorStateEvent instance contains ARKit data such as the transformand extents of the anchor, that I can use to position my game.Let’s go through this in code.I’ll start by creating a SpatialTrackingSession which will allowRealityKit AnchorEntities to be tracked to the environment.For my game I need to track a plane to spawn the treasure chest on,so I’ll set up a SpatialTrackingSession configuration with plane tracking enabled.Now I can start the tracking session by running the configuration I just created.Next, I can spawn an AnchorEntity to help position my game on the table.I’ll classify the AnchorEntity as a table that should be a horizontal surfacewith a minimum bounds of 15cm squared associated with it.This AnchorEntity will start off in an unanchored state, but will become anchoredwhen a table plane is detected that matches the classificationand bounds provided.In order to receive updates when the anchor state changes I’ll need to usethe new AnchorStateEvents API.The AnchorStateEvents API let’s you subscribe to events for when entitieshave been anchored, if they are about to be unanchored,or if they have failed to anchor.I will use the DidAnchor event in my app to position my game entitieswithin the bounds of the table surface.In my code I’ve added a subscription to the DidAnchor event to know whenthe anchor entity has successfully been anchored to the environment.The event structure provides me with my anchor entity, which has been updatedto contain the new ARKitAnchorComponent.This component holds the ARKit data such as extents and transforms that I can useto position my game entities onto the anchored surface.I can access this data by utilizing the anchor propertyof the ARKitAnchorComponent.To use the anchor property I’ll have to cast it to its ARKit anchor type.In this case, I’ll try to cast it to a PlaneAnchor since my AnchorEntityis set up to look for planes.Now I have access to the raw ARKit extents and transforms for the anchor.I’ll be using the ARKit transforms, specifically the originFromAnchorTransformand anchorFromExtentTransform, to position my game and ensure it is centeredon the anchored surface.Now, when I open the immersive space,game objects will be spawned once a suitable surface has been found.Next I’m going to add interactions to my scene using the new ManipulationComponentin RealityKit.The ManipulationComponent simplifies the process of picking upand rotating the 3D entities in your scene.It even supports advanced gestures like swapping hands!I’ll use the ManipulationComponent to enable the player to graband rotate the game objects to find the key underneath them.I’ll show you how you can add this functionality to the game, but if you wantmore information about how the ManipulationComponent worksand what you can do with it, please watchthe "Better together: SwiftUI and RealityKit" session.To enable you to pick up the entities and interact with them, you only need to callthe ManipulationComponent configureEntity function.This function will automatically add the necessary InputTarget, Collision,HoverEffect, and Manipulation components to the entity.And that’s it!Now you can pick up and rotate the entities with your hands.However, you’ll notice that the objects will smoothly animate backto where they originated from when they’re released.I’ll go back into my code and set the `releaseBehavior` propertyon the ManipulationComponent to stay.I’ll then assign this manipulationComponent instanceto my entity.This will prevent the objectfrom automatically animating back to its starting positionwhen I release it.It will instead remain stationary.Next, I’ll have to make it fall to the floor.For that I’ll add a PhysicsBodyComponent to my entity.I’d like to be careful to only enable gravity on the PhysicsBodyComponentwhen the object is not being pinched and picked up.The new ManipulationEvents API makes it easy to do this!ManipulationEvents are events emitted by RealityKit that describe variousinteraction states that entities go throughwhen they are being interacted with.For example, the WillRelease event will get triggered when an entityis released by the player.Similarly, there are other events as well like WillBegin, WillEnd,DidUpdateTransform as well as DidHandOff.Please read the documentation on developer.apple.com for more details.I’ll be using the WillBegin and WillEnd events to make sure my game entitiesare only reacting to gravity when they are not being interacted with.First I’ll add a subscription to the WillBegin event to change thephysicsBodyComponent mode to kinematic to keep the physics system from interfering,while the object is being moved.Doing this will also prevent gravity from affecting the entity.Next, I’ll add a subscription to the WillEnd eventto change the physicsBodyComponent mode back to dynamicsince the entity is no longer being interacted with.Doing this will allow the entity to react to other physics objects in the scene.It will also respond to gravity!Now that I have the game objects responding to physics,I need them to collide with the player’s surroundings.I can do this by using the new Scene Understanding API that adds the mesh frommy room to the app’s physics simulation.Through the SpatialTrackingSession API, RealityKit is able to generatea mesh of your surroundings.This mesh, known as the Scene Understanding mesh, can be used to addcollision and physics to the real world objects in your room.You can utilize the scene understanding mesh from your real world surroundingsby setting the SceneUnderstandingFlagsunder the SpatialTrackingSession Configuration.visionOS currently supports the collision and physics flags.I’ll be using both of these to enable my game objectsto collide with the scene understanding mesh.I’ll set these flags on the SpatialTrackingSession Configurationbefore running it.To do this, I’ll need to update the SpatialTrackingSession I set up earlier.All I have to do is add the collision and physics scene understanding flagsto the SpatialTrackingSession configuration before we start the session.Now, the scene understanding mesh will participate in our game’s physicssimulation and game objects will collide with the environment when I drop themon the table or the floor.Now that my game can interact with the environment, I want to have it respondvisually to the environment as well.For this I can use the EnvironmentBlendingComponent.The EnvironmentBlendingComponent is a new component designed for immersive spaceapps in this year’s RealityKit update.This component allows entities to be hidden by static real world objects.Entities with this component are realistically occluded either partially orfully depending on how much of the entity is covered by a static real world object.Dynamic moving objects like people and petswill not occlude objects with this component.If I want to add this functionality, all I have to do is addthe EnvironmentBlendingComponent and set its preferred blending modeto be occluded by the surroundings.Now if an entity with the EnvironmentBlendingComponent is positionedbehind a real world object you will notice that the entity will be occluded by it!Please note that entities that use the EnvironmentBlendingComponent will betreated as part of their background environment and will alwaysget drawn behind other virtual objects in your scene.Now that I have the EnvironmentBlendingComponent working,I can add some decorationsto the surrounding game area using the new MeshInstancesComponent.Last year the LowLevelMesh and LowLevelTexture APIs were addedto RealityKit to give you far greater control of your rendering data.In this year’s RealityKit update, this low level access is being expandedto another aspect of rendering: Instancing.In my app, I want to decorate the surrounding spaceand also define a playable area.I could spawn a bunch of duplicate entities around to decorate the space.However, to do this I’d need to clone my entity many times,which would create many copies of my ModelComponent.This could result in a large memory and processing footprint.A more efficient, and also convenient way to do thisis by using the new MeshInstancesComponent.The MeshInstancesComponent allows you to drawa mesh multiple times with a single entity.All you have to provide is a list of transforms to draw the mesh with.On iOS, iPadOS, macOS, and tvOS you can use a LowLevelBuffer to pass render datato your CustomMaterial to make each mesh instance look unique.In addition to being convenient, the MeshInstancesComponent can also improveperformance by reducing the amount of data that needs to be sent to the GPU.Instead of sending multiple copies of the model and materials to the GPUwhen drawing duplicate meshes, the MeshInstancesComponent willonly send that data once.It’s important to note that the models that are drawn with a singleMeshInstancesComponent are still considered a part of a single entity.If you use this component to cover a large area, it may make sense to break it upinto several smaller entities to allow culling to take place.Here’s how you can use the MeshInstancesComponent in code.First, I need a mesh to instance.I’ll get this by loading an entity from the app’s content bundle.Now, I can initialize the MeshInstancesComponentand the LowLevelInstanceData object.The LowLevelInstanceData object is what holdsthe data for each of the individual mesh instances.When I create a LowLevelInstanceData object I have to provide the numberof instances that I need for my app.I’ll use 20 here to display a rough approximation of the play areawithout over crowding it.Next, I can assign the LowLevelInstanceData object to theMeshInstancesComponent subscriptedby the index of the mesh part that I want to instance.In my case, I know the mesh that I’m instancing is simple and only has onemesh part, so I am going to assign the LowLevelDataObject to partIndex: 0.Now I can populate the LowLevelInstanceData objectwith the transforms for each mesh instance.In order to have varied decorations, I’ll randomize the scale,angle, and position for each of these instances.With these values I can create a transform matrix and assign that to an instance.Now I can add the meshInstancesComponent to my entity and whenever my entity isdrawn it’ll draw using the data from the MeshInstancesComponent.With that ...the game is completed!You can start the game and anchor it to the surface in front of you.You can pick up and rotate the objects in the play area to findthe key that unlocks the chest!I’ll briefly recap the new APIs I used to create this app.I used the new AnchorStateEvent APIs to anchor the content.Then I used the ManipulationComponent to allow interaction with the objects.I used the Scene understanding flagsto enable the game entities to collide with the scene understanding mesh.Finally, I used the EnvironmentBlendingComponentand MeshInstancesComponent to help the game blend in with the real world.Next, I will share some other exciting features that are being addedto RealityKit this year, like support for new immersive media.This year, we are introducing a brand new component calledImagePresentationComponent that is used for presenting images in RealityKit.It supports three kinds of images:traditional 2D images and photos,spatial photos, which are stereoscopic photos from your iPhoneor Vision Pro,and spatial scenes, a new kind of 3D imagecreated from an existing 2D image or photo.Spatial scenes are 3D images with real depth, generated from a 2D image.They’re like a diorama version of a photo, with motion parallax to accentuatethe spatial scene’s depth as the viewer moves their head relative to the scene.Spatial scenes are a great way to bring your existing 2D photos to life,both in the Photos app on visionOS, and in your own app, with RealityKit.Let me take you through the code to add the three kinds of images to your app.I’ll start by showing you how to present a 2D image or photo with RealityKit.I’ll first find a URL for a 2D photo, and use that URL to createa new image presentation component.The initializer for the component is asyncsince it can take a short while to load the image into memory.Once the component is initialized, I can assign it to an entity to display itin my RealityKit scene.For presenting spatial photos, there is one more step involved.You will need to assign a desired viewing mode for the componentbefore setting it on the entity.And you can specify the desired viewing mode by first checkingif your image supports it.If you do not specify a desired viewing mode or your image does not support it,then the ImagePresentationComponent will present your image in a 2D, or monoscopicviewing mode, even if it is a spatial photo.To opt into immersive spatial photo presentation, use a desired viewing modeof spatialStereoImmersive instead.Whenever you create an image presentation component from a spatial photo,both spatial stereo modes will be available.Both 2D images and spatial photos are loaded from a file on disk.Displaying this image as a spatial scene requires a few additional steps,because we need to generate the spatial scene before we can present it.Let me show you how you can generate and present a spatial scene in code.You can generate a spatial scene using a 2D imageor a spatial photo.If you generate a spatial scene from a spatial photo,only one of the channels in the spatial photo,will be used as the 2D image for conversion.To create a spatial scene, you don’t initializethe ImagePresentationComponent directly from the image URL.Instead, you can create a Spatial3DImage from the URL,and use the spatial 3D image to initialize the ImagePresentationComponent.However, the component isn’t yet ready to present as a spatial scenefor that, you need to generate the scene first.We do this by calling the spatial 3D image’s generate method.This will generate the spatial scene in a few seconds.After successful generation,the ImagePresentationComponent’s availableViewingModeswill update to include the spatial3D and spatial3DImmersive modes.You can then set one of them as the desired viewing mode to opt into windowedor immersive presentation of the spatial scene.Note that you don’t have to generate the spatial scene in advance.You might want to wait until the person using your app presses a button,like in the Photos app.Setting the component’s desired viewing mode to .spatial3D before callinggenerate tells the component that you want to showthe spatial scene as soon as it is ready.This prompts the component to show a progress animation during the generationprocess, and to display the spatial scene as soon as generation completes.Here’s an example of how that looks on Vision Pro.Image presentation component shows the same generation animation as the Photosapp on visionOS, and the end result looks great in 3D.Here’s a quick summary of all the different ways you can useImagePresentationComponent to present a 2D image,a spatial photo, or a spatial scene.For more information on this component,check out the “Presenting images in RealityKit” Sample Codeon developer.apple.com.Another immersive media update this year is that VideoPlayerComponent has beenupdated to support the playback of a wide range of immersive video formats!It now supports spatial video playback with full spatial styling, in both portaland immersive modes.Apple Projected Media Profile videos such as 180 degree, 360 degree,and wide-field-of-view videos are also supported!People can also configure comfort settings for Apple Projected Media Profile videosand RealityKit will automatically adjust playback to accommodate.These video formats, in addition to Apple Immersive Video, can be configuredto be played in a variety of viewing modes.For a deeper look into these updates, please check out the “Support immersivevideo playback in visionOS apps” session.Next I’ll discuss some of our other updates this year.First, I'll introduce tracked Spatial Accessories.Next, I'll go over the latest updates with SwiftUI and RealityKit integration.After that, I'll cover the new entity updates.I'll give an overview of RealityKit's AVIF texture support.Then I'll discuss the new hover effect groupID feature.And finally I'll talk about the additionof post processing effects to RealityViews.Let's begin.RealityKit is adding support for tracking spatial accessories that allow youto interact with your apps in both Shared space and Full space.You can track spatial accessories in six degrees of freedom, and they also supporthaptics to enhance the interactions in your apps and games.To learn more about how to add spatial accessory input to your apps,watch the “Explore spatial accessory input on visionOS” session.This year, RealityKit is also introducing some brand new components to allow forbetter SwiftUI integration.The ViewAttachmentComponent makes it very simpleto add SwiftUI views directly to your entities.Additionally, the PresentationComponent enables you to add modal presentations,like popovers, to entitiesAlso, the new GestureComponent simplifies the process of adding SwiftUIgestures to entities.Please check out the "Better together: SwiftUI and RealityKit" session for moreinformation on what’s new with SwiftUI and RealityKit integration this year!There is also a new entity attach method that allows you to attach one entityto the pin of another entity.This API greatly simplifies attaching meshes to the jointsof an animated skeleton.Attaching meshes this way will avoid having to manually align the meshesand will also avoid expensive hierarchical transform updates.Additionally, there is a new Entity initializer that allows you to loadentities from in memory Data objects.With this new initializer you can load entire RealityKit scenes or USDs froman online source or stream them over the network.The new initializer supports the same file formatsas the existing entity initializers.Furthermore, RealityKit is adding support for AVIF encoded textures,which offer quality similar to jpeg with support for 10 bit colorswhile being significantly smaller in size.You can use the Preview app on your Mac or usdcrush in the Terminal to export USDswith this compression enabled.Also, the HoverEffectComponent is receiving a new feature: GroupIDs.GroupIDs are a way to create associations between hover effects.Any hover effects that share a GroupID will also share activations.Assigning hover effects a GroupID will give you complete control of howa hover effect is activated irrespective of their relative hierarchy to each other.Normally hover effects are applied hierarchically like in the exampleon the left where child entities inherit the effects of their parent entities.However, if an entity has a GroupID like Entity A and Entity B in the exampleon the right, then they will not propagate their effects to their children.Another cool addition this year is supportfor post processing effects in RealityView.You can use the customPostProcessing API to add custom effects,like bloom, to your apps using Metal Performance Shaders, CIFilters,or your own shaders.This API is supported on iOS, iPadOS, macOS, and tvOS.This year’s RealityKit update is focused on making the creation of 3D experienceswith RealityKit easier than ever.I went over how you can create a spatial puzzle game that uses some of the newRealityKit APIs to anchor the game to your environmentand to enable intuitive ways to interact with the game pieces.I also discussed the new immersive media updates that enable your apps to displayspatial content directly in RealityKit.Then I covered some additional updates such as spatial accessory tracking,entity updates, and hover effect groupsIDs.With these new features it’s easier than ever to build 3D apps with RealityKitand I’m so excited to see what experiences you come up with.Thanks for watching.

Hi, I’m Laurence, a software engineer on the RealityKit team.Welcome to my session, “What’s new in RealityKit”.In this session, I’ll discuss some of the new RealityKit featuresthat are being released this year.

We introduced RealityKit in 2019to enable you to integrate 3D content into your apps,providing realistic rendering and enhancing immersive experiences.Since then we’ve received a lot of feedback from you, which has helped usmake this framework better year after year.RealityKit offers a wide variety of capabilitiesfor your 3D content to blend seamlessly with the real-world environment,enabling you to create immersive apps and games on visionOS.

Alongside visionOS, RealityKit also brings many key features of RealityKitto iOS, iPadOS, and macOS.With RealityKit’s cross-platform capabilities, you can write your app onceand bring it to many different platforms with minimal code changes.This year I’m proud to announce that RealityKit is now supportedon the latest tvOS!Now you can bring your existing apps and experiences to AppleTV or create new onesfor the big screen!RealityKit is supported on all generations of AppleTV 4K.This year’s RealityKit update brings a wide array of new functionalities thatmake it easier than ever to create 3D experiences that blend the virtualand real world together.In this session, I’ll take you through some of these features,like the ManipulationComponent, EnvironmentBlendingComponent,MeshInstancesComponent and more.I’ll use some of these features to build a spatial puzzle game.

The game begins with a locked chest that’s anchored to a surface in front you.Around this chest there will be several objects that you can interact with.One of these objects will have the key that unlocks the chestattached to the bottom of it.

By grabbing and inspecting the objects you can find which one has the key.And once you have the key, you can unlock the chest and see your prize:a tiny fireworks display!First, I’m going to use the new native ARKit support in RealityKit to anchormodels to the space in front of the player and handle anchor lifecycle changes.Then I’ll show how you can use the new ManipulationComponent to add interactionsto 3D entities in a scene.I’ll also use a PhysicsBodyComponentto have them realistically drop down when released.

Then I will use the new SceneUnderstanding APIs to allow the game entitiesto collide with our scene understanding mesh.

After that I’m going to use the EnvironmentBlendingComponent to allowthe app to better blend with the real world.

Next, I will show you how you can use the new MeshInstancesComponentto efficiently draw multiple instances of a 3D modelthat I can use to decorate my scene.

Then, I will go over some exciting new immersive media updates.

I will also cover other new announcements like spatial accessories,updates to entities and more.I will start by anchoring 3D models to the real world environment using a new APIthat provides access to ARKit data directly through RealityKit.I’ll be using a RealityKit AnchorEntity to position my game on a table.Anchor entities are used to attach virtual content to real world surfaces.This year, we are making AnchorEntities more powerful by exposing the ARKitanchoring data directly.Let me show you how this is done.To gain access to ARKit data directly through RealityKit, you will need to firstcreate a SpatialTrackingSession.The configuration for the session will tell RealityKit to send the newAnchorStateEvents to your app as the AnchorEntities’ states change.Then you can set up an AnchorEntity to filter the propertiesof the anchor you are looking for.In my app I want to look for a table with specific dimensions.

Once RealityKit finds the best anchor that matches the properties I set on myAnchorEntity, it will fire an AnchorStateEvent.

That AnchorStateEvent instance contains ARKit data such as the transformand extents of the anchor, that I can use to position my game.Let’s go through this in code.

I’ll start by creating a SpatialTrackingSession which will allowRealityKit AnchorEntities to be tracked to the environment.For my game I need to track a plane to spawn the treasure chest on,so I’ll set up a SpatialTrackingSession configuration with plane tracking enabled.

Now I can start the tracking session by running the configuration I just created.

Next, I can spawn an AnchorEntity to help position my game on the table.

I’ll classify the AnchorEntity as a table that should be a horizontal surfacewith a minimum bounds of 15cm squared associated with it.This AnchorEntity will start off in an unanchored state, but will become anchoredwhen a table plane is detected that matches the classificationand bounds provided.

In order to receive updates when the anchor state changes I’ll need to usethe new AnchorStateEvents API.

The AnchorStateEvents API let’s you subscribe to events for when entitieshave been anchored, if they are about to be unanchored,or if they have failed to anchor.

I will use the DidAnchor event in my app to position my game entitieswithin the bounds of the table surface.

In my code I’ve added a subscription to the DidAnchor event to know whenthe anchor entity has successfully been anchored to the environment.The event structure provides me with my anchor entity, which has been updatedto contain the new ARKitAnchorComponent.

This component holds the ARKit data such as extents and transforms that I can useto position my game entities onto the anchored surface.I can access this data by utilizing the anchor propertyof the ARKitAnchorComponent.

To use the anchor property I’ll have to cast it to its ARKit anchor type.In this case, I’ll try to cast it to a PlaneAnchor since my AnchorEntityis set up to look for planes.

Now I have access to the raw ARKit extents and transforms for the anchor.I’ll be using the ARKit transforms, specifically the originFromAnchorTransformand anchorFromExtentTransform, to position my game and ensure it is centeredon the anchored surface.

Now, when I open the immersive space,game objects will be spawned once a suitable surface has been found.

Next I’m going to add interactions to my scene using the new ManipulationComponentin RealityKit.The ManipulationComponent simplifies the process of picking upand rotating the 3D entities in your scene.

It even supports advanced gestures like swapping hands!I’ll use the ManipulationComponent to enable the player to graband rotate the game objects to find the key underneath them.I’ll show you how you can add this functionality to the game, but if you wantmore information about how the ManipulationComponent worksand what you can do with it, please watchthe "Better together: SwiftUI and RealityKit" session.

To enable you to pick up the entities and interact with them, you only need to callthe ManipulationComponent configureEntity function.This function will automatically add the necessary InputTarget, Collision,HoverEffect, and Manipulation components to the entity.And that’s it!Now you can pick up and rotate the entities with your hands.However, you’ll notice that the objects will smoothly animate backto where they originated from when they’re released.

I’ll go back into my code and set the `releaseBehavior` propertyon the ManipulationComponent to stay.

I’ll then assign this manipulationComponent instanceto my entity.This will prevent the objectfrom automatically animating back to its starting positionwhen I release it.It will instead remain stationary.

Next, I’ll have to make it fall to the floor.For that I’ll add a PhysicsBodyComponent to my entity.I’d like to be careful to only enable gravity on the PhysicsBodyComponentwhen the object is not being pinched and picked up.The new ManipulationEvents API makes it easy to do this!ManipulationEvents are events emitted by RealityKit that describe variousinteraction states that entities go throughwhen they are being interacted with.

For example, the WillRelease event will get triggered when an entityis released by the player.Similarly, there are other events as well like WillBegin, WillEnd,DidUpdateTransform as well as DidHandOff.

Please read the documentation on developer.apple.com for more details.

I’ll be using the WillBegin and WillEnd events to make sure my game entitiesare only reacting to gravity when they are not being interacted with.

First I’ll add a subscription to the WillBegin event to change thephysicsBodyComponent mode to kinematic to keep the physics system from interfering,while the object is being moved.Doing this will also prevent gravity from affecting the entity.

Next, I’ll add a subscription to the WillEnd eventto change the physicsBodyComponent mode back to dynamicsince the entity is no longer being interacted with.Doing this will allow the entity to react to other physics objects in the scene.It will also respond to gravity!Now that I have the game objects responding to physics,I need them to collide with the player’s surroundings.I can do this by using the new Scene Understanding API that adds the mesh frommy room to the app’s physics simulation.Through the SpatialTrackingSession API, RealityKit is able to generatea mesh of your surroundings.This mesh, known as the Scene Understanding mesh, can be used to addcollision and physics to the real world objects in your room.

You can utilize the scene understanding mesh from your real world surroundingsby setting the SceneUnderstandingFlagsunder the SpatialTrackingSession Configuration.

visionOS currently supports the collision and physics flags.I’ll be using both of these to enable my game objectsto collide with the scene understanding mesh.I’ll set these flags on the SpatialTrackingSession Configurationbefore running it.

To do this, I’ll need to update the SpatialTrackingSession I set up earlier.All I have to do is add the collision and physics scene understanding flagsto the SpatialTrackingSession configuration before we start the session.

Now, the scene understanding mesh will participate in our game’s physicssimulation and game objects will collide with the environment when I drop themon the table or the floor.

Now that my game can interact with the environment, I want to have it respondvisually to the environment as well.For this I can use the EnvironmentBlendingComponent.The EnvironmentBlendingComponent is a new component designed for immersive spaceapps in this year’s RealityKit update.

This component allows entities to be hidden by static real world objects.Entities with this component are realistically occluded either partially orfully depending on how much of the entity is covered by a static real world object.Dynamic moving objects like people and petswill not occlude objects with this component.

If I want to add this functionality, all I have to do is addthe EnvironmentBlendingComponent and set its preferred blending modeto be occluded by the surroundings.

Now if an entity with the EnvironmentBlendingComponent is positionedbehind a real world object you will notice that the entity will be occluded by it!Please note that entities that use the EnvironmentBlendingComponent will betreated as part of their background environment and will alwaysget drawn behind other virtual objects in your scene.Now that I have the EnvironmentBlendingComponent working,I can add some decorationsto the surrounding game area using the new MeshInstancesComponent.Last year the LowLevelMesh and LowLevelTexture APIs were addedto RealityKit to give you far greater control of your rendering data.In this year’s RealityKit update, this low level access is being expandedto another aspect of rendering: Instancing.In my app, I want to decorate the surrounding spaceand also define a playable area.

I could spawn a bunch of duplicate entities around to decorate the space.However, to do this I’d need to clone my entity many times,which would create many copies of my ModelComponent.This could result in a large memory and processing footprint.A more efficient, and also convenient way to do thisis by using the new MeshInstancesComponent.

The MeshInstancesComponent allows you to drawa mesh multiple times with a single entity.All you have to provide is a list of transforms to draw the mesh with.On iOS, iPadOS, macOS, and tvOS you can use a LowLevelBuffer to pass render datato your CustomMaterial to make each mesh instance look unique.

In addition to being convenient, the MeshInstancesComponent can also improveperformance by reducing the amount of data that needs to be sent to the GPU.Instead of sending multiple copies of the model and materials to the GPUwhen drawing duplicate meshes, the MeshInstancesComponent willonly send that data once.

It’s important to note that the models that are drawn with a singleMeshInstancesComponent are still considered a part of a single entity.If you use this component to cover a large area, it may make sense to break it upinto several smaller entities to allow culling to take place.Here’s how you can use the MeshInstancesComponent in code.First, I need a mesh to instance.I’ll get this by loading an entity from the app’s content bundle.Now, I can initialize the MeshInstancesComponentand the LowLevelInstanceData object.The LowLevelInstanceData object is what holdsthe data for each of the individual mesh instances.

When I create a LowLevelInstanceData object I have to provide the numberof instances that I need for my app.I’ll use 20 here to display a rough approximation of the play areawithout over crowding it.Next, I can assign the LowLevelInstanceData object to theMeshInstancesComponent subscriptedby the index of the mesh part that I want to instance.

In my case, I know the mesh that I’m instancing is simple and only has onemesh part, so I am going to assign the LowLevelDataObject to partIndex: 0.

Now I can populate the LowLevelInstanceData objectwith the transforms for each mesh instance.

In order to have varied decorations, I’ll randomize the scale,angle, and position for each of these instances.

With these values I can create a transform matrix and assign that to an instance.

Now I can add the meshInstancesComponent to my entity and whenever my entity isdrawn it’ll draw using the data from the MeshInstancesComponent.

With that ...the game is completed!You can start the game and anchor it to the surface in front of you.You can pick up and rotate the objects in the play area to findthe key that unlocks the chest!I’ll briefly recap the new APIs I used to create this app.I used the new AnchorStateEvent APIs to anchor the content.Then I used the ManipulationComponent to allow interaction with the objects.

I used the Scene understanding flagsto enable the game entities to collide with the scene understanding mesh.Finally, I used the EnvironmentBlendingComponentand MeshInstancesComponent to help the game blend in with the real world.

Next, I will share some other exciting features that are being addedto RealityKit this year, like support for new immersive media.This year, we are introducing a brand new component calledImagePresentationComponent that is used for presenting images in RealityKit.It supports three kinds of images:traditional 2D images and photos,spatial photos, which are stereoscopic photos from your iPhoneor Vision Pro,and spatial scenes, a new kind of 3D imagecreated from an existing 2D image or photo.

Spatial scenes are 3D images with real depth, generated from a 2D image.They’re like a diorama version of a photo, with motion parallax to accentuatethe spatial scene’s depth as the viewer moves their head relative to the scene.Spatial scenes are a great way to bring your existing 2D photos to life,both in the Photos app on visionOS, and in your own app, with RealityKit.

Let me take you through the code to add the three kinds of images to your app.I’ll start by showing you how to present a 2D image or photo with RealityKit.

I’ll first find a URL for a 2D photo, and use that URL to createa new image presentation component.The initializer for the component is asyncsince it can take a short while to load the image into memory.

Once the component is initialized, I can assign it to an entity to display itin my RealityKit scene.

For presenting spatial photos, there is one more step involved.You will need to assign a desired viewing mode for the componentbefore setting it on the entity.And you can specify the desired viewing mode by first checkingif your image supports it.If you do not specify a desired viewing mode or your image does not support it,then the ImagePresentationComponent will present your image in a 2D, or monoscopicviewing mode, even if it is a spatial photo.

To opt into immersive spatial photo presentation, use a desired viewing modeof spatialStereoImmersive instead.Whenever you create an image presentation component from a spatial photo,both spatial stereo modes will be available.Both 2D images and spatial photos are loaded from a file on disk.Displaying this image as a spatial scene requires a few additional steps,because we need to generate the spatial scene before we can present it.Let me show you how you can generate and present a spatial scene in code.

You can generate a spatial scene using a 2D imageor a spatial photo.If you generate a spatial scene from a spatial photo,only one of the channels in the spatial photo,will be used as the 2D image for conversion.

To create a spatial scene, you don’t initializethe ImagePresentationComponent directly from the image URL.Instead, you can create a Spatial3DImage from the URL,and use the spatial 3D image to initialize the ImagePresentationComponent.However, the component isn’t yet ready to present as a spatial scenefor that, you need to generate the scene first.

We do this by calling the spatial 3D image’s generate method.This will generate the spatial scene in a few seconds.After successful generation,the ImagePresentationComponent’s availableViewingModeswill update to include the spatial3D and spatial3DImmersive modes.You can then set one of them as the desired viewing mode to opt into windowedor immersive presentation of the spatial scene.Note that you don’t have to generate the spatial scene in advance.You might want to wait until the person using your app presses a button,like in the Photos app.Setting the component’s desired viewing mode to .spatial3D before callinggenerate tells the component that you want to showthe spatial scene as soon as it is ready.

This prompts the component to show a progress animation during the generationprocess, and to display the spatial scene as soon as generation completes.Here’s an example of how that looks on Vision Pro.Image presentation component shows the same generation animation as the Photosapp on visionOS, and the end result looks great in 3D.

Here’s a quick summary of all the different ways you can useImagePresentationComponent to present a 2D image,a spatial photo, or a spatial scene.

For more information on this component,check out the “Presenting images in RealityKit” Sample Codeon developer.apple.com.

Another immersive media update this year is that VideoPlayerComponent has beenupdated to support the playback of a wide range of immersive video formats!It now supports spatial video playback with full spatial styling, in both portaland immersive modes.

Apple Projected Media Profile videos such as 180 degree, 360 degree,and wide-field-of-view videos are also supported!People can also configure comfort settings for Apple Projected Media Profile videosand RealityKit will automatically adjust playback to accommodate.

These video formats, in addition to Apple Immersive Video, can be configuredto be played in a variety of viewing modes.For a deeper look into these updates, please check out the “Support immersivevideo playback in visionOS apps” session.

Next I’ll discuss some of our other updates this year.First, I'll introduce tracked Spatial Accessories.

Next, I'll go over the latest updates with SwiftUI and RealityKit integration.

After that, I'll cover the new entity updates.I'll give an overview of RealityKit's AVIF texture support.

Then I'll discuss the new hover effect groupID feature.

And finally I'll talk about the additionof post processing effects to RealityViews.Let's begin.RealityKit is adding support for tracking spatial accessories that allow youto interact with your apps in both Shared space and Full space.You can track spatial accessories in six degrees of freedom, and they also supporthaptics to enhance the interactions in your apps and games.To learn more about how to add spatial accessory input to your apps,watch the “Explore spatial accessory input on visionOS” session.This year, RealityKit is also introducing some brand new components to allow forbetter SwiftUI integration.The ViewAttachmentComponent makes it very simpleto add SwiftUI views directly to your entities.Additionally, the PresentationComponent enables you to add modal presentations,like popovers, to entitiesAlso, the new GestureComponent simplifies the process of adding SwiftUIgestures to entities.

Please check out the "Better together: SwiftUI and RealityKit" session for moreinformation on what’s new with SwiftUI and RealityKit integration this year!There is also a new entity attach method that allows you to attach one entityto the pin of another entity.This API greatly simplifies attaching meshes to the jointsof an animated skeleton.

Attaching meshes this way will avoid having to manually align the meshesand will also avoid expensive hierarchical transform updates.Additionally, there is a new Entity initializer that allows you to loadentities from in memory Data objects.

With this new initializer you can load entire RealityKit scenes or USDs froman online source or stream them over the network.The new initializer supports the same file formatsas the existing entity initializers.

Furthermore, RealityKit is adding support for AVIF encoded textures,which offer quality similar to jpeg with support for 10 bit colorswhile being significantly smaller in size.You can use the Preview app on your Mac or usdcrush in the Terminal to export USDswith this compression enabled.Also, the HoverEffectComponent is receiving a new feature: GroupIDs.GroupIDs are a way to create associations between hover effects.Any hover effects that share a GroupID will also share activations.

Assigning hover effects a GroupID will give you complete control of howa hover effect is activated irrespective of their relative hierarchy to each other.Normally hover effects are applied hierarchically like in the exampleon the left where child entities inherit the effects of their parent entities.However, if an entity has a GroupID like Entity A and Entity B in the exampleon the right, then they will not propagate their effects to their children.

Another cool addition this year is supportfor post processing effects in RealityView.You can use the customPostProcessing API to add custom effects,like bloom, to your apps using Metal Performance Shaders, CIFilters,or your own shaders.This API is supported on iOS, iPadOS, macOS, and tvOS.

This year’s RealityKit update is focused on making the creation of 3D experienceswith RealityKit easier than ever.I went over how you can create a spatial puzzle game that uses some of the newRealityKit APIs to anchor the game to your environmentand to enable intuitive ways to interact with the game pieces.

I also discussed the new immersive media updates that enable your apps to displayspatial content directly in RealityKit.

Then I covered some additional updates such as spatial accessory tracking,entity updates, and hover effect groupsIDs.With these new features it’s easier than ever to build 3D apps with RealityKitand I’m so excited to see what experiences you come up with.Thanks for watching.

4:33 -Set up SpatialTrackingSession

4:34 -Set up PlaneAnchor

5:48 -Handle DidAnchor event

7:38 -Set up ManipulationComponent

9:28 -Subscribe to willBegin ManipulationEvent

9:29 -Subscribe to willEnd ManipulationEvent

10:52 -Set up Scene understanding mesh collision and physics​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​

11:56 -Set up EnvironmentBlendingComponent

14:20 -Set up MeshInstancesComponent

17:36 -Load and display a 2D photo

17:57 -Load and display a spatial photo with windowed presentation

18:22 -Load and display a spatial photo with immserive presentation

18:56 -Load a spatial photo and use it to generate and present a spatial scene

20:06 -Generating a spatial scene as needed

23:35 -Load entity from Data object​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​

0:00 -IntroductionThe RealityKit framework, introduced in 2019, enables developers to create immersive 3D apps and games across various Apple platforms, including visionOS, iOS, iPadOS, macOS, and now tvOS. 

Building on feedback, the latest update introduces several new features, such as direct access to ARKit data, the 'ManipulationComponent' for adding interactions to 3D entities, the 'EnvironmentBlendingComponent' for seamless real-world occlusion, and the 'MeshInstancesComponent' for efficient rendering of multiple 3D models.

These features are demonstrated through the creation of a spatial puzzle game where players interact with objects to unlock a chest, showcasing how RealityKit enhances the blending of virtual and real-world experiences.3:19 -Anchoring updatesWith this new API, you can directly access ARKit data through RealityKit, enhancing the power of 'AnchorEntities'. By creating a 'SpatialTrackingSession' with plane tracking enabled, you can set up 'AnchorEntities' to filter specific real-world surfaces, such as tables with minimum dimensions.  

When the system detects a matching surface, it triggers an 'AnchorStateEvent', providing ARKit data, like transform and extents. This data can then be used to position entities accurately on the anchored surface, ensuring they are centered and aligned to the viewer.6:52 -ManipulationComponentIn the immersive space, game objects spawn on suitable surfaces. The 'ManipulationComponent' in RealityKit enables people to pick up, rotate, and swap hands with these 3D entities.  

Calling the 'configureEntity' function automatically adds the necessary components. To prevent objects from resetting after release, set the `releaseBehavior` property to 'stay'.

An added 'PhysicsBodyComponent' makes objects fall to the floor. The 'ManipulationEvents' API is used to toggle gravity on and off; gravity is turned on when someone releases an object (`WillEnd` event) and turned off when it's picked up (`WillBegin` event), allowing for realistic interactions while maintaining control during manipulation.10:01 -Scene understandingIn visionOS, with RealityKit's Scene Understanding API, you can integrate the real-world environment into a game's physics simulation. By utilizing the 'SpatialTrackingSession' API, you can generate a mesh of the surroundings. For game objects to collide with this real-world mesh, the 'SceneUnderstandingFlags' for collision and physics need to be set in the 'SpatialTrackingSession' configuration before starting the session. This update allows game objects to interact realistically with the physical environment, such as dropping them on tables or floors.11:18 -EnvironmentBlendingComponentWith the latest RealityKit update, you can use the 'EnvironmentBlendingComponent' in your immersive space apps to occlude virtual entities  with the real world surroundings. When you add this component and set the blending mode, then entities can be realistically occluded by static real-world objects, creating a more seamless experience.  However, it's important to note that these entities are always drawn behind other virtual objects in the scene and won't be occluded by dynamic moving objects, like people or pets.12:26 -MeshInstancesComponentThe latest RealityKit update also introduces the 'MeshInstancesComponent', which enhances rendering efficiency by allowing you to draw a single mesh multiple times within an entity. This new component replaces the need to clone entities, significantly reducing memory and processing footprint.    

By utilizing the 'MeshInstancesComponent', you can provide a list of transforms for each mesh instance, enabling unique appearances through 'CustomMaterials' on supported platforms. This method improves performance by optimizing GPU data transfer.

When using this component for large areas, for proper culling, break it into smaller entities. There's a demonstration of how to initialize and populate the 'MeshInstancesComponent' with randomized transforms, creating a playable area with decorations.

Together these APIs, namely, 'AnchorStateEvent', 'ManipulationComponent', sceneUnderstanding flags, and 'EnvironmentBlendingComponent' — together enable interactive, real-world-blending AR experiences.16:28 -Immersive mediaRealityKit is receiving significant updates this year, enhancing its support for immersive media. With a new component, 'ImagePresentationComponent', you can display three types of images: traditional 2D images, spatial photos, and spatial scenes. 

Spatial scenes are a notable addition; they're 3D images generated from 2D photos, creating a diorama-like effect with motion parallax, making static images more engaging. To present spatial scenes, you generate them from a 2D image or spatial photo using the 'Spatial3DImage' class, and then set the desired viewing mode. 

The 'VideoPlayerComponent' is also updated to support various immersive video formats, including spatial videos, Apple Projected Media Profile videos such as 180° and 360° videos, and Apple Immersive Video. You can play these videos in different viewing modes and you can configure comfort settings for Apple Projected Media Profile videos.21:43 -Accessories and moreThis year's RealityKit update significantly enhances 3D app development. Key updates include the following.

Spatial accessories: RealityKit now supports tracking spatial accessories in six degrees of freedom with haptics, enabling more immersive interactions in Shared and Full space.

SwiftUI integration: New components like 'ViewAttachmentComponent', 'PresentationComponent', and 'GestureComponent' simplify adding SwiftUI views, modal presentations, and gestures to entities.

Entity updates: A new entity attach method streamlines attaching meshes to animated skeletons, and an initializer allows loading entities from in-memory data objects, supporting online scene and USD streaming. 

Texture and hover effect support: RealityKit now supports AVIF encoded textures for smaller file sizes without compromising quality. The 'HoverEffectComponent' is updated with 'GroupID's, enabling more control over hover effect activations. 

Post-processing effects: 'RealityViews' now support post-processing effects, allowing you to add custom effects like bloom using Metal Performance Shaders, 'CIFilter's, or custom shaders, across iOS, iPadOS, macOS, and tvOS.

0:00 -Introduction

The RealityKit framework, introduced in 2019, enables developers to create immersive 3D apps and games across various Apple platforms, including visionOS, iOS, iPadOS, macOS, and now tvOS. 

Building on feedback, the latest update introduces several new features, such as direct access to ARKit data, the 'ManipulationComponent' for adding interactions to 3D entities, the 'EnvironmentBlendingComponent' for seamless real-world occlusion, and the 'MeshInstancesComponent' for efficient rendering of multiple 3D models.

These features are demonstrated through the creation of a spatial puzzle game where players interact with objects to unlock a chest, showcasing how RealityKit enhances the blending of virtual and real-world experiences.

The RealityKit framework, introduced in 2019, enables developers to create immersive 3D apps and games across various Apple platforms, including visionOS, iOS, iPadOS, macOS, and now tvOS. 

Building on feedback, the latest update introduces several new features, such as direct access to ARKit data, the 'ManipulationComponent' for adding interactions to 3D entities, the 'EnvironmentBlendingComponent' for seamless real-world occlusion, and the 'MeshInstancesComponent' for efficient rendering of multiple 3D models.

These features are demonstrated through the creation of a spatial puzzle game where players interact with objects to unlock a chest, showcasing how RealityKit enhances the blending of virtual and real-world experiences.

3:19 -Anchoring updates

With this new API, you can directly access ARKit data through RealityKit, enhancing the power of 'AnchorEntities'. By creating a 'SpatialTrackingSession' with plane tracking enabled, you can set up 'AnchorEntities' to filter specific real-world surfaces, such as tables with minimum dimensions.  

When the system detects a matching surface, it triggers an 'AnchorStateEvent', providing ARKit data, like transform and extents. This data can then be used to position entities accurately on the anchored surface, ensuring they are centered and aligned to the viewer.

With this new API, you can directly access ARKit data through RealityKit, enhancing the power of 'AnchorEntities'. By creating a 'SpatialTrackingSession' with plane tracking enabled, you can set up 'AnchorEntities' to filter specific real-world surfaces, such as tables with minimum dimensions.  

When the system detects a matching surface, it triggers an 'AnchorStateEvent', providing ARKit data, like transform and extents. This data can then be used to position entities accurately on the anchored surface, ensuring they are centered and aligned to the viewer.

6:52 -ManipulationComponent

In the immersive space, game objects spawn on suitable surfaces. The 'ManipulationComponent' in RealityKit enables people to pick up, rotate, and swap hands with these 3D entities.  

Calling the 'configureEntity' function automatically adds the necessary components. To prevent objects from resetting after release, set the `releaseBehavior` property to 'stay'.

An added 'PhysicsBodyComponent' makes objects fall to the floor. The 'ManipulationEvents' API is used to toggle gravity on and off; gravity is turned on when someone releases an object (`WillEnd` event) and turned off when it's picked up (`WillBegin` event), allowing for realistic interactions while maintaining control during manipulation.

In the immersive space, game objects spawn on suitable surfaces. The 'ManipulationComponent' in RealityKit enables people to pick up, rotate, and swap hands with these 3D entities.  

Calling the 'configureEntity' function automatically adds the necessary components. To prevent objects from resetting after release, set the `releaseBehavior` property to 'stay'.

An added 'PhysicsBodyComponent' makes objects fall to the floor. The 'ManipulationEvents' API is used to toggle gravity on and off; gravity is turned on when someone releases an object (`WillEnd` event) and turned off when it's picked up (`WillBegin` event), allowing for realistic interactions while maintaining control during manipulation.

10:01 -Scene understanding

In visionOS, with RealityKit's Scene Understanding API, you can integrate the real-world environment into a game's physics simulation. By utilizing the 'SpatialTrackingSession' API, you can generate a mesh of the surroundings. For game objects to collide with this real-world mesh, the 'SceneUnderstandingFlags' for collision and physics need to be set in the 'SpatialTrackingSession' configuration before starting the session. This update allows game objects to interact realistically with the physical environment, such as dropping them on tables or floors.

In visionOS, with RealityKit's Scene Understanding API, you can integrate the real-world environment into a game's physics simulation. By utilizing the 'SpatialTrackingSession' API, you can generate a mesh of the surroundings. For game objects to collide with this real-world mesh, the 'SceneUnderstandingFlags' for collision and physics need to be set in the 'SpatialTrackingSession' configuration before starting the session. This update allows game objects to interact realistically with the physical environment, such as dropping them on tables or floors.

11:18 -EnvironmentBlendingComponent

With the latest RealityKit update, you can use the 'EnvironmentBlendingComponent' in your immersive space apps to occlude virtual entities  with the real world surroundings. When you add this component and set the blending mode, then entities can be realistically occluded by static real-world objects, creating a more seamless experience.  However, it's important to note that these entities are always drawn behind other virtual objects in the scene and won't be occluded by dynamic moving objects, like people or pets.

With the latest RealityKit update, you can use the 'EnvironmentBlendingComponent' in your immersive space apps to occlude virtual entities  with the real world surroundings. When you add this component and set the blending mode, then entities can be realistically occluded by static real-world objects, creating a more seamless experience.  However, it's important to note that these entities are always drawn behind other virtual objects in the scene and won't be occluded by dynamic moving objects, like people or pets.

12:26 -MeshInstancesComponent

The latest RealityKit update also introduces the 'MeshInstancesComponent', which enhances rendering efficiency by allowing you to draw a single mesh multiple times within an entity. This new component replaces the need to clone entities, significantly reducing memory and processing footprint.    

By utilizing the 'MeshInstancesComponent', you can provide a list of transforms for each mesh instance, enabling unique appearances through 'CustomMaterials' on supported platforms. This method improves performance by optimizing GPU data transfer.

When using this component for large areas, for proper culling, break it into smaller entities. There's a demonstration of how to initialize and populate the 'MeshInstancesComponent' with randomized transforms, creating a playable area with decorations.

Together these APIs, namely, 'AnchorStateEvent', 'ManipulationComponent', sceneUnderstanding flags, and 'EnvironmentBlendingComponent' — together enable interactive, real-world-blending AR experiences.

The latest RealityKit update also introduces the 'MeshInstancesComponent', which enhances rendering efficiency by allowing you to draw a single mesh multiple times within an entity. This new component replaces the need to clone entities, significantly reducing memory and processing footprint.    

By utilizing the 'MeshInstancesComponent', you can provide a list of transforms for each mesh instance, enabling unique appearances through 'CustomMaterials' on supported platforms. This method improves performance by optimizing GPU data transfer.

When using this component for large areas, for proper culling, break it into smaller entities. There's a demonstration of how to initialize and populate the 'MeshInstancesComponent' with randomized transforms, creating a playable area with decorations.

Together these APIs, namely, 'AnchorStateEvent', 'ManipulationComponent', sceneUnderstanding flags, and 'EnvironmentBlendingComponent' — together enable interactive, real-world-blending AR experiences.

16:28 -Immersive media

RealityKit is receiving significant updates this year, enhancing its support for immersive media. With a new component, 'ImagePresentationComponent', you can display three types of images: traditional 2D images, spatial photos, and spatial scenes. 

Spatial scenes are a notable addition; they're 3D images generated from 2D photos, creating a diorama-like effect with motion parallax, making static images more engaging. To present spatial scenes, you generate them from a 2D image or spatial photo using the 'Spatial3DImage' class, and then set the desired viewing mode. 

The 'VideoPlayerComponent' is also updated to support various immersive video formats, including spatial videos, Apple Projected Media Profile videos such as 180° and 360° videos, and Apple Immersive Video. You can play these videos in different viewing modes and you can configure comfort settings for Apple Projected Media Profile videos.

RealityKit is receiving significant updates this year, enhancing its support for immersive media. With a new component, 'ImagePresentationComponent', you can display three types of images: traditional 2D images, spatial photos, and spatial scenes. 

Spatial scenes are a notable addition; they're 3D images generated from 2D photos, creating a diorama-like effect with motion parallax, making static images more engaging. To present spatial scenes, you generate them from a 2D image or spatial photo using the 'Spatial3DImage' class, and then set the desired viewing mode. 

The 'VideoPlayerComponent' is also updated to support various immersive video formats, including spatial videos, Apple Projected Media Profile videos such as 180° and 360° videos, and Apple Immersive Video. You can play these videos in different viewing modes and you can configure comfort settings for Apple Projected Media Profile videos.

21:43 -Accessories and more

This year's RealityKit update significantly enhances 3D app development. Key updates include the following.

Spatial accessories: RealityKit now supports tracking spatial accessories in six degrees of freedom with haptics, enabling more immersive interactions in Shared and Full space.

SwiftUI integration: New components like 'ViewAttachmentComponent', 'PresentationComponent', and 'GestureComponent' simplify adding SwiftUI views, modal presentations, and gestures to entities.

Entity updates: A new entity attach method streamlines attaching meshes to animated skeletons, and an initializer allows loading entities from in-memory data objects, supporting online scene and USD streaming. 

Texture and hover effect support: RealityKit now supports AVIF encoded textures for smaller file sizes without compromising quality. The 'HoverEffectComponent' is updated with 'GroupID's, enabling more control over hover effect activations. 

Post-processing effects: 'RealityViews' now support post-processing effects, allowing you to add custom effects like bloom using Metal Performance Shaders, 'CIFilter's, or custom shaders, across iOS, iPadOS, macOS, and tvOS.

This year's RealityKit update significantly enhances 3D app development. Key updates include the following.

Spatial accessories: RealityKit now supports tracking spatial accessories in six degrees of freedom with haptics, enabling more immersive interactions in Shared and Full space.

SwiftUI integration: New components like 'ViewAttachmentComponent', 'PresentationComponent', and 'GestureComponent' simplify adding SwiftUI views, modal presentations, and gestures to entities.

Entity updates: A new entity attach method streamlines attaching meshes to animated skeletons, and an initializer allows loading entities from in-memory data objects, supporting online scene and USD streaming. 

Texture and hover effect support: RealityKit now supports AVIF encoded textures for smaller file sizes without compromising quality. The 'HoverEffectComponent' is updated with 'GroupID's, enabling more control over hover effect activations. 

Post-processing effects: 'RealityViews' now support post-processing effects, allowing you to add custom effects like bloom using Metal Performance Shaders, 'CIFilter's, or custom shaders, across iOS, iPadOS, macOS, and tvOS.

## Code Samples

```swift
// Set up SpatialTrackingSession


@State
 
var
 spatialTrackingSession 
=
 
SpatialTrackingSession
()


RealityView
 { content 
in

             
    
let
 configuration 
=
 
SpatialTrackingSession
.
Configuration
(
        tracking: [.plane]
    )
		
// Run the configuration

    
if
 
let
 unavailableCapabilities 
=
 
await
 spatialTrackingSession.run(configuration) {
        
// Handle errors

    }
}
```

```swift
// Set up PlaneAnchor


RealityView
 { content 
in


		
// Set up the SpatialTrackingSession


    
// Add a PlaneAnchor

    
let
 planeAnchor 
=
 
AnchorEntity
(.plane(.horizontal,
                                          classification: .table,
                                          minimumBounds: [
0.15
, 
0.15
]))
    content.add(planeAnchor)
}
```

```swift
// Handle DidAnchor event


		didAnchor 
=
 content.subscribe(to: 
AnchorStateEvents
.
DidAnchor
.
self
) { event 
in


		
guard
 
let
 anchorComponent 
=

           event.entity.components[
ARKitAnchorComponent
.
self
] 
else
 { 
return
 }


		
guard
 
let
 planeAnchor 
=
 anchorComponent.anchor 
as?
 
PlaneAnchor
 
else
 { 
return
 }

		
let
 worldSpaceFromExtent 
=

    planeAnchor.originFromAnchorTransform 
*

    planeAnchor.geometry.extent.anchorFromExtentTransform

    gameRoot.transform 
=
 
Transform
(matrix: worldSpaceFromExtent)

    
// Add game objects to gameRoot 

}
```

```swift
// Set up ManipulationComponent


extension
 
Entity
 {
    
static
 
func
 
loadModelAndSetUp
(
modelName
: 
String
,
                                  
in
 
bundle
: 
Bundle
) 
async
 
throws
 -> 
Entity
 {

        
let
 entity 
=
 
// Load model and assign PhysicsBodyComponent

        
let
 shapes 
=
 
// Generate convex shape that fits the entity model


        
// Initialize manipulation

        
ManipulationComponent
.configureEntity(entity, collisionShapes: [shapes])
        
var
 manipulationComponent 
=
 
ManipulationComponent
()
        manipulationComponent.releaseBehavior 
=
 .stay
        entity.components.set(manipulationComponent)

        
// Continue entity set up

    }
}
```

```swift
// Subscribe to ManipulationEvents



// Update the PhysicsBodyComponent to support movement

willBegin 
=
 content.subscribe(to: 
ManipulationEvents
.
WillBegin
.
self
) { event 
in

    
if
 
var
 physicsBody 
=
 event.entity.components[
PhysicsBodyComponent
.
self
] {
        physicsBody.mode 
=
 .kinematic
        event.entity.components.set(physicsBody)
    }
}
```

```swift
// Subscribe to ManipulationEvents

                

// Update the PhysicsBodyComponent to be a dynamic object

willEnd 
=
 content.subscribe(to: 
ManipulationEvents
.
WillEnd
.
self
) { event 
in

    
if
 
var
 physicsBody 
=
 event.entity.components[
PhysicsBodyComponent
.
self
] {
        physicsBody.mode 
=
 .dynamic
        event.entity.components.set(physicsBody)
    }
}
```

```swift
// Set up Scene understanding mesh collision/physics



let
 configuration 
=
 
SpatialTrackingSession
.
Configuration
(
    tracking: [.plane],
    sceneUnderstanding: [.collision, .physics]
)
```

```swift
// Set up EnvironmentBlendingComponent


entity.components.set(
    
EnvironmentBlendingComponent
(preferredBlendingMode: .occluded(by: .surroundings))​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​
)
```

```swift
// Set up MeshInstancesComponent entity



let
 entity 
=
 
try
 
await
 
ModelEntity
(named:
"PebbleStriped.usdz"
)

var
 meshInstancesComponent 
=
 
MeshInstancesComponent
()

let
 instances 
=
 
try
 
LowLevelInstanceData
(instanceCount: 
20
)
meshInstancesComponent[partIndex: 
0
] 
=
 instances
    
instances.withMutableTransforms { transforms 
in

    
for
 i 
in
 
0
..<
20
 { 
        
let
 scale: 
Float
 
=
 .random(in:
0.018
...
0.025
)
        
let
 angle: 
Float
 
=
 .random(in:
0
..<
2
) 
*
 .pi
        
let
 position 
=
 randomPoint(in: inArea, with: scene)
        
let
 transform 
=
 
Transform
(scale: .
init
(repeating: scale),
                                  rotation: .
init
(angle: angle,axis: [
0
, 
1
, 
0
]),
                                  translation: position)
        transforms[i] 
=
 transform.matrix
    }
}
        
entity.components.set(meshInstancesComponent)
```

```swift
// Load and display a 2D photo

    

guard
 
let
 url 
=
 
Bundle
.main.url(forResource: 
"my2DPhoto"
, withExtension: 
"heic"
) 
else
 {a​​​​​​​​​​​​​​​​​​​​​​​​​​
    
return

}


let
 component 
=
 
try
 
await
 
ImagePresentationComponent
(contentsOf: url)


let
 entity 
=
 
Entity
()
entity.components.set(component)
```

```swift
// Load and display a spatial photo with windowed presentation

    

guard
 
let
 url 
=
 
Bundle
.main.url(forResource: 
"mySpatialPhoto"
, withExtension: 
"heic"
) 
else
 {
    
return

}


var
 component 
=
 
try
 
await
 
ImagePresentationComponent
(contentsOf: url)


// Discover if the component supports windowed spatial photo presentation.


if
 component.availableViewingModes.contains(.spatialStereo) {
    component.desiredViewingMode 
=
 .spatialStereo
}

entity.components.set(component)
```

```swift
// Load and display a spatial photo with immersive presentation

    

guard
 
let
 url 
=
 
Bundle
.main.url(forResource: 
"mySpatialPhoto"
, withExtension: 
"heic"
) 
else
 {
    
return

}


var
 component 
=
 
try
 
await
 
ImagePresentationComponent
(contentsOf: url)


// Discover if the component supports immersive spatial photo presentation.


if
 component.availableViewingModes.contains(.spatialStereoImmersive) {
    component.desiredViewingMode 
=
 .spatialStereoImmersive
}

entity.components.set(component)
```

```swift
// Load a spatial photo and use it to generate and present a spatial scene

    

guard
 
let
 url 
=
 
Bundle
.main.url(forResource: 
"mySpatialPhoto"
, withExtension: 
"heic"
) 
else
 {
    
return

}


let
 spatial3DImage 
=
 
try
 
await
 
ImagePresentationComponent
.
Spatial3DImage
(contentsOf: url)

var
 component 
=
 
ImagePresentationComponent
(spatial3DImage: spatial3DImage)


try
 
await
 spatial3DImage.generate()


// Discover if the component supports windowed spatial scene presentation.


if
 component.availableViewingModes.contains(.spatial3D) {
    component.desiredViewingMode 
=
 .spatial3D
}

entity.components.set(component)
```

```swift
// Load a spatial photo and use it to generate and present a spatial scene

    

guard
 
let
 url 
=
 
Bundle
.main.url(forResource: 
"mySpatialPhoto"
, withExtension: 
"heic"
) 
else
 {
    
return

}


let
 spatial3DImage 
=
 
try
 
await
 
ImagePresentationComponent
.
Spatial3DImage
(contentsOf: url)

var
 component 
=
 
ImagePresentationComponent
(spatial3DImage: spatial3DImage)

component.desiredViewingMode 
=
 .spatial3D 
// (or .spatial3DImmersive)


entity.components.set(component)


try
 
await
 spatial3DImage.generate()
```

```swift
// Load entity from Data object

    

if
 
let
 (data, response) 
=
 
try?
 
await
 
URLSession
.shared.data(from: url) {
    
if
 
let
 entity 
=
 
try?
 
await
 
Entity
(from: data) {
        content.add(entity)
    }
}
```

