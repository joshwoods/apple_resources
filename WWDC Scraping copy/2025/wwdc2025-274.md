# Wwdc2025 274

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Transcript

Code

Better together: SwiftUI and RealityKitDiscover how to seamlessly blend SwiftUI and RealityKit in visionOS 26. We'll explore enhancements to Model3D, including animation and ConfigurationCatalog support, and demonstrate smooth transitions to RealityView. You'll learn how to leverage SwiftUI animations to drive RealityKit component changes, implement interactive manipulation, use new SwiftUI components for richer interactions, and observe RealityKit changes from your SwiftUI code. We'll also cover how to use unified coordinate conversion for cross-framework coordinate transformations.Chapters0:00 -Introduction1:24 -Model3D enhancements6:13 -RealityView transition11:52 -Object manipulation15:35 -SwiftUI components19:08 -Information flow24:56 -Unified coordinate conversion27:01 -Animation29:41 -Next stepsResourcesCanyon Crosser: Building a volumetric hike-planning appRendering hover effects in Metal immersive appsHD VideoSD VideoRelated VideosWWDC24Compose interactive 3D content in Reality Composer ProWWDC23Discover Observation in SwiftUIWWDC21Dive into RealityKit 2WWDC20Data Essentials in SwiftUI

Discover how to seamlessly blend SwiftUI and RealityKit in visionOS 26. We'll explore enhancements to Model3D, including animation and ConfigurationCatalog support, and demonstrate smooth transitions to RealityView. You'll learn how to leverage SwiftUI animations to drive RealityKit component changes, implement interactive manipulation, use new SwiftUI components for richer interactions, and observe RealityKit changes from your SwiftUI code. We'll also cover how to use unified coordinate conversion for cross-framework coordinate transformations.

0:00 -Introduction

1:24 -Model3D enhancements

6:13 -RealityView transition

11:52 -Object manipulation

15:35 -SwiftUI components

19:08 -Information flow

24:56 -Unified coordinate conversion

27:01 -Animation

29:41 -Next steps

Canyon Crosser: Building a volumetric hike-planning app

Rendering hover effects in Metal immersive apps

HD VideoSD Video

HD Video

SD Video

Compose interactive 3D content in Reality Composer Pro

Discover Observation in SwiftUI

Dive into RealityKit 2

Data Essentials in SwiftUI

Search this video…Hi. I'm Amanda, and I'm a RealityKit engineer.And I'm Maks.I'm a SwiftUI engineer.Today we'll share some great enhancements to both SwiftUI and RealityKitthat help them work even better together!Check out this adorable scene!We've got a charming SwiftUI robot, hovering in mid-air, and a groundedRealityKit robot – both yearning for connection.When they get close, sparks fly!But how can they get close enough to truly interact?Maks and I will share how to combine the worldsof traditional UI and interactive 3D content.First, I'll share some enhancements to Model3D.Then, I'll demonstrate how to transition from using Model3D to using RealityView,and talk about when to choose one versus the other.I'll tell you about the new Object Manipulation API.RealityKit gets new Component types, integrating more aspects of SwiftUI.Information can now flow both ways between SwiftUI and RealityKit - we'll explain.Coordinate space conversion is easier than ever.Drive RealityKit component changes with SwiftUI animations.Let's wire it up!Display 3D models in your apps with just one line of code using Model3D.In visionOS 26, two enhancements let you do even more with Model3D -playing animations, and loading from a ConfigurationCatalog.Since Model3D is a SwiftUI view, it participates in the SwiftUI layout system.I'll use that layout systemto make a little sign that displays the robot's name.Now the sign says that this robot's name is Sparky.Sparky also has some sweet dance moves!The artist bundled this animation with the robot model asset.New in visionOS 26 is the Model3DAsset type.Load and control animations on your 3D contentby constructing a Model3D using a Model3DAsset.The model loads the animations from the asset,and lets you choose which one to play."Model" is an overloaded term,especially in this session as we're converging a UI frameworkand a 3D game framework.In UI frameworks, a "model" refers to the data structurethat represents the information your app uses.The model holds the data and business logic,allowing the view to display this information.In 3D frameworks like RealityKit, a model refers to a 3D objectthat can be placed in a scene.You access it via the ModelComponent, which consists of a mesh resourcethat defines its shape, and materials that determine its appearance.Sometimes that happens.Two worlds collide, bringing their terminologies with them,and sometimes there's overlap.Now, back to Sparky and its animation.I'm placing the Model3D above a Picker, a Play button, and a time scrubber.In my RobotView, I'm displaying the animated robot itself,and under that, I'm placing a Picker to choose which animation to play,plus the animation playback controls.First, I'm initializing a Model3DAssetwith the scene name to load from my bundle.Then, once the asset is present, I pass it to the Model3D initializer.Underneath that in the VStack, I'm presenting a customized Pickerthat lists the animations that are available in this model asset.When an item is chosen from the list,the Picker sets the asset's `selectedAnimation` to the new value.Then the Model3DAsset creates an AnimationPlaybackControllerto control playback of that chosen animation.The asset vends an `animationPlaybackController`.Use this object to pause, resume, and seek in the animation.I'm passing that animationController into my RobotAnimationControls view,which we'll also look at shortly.In visionOS 26, the existing RealityKit class `AnimationPlaybackController`is now Observable.In my SwiftUI view, I observe the `time` propertyto display the animation's progress.I have a @Bindable property called `controller`,which means I'm using the`AnimationPlaybackController`as my view's data model.When the controller's isPlaying value changes,SwiftUI will re-evaluate my RobotAnimationControls view.I've got a Slider that shows the current time in the animation,relative to the total duration of the animation.You can drag this slider and it will scrub through the animation.Here's Sparky doing its celebration animation!I can fast forward and rewind using the Slider.Go Sparky, it's your birthday!With its dance moves down, Sparky wants to dress upbefore it heads to the greenhouse to meet the other robot there.I can help it do that with enhancements to RealityKit's ConfigurationCatalog type.This type stores alternatives representations of an entity,such as different mesh geometries, component values, or material properties.In visionOS 26, you can initialize a Model3Dwith a ConfigurationCatalogand switch between its various representations.To allow Sparky to try on different outfits,my artist bundled a reality file with several different body types.I load this file as a ConfigurationCatalog from my app's main Bundle.Then, I create my Model3D with the configuration.This popover presents the configuration options.Choosing from the popover changes Sparky's look.Dance moves?Check. Outfit?Check.Sparky is ready to meet its new friend in the RealityKit greenhouse.Sparks are going to fly!To make those sparks fly, I'll use a Particle Emitter.But - that’s not something I can do at runtime with the Model3D type.Particle Emitter is a component that I add to a RealityKit entity.More on that in a moment.Importantly, Model3D doesn't support adding components.So, to add a particle emitter, I'll switch to RealityView.I'll share how to smoothly replace my Model3D with a RealityViewwithout changing the layout.First, I switch the view from Model3D to RealityView.I load the botanist model from the app bundleinside the `make` closure of the RealityView,creating an entity.I add that entity to the contents of the RealityViewso Sparky appears on screen.But... now the name sign is pushed too far over to the side.That wasn't happening before when we were using a Model3D.It's happening now because, by default,RealityView takes up all the available spacethat the SwiftUI layout system gives it.By contrast, the Model3D sizes itselfbased on the intrinsic size of the underlying model file.I can fix this!I apply the new`.realityViewLayoutBehavior` modifier with `.fixedSize`to make the RealityView tightly wrap the model's initial bounds.Much better.RealityView will use the visual bounds of the entitiesin its contents to figure out its size.This sizing is only evaluated once - right after your `make` closure is run.The other optionsfor `realityViewLayoutBehavior` are .flexible and .centered.In all three of these RealityViews,I have the bottom of the Sparky model sitting on the origin of the scene,and I've marked that origin with a gizmo,the little multicolored cross showing the axes and origin.On the left, with the `.flexible` option,the RealityView acts as if it doesn't have the modifier applied.The origin remains in the center of the view.The `.centered` option moves the origin of the RealityViewso that the contents are centered in the view.`.fixedSize` makes the RealityView tightly wrap the contents' bounds,and makes your RealityView behave just like Model3D.None of these options re-position or scale your entitieswith respect to the RealityViewContent;they just re-position the RealityView's own origin point.I've sorted out Sparky's sizing in the RealityView.Next I'll get Sparky animating again.I'll move from Model3D's new animation APIto a RealityKit animation API directly on the Entity.For more detail on the many ways of workingwith animation in RealityKit,check out the session "Compose interactive 3D content in Reality Composer Pro".I switched from Model3Dto RealityView so I could give Sparky a ParticleEmitterComponent,because sparks need to fly when these two robots get close to each other.Particle Emitters let you make effectsthat involve hundreds of tiny particles animating at once,like fireworks, rain, and twinkles.RealityKit provides preset values for these,and you can adjust those presets to get the effect you're after.You can use Reality Composer Pro to design them, and you can configure them in code.You add the ParticleEmitter to an entity as a Component.Components are a central part of RealityKit,which is based on the "Entity Component System" paradigm.Each object in your scene is an Entity, and you add components to it to tell itwhat traits and behaviors it has.A Component is the type that holds data about an Entity.A System processes entities that have specific components,performing logic involving that data.There are built-in systems for things like animating particles,for handling physics, for rendering, and many more.You can write your own custom system in RealityKit to do customlogic for your game or app.Watch Dive into RealityKit 2 for a more in-depth lookat the Entity Component System in RealityKit.I'll add a particle emitter to each side of Sparky's head.First I make two invisible entitiesthat serve as containers for the sparks effect.I designed my sparks emitter to point to the right.I'll add it directly to my invisible entity on Sparky's right side.On the other side, I rotate the entity 180 degreesabout the y axis so it's pointing leftward.Putting it all together in the RealityView,here's Sparky with its animation,its name sign in the right position, and sparks flying!RealityKit is great for detailed creation like this!If you're making a game or play-oriented experience,or need fine-grained control over the behavior of your 3D content,choose RealityView.On the other hand, use Model3D to display a self-contained 3D asset on its own.Think of it like SwiftUI's Image view but for 3D assets.Model3D's new animation and configuration catalogs let you do more with Model3D.And if your design evolves and you need direct access to the entities, components,and systems, transition smoothlyfrom Model3D to RealityView using realityViewLayoutBehavior.Next I'll share details about the new Object ManipulationAPI in visionOS 26, which lets people pick up the virtual objects in your app!Object manipulation works from both SwiftUI and RealityKit.With Object manipulation you move the object with a single hand,rotate it with one or both hands,and scale it by pinching and dragging with both hands.You can even pass the object from one hand to the other.There are two ways to enable this,depending on whether the object is a RealityKit Entity or SwiftUI View.In SwiftUI, add the new `manipulable` modifier.To disallow scaling, but keep the ability to moveand rotate the robot with either hand,I specify what Operations are supported.To make the robot feel super heavy, I specify that it has high inertia.The .manipulable modifier works when Sparky is displayed in a Model3D view.It applies to the whole Model3D, or to any View it's attached to.When Sparky's in a RealityView, I want to enable manipulationon just the robot entity itself, not the whole RealityView.In visionOS 26, ManipulationComponent is a new type that you can set on an entityto enable Object Manipulation.The static function `configureEntity` adds the ManipulationComponent to your entity.It also adds a CollisionComponent so that the interaction system knowswhen you've tapped on this entity.It adds an InputTargetComponent which tells the systemthat this entity responds to gestures.And finally, it adds a HoverEffectComponent which applies a visual effectwhen a person looks at or hovers their mouse over it.This is the only line you need to enable manipulation of an entity in your scene.To customize the experience further, there are several parameters you can pass.Here, I'm specifying a purple spotlight effect.I'm allowing all types of input: direct touch and indirect gaze and pinch.And I'm supplying collision shapes that define the outer dimensions of the robot.To respond when a person interacts with an object in your app,the object manipulation system raises events at key moments,such as when the interaction starts and stops,gets updated as the entity is moved, rotated, and scaled, when it is released,and when it is handed off from one hand to another.Subscribe to these events to update your state.By default, standard sounds play when the interaction begins,a handoff occurs, or the object is released.To apply custom sounds,I first set the audioConfiguration to `none`.That disables the standard sounds.Then I subscribe to the ManipulationEvent DidHandOff,which is delivered when a person passes the robot from one hand to the other.In that closure, I play my own audio resource.Well, Maks.Sparky's journey has been exciting: animating in Model3D, finding its new homein a RealityView, showing its personality with sparks, and letting people reach outand interact with it.It's come a long way on its path towards the RealityKit greenhouse.It sure has!But for Sparky to truly connect with the robot waiting there,the objects in their virtual space need new capabilities.They need to respond to gestures,present information about themselves,and trigger actions in a way that feels native to SwiftUI.Sparky's journey toward the RealityKit greenhouseis all about building connection.Deep connection requires rich interactions.That's exactly what the new SwiftUI RealityKit componentsare designed to enable.The new components in visionOS 26 bring powerful,familiar SwiftUI capabilities directly to RealityKit entities.RealityKit introduces three key components:First, the ViewAttachmentComponent allows youto add SwiftUI views directly to your entities.Next, the GestureComponent makes your entities responsive to touch and gestures.And finally, the PresentationComponent, which presents SwiftUI views,like popovers, from within your RealityKit scene.visionOS 1 let you declare attachments up frontas part of the RealityView initializer.After evaluating your attachment view builder,the system called your update closure with the results as entities.You could add these entities to your scene and position them in 3D space.In visionOS 26, this is simplified.Now you create attachments using a RealityKit componentfrom anywhere in your app.Create your ViewAttachmentComponentby giving it any SwiftUI View.Then, add it to an entity's components collection.And just like that I moved our NameSign from SwiftUI to RealityKit.Let’s explore gestures next!You can already attach gestures to your RealityViewusing `targetedToEntity`gesture modifiers.New in visionOS 26 is GestureComponent.Just like ViewAttachmentComponent, you add GestureComponent to your entities directly,passing regular SwiftUI gestures to it.The gesture values are by default reported in the entity’s coordinate space.Super handy!I use GestureComponent with a tap gesture to toggle the name sign on and off.Check it out.This robot's name is... Bolts!Pro tip: on any entity that's the target of a gesture,also add both InputTargetComponent and CollisionComponent.This advice applies to both GestureComponentand the targeted gestures API.GestureComponent and ViewAttachmentComponentlet me create a name sign for Bolts.But, Bolts is getting ready for a special visitor: Sparky!Bolts wants to look its absolute best for their meeting in the greenhouse.Time for another outfit change!I'll replace Bolts' name sign with UI to pick what Bolts will wear.Truly, a momentous decision.To emphasize that, I'll show this UI in a popover,using PresentationComponent, directly from RealityKit.First, I replace `ViewAttachmentComponent` with `PresentationComponent.The component takes a boolean binding to control when the popover is presented,and to notify you when someone dismisses the popover.The `configuration` parameter is the type of presentation to be shown.I'm specifying `popover`.Inside the popover,I'll present a view with configuration catalog options to dress up Bolts.Now, I can help Bolts pick its best color for when Sparky comes to visit.Hey Maks, do you think Bolts is more of a summer?Or an autumn?That's a fashion joke.Bolts is dressed to impress.But first, it has to go to work.Bolts waters plants in the greenhouse.I'll make a mini map, like on a heads-up display in a game,to track Bolts' position in the greenhouse.For that, I need to observe the robot's Transform component.In visionOS 26, entities are now observable.They can notify other code when their properties change.To be notified, just read an entity’s "observable" property.From the “observable” property you can watch for changes to the entity's position,scale, and rotation, its Children collection,and even its Components, including your own custom components!Observe these properties directly using a `withObservationTracking` block.Or lean on SwiftUI's built-in observation tracking.I’ll use SwiftUI to implement my Minimap.To learn more about Observation, watch "Discover Observation in SwiftUI".In this view, I display my entity's position on a MiniMap.I'm accessing this observable value on my entity.This tells SwiftUI that my view depends on this value.As Bolts moves about the greenhouse,watering the plants, its position will change.Each time it does, SwiftUI will call my View's body again,moving its counterpart symbol in the minimap!For a deeper explanation of SwiftUI's data flow,check out the session "Data Essentials in SwiftUI."Our robot friends are really coming together!That's the dream!I liked your description of the difference between "model" and "model" earlier.And sometimes you need to pass data from your data modelto your 3D object model, and vice versa.In visionOS 26, observable entities give us a new tool to do that.Since the beginning, you could pass information from SwiftUI to RealityKitin the `update` closure of RealityView.Now with entity's `observable` property, you can send information the other way.RealityKit entities can act like model objectsto drive updates to your SwiftUI views!So information can flow both ways now:from SwiftUI to RealityKit and from RealityKit to SwiftUI.But...does this create the potential for an infinite loop?Yes!Let's look at how to avoid creating infinite loopsbetween SwiftUI and RealityKit.When you read an observable property inside the body of a view,you create a dependency; your view depends on that property.When the property’s value changes, SwiftUI will update the viewand re-run its body.RealityView has some special behavior.Think of its…update closure as an extension of the containing view's body.SwiftUI will call the closure whenever any of that view's state changes,not only when state that is explicitly observed in that closure changes.Here in my RealityView's update closure, I'm changing that position.This will write to the position value,which will cause SwiftUI to update the view and re-run its body,causing an infinite loop.To avoid creating an infinite loop don’t modify your observed statewithin your update closure.You are free to modify entities that you're not observing.That won't create an infinite loop because changes to themwon't trigger SwiftUI to re-evaluate the view body.If you do need to modify an observed property,check the existing value of that property and avoid writing that same value back.This breaks the cycle and avoids an infinite loop.Note that the RealityView's make closure is special.When you access an observable property in the make closure,that doesn't create a dependency.It's not included in the containing view's observation scope.Also, the `make` closure is not re-run on changes.It only runs when the containing view first appears.You can also update the propertieson an observed entity from within your own custom system.A system's update function is not inside the scopeof the SwiftUI view body evaluation,so this is a good place to change my observed entities' values.The closures of Gestures are also not inside the scopeof the SwiftUI view body evaluation.They are called in response to user input.You can modify your observed entities' values here, too.To recap, it's cool to modifyyour observed entities in some places, and not in others.If do you find that you have an infinite loop in your app,here's a tip for fixing it:Split up your larger views into smaller,self-contained views, each one having only their own necessary state.That way, a change in some unrelated entitywon't cause your small view to be re-evaluated.This is also great for performance!You know, Maks, you might find that you don't needto use your `update` closure at all anymore.Since your Entity can now be your view's state,you can modify it in the normal places that you're used to modifying state,and forgo the update closure altogether.Yeah!I feel like avoiding infinite loopsis something I have to learn over and over again.But, if I don't use an update closure, I'm less likely to run into one.I think it's time to bring Bolts and Sparky together.Bolts is finally done with work - time for its date with Sparky!As I pick up Sparky to bring it over, and the two robots get closer together,I want to make sparks fly as a function of the decreasing distance between them.I'll use our new Unified Coordinate Conversion API to enable this.Sparky is in a Model3D SwiftUI view now,and Bolts is an Entity in the RealityKit greenhouse.I need to get the absolute distance between these two robots,even though they're in different coordinate spaces.To solve this, the Spatial framework now defines a `CoordinateSpace3D` protocolthat represents an abstract coordinate space.You can easily convert values between any two typesthat conform to CoordinateSpace3D, even from different frameworks.RealityKit's `Entity` and `Scene` types conform to CoordinateSpace3D.On the SwiftUI side, GeometryProxy3D has a new .coordinateSpace3D() functionthat gives you its coordinate space.Additionally, several Gesture types can provide their values relativeto any given CoordinateSpace3D.CoordinateSpace3D protocol works by first converting a valuein Sparky’s coordinate space to a coordinate space sharedby both RealityKit and SwiftUI.After that, it converts from the shared space into Bolt’s coordinate space,while taking low-level details like points-to-meter conversionand axis direction into account.In Sparky's Model3D view, whenever the view geometry changes,the system calls my `onGeometryChange3D` function.It passes in a GeometryProxy3D which I use to get its coordinate space.Then, I can convert my view's position to a point in the entity's spaceso I know how far apart my two robots are from each other.Now as Amanda brings Bolts and Sparky together, the sparks increase.As she pulls them apart, the sparks decrease.Next, I'll teach these robots to move together and to coordinate their actions.I'll use SwiftUI driven animation for RealityKit components.SwiftUI already comes with great animation APIsto implicitly animate changes to your view properties.Here, I animate the Model3D view that Sparky is in.I send it over to the left when I toggle,and then it bounces back to the original position when I toggle again.I’m adding an animation to my `isOffset` binding,and I'm specifying that I want an extra bouncy animation for it.In visionOS 26, you can now use SwiftUI animationto implicitly animate changes to RealityKit components.All you need to do is set a supported componenton your entity within a RealityKit animation blockand the framework will take care of the rest.There are two ways to associate an animation with a state change.From within a RealityView, you can use `content.animate()`to set new values for your components inside the animate block.RealityKit will use the animation associated with the SwiftUI transactionthat triggered the `update` closure,which, in this case, is an extra bouncy animation.The other way is to call the new Entity.animate() function,passing a SwiftUI animation,and a closure that sets new values for your components.Here, whenever the isOffset property changes,I send Sparky left or right using the entity’s position.Setting the position inside the animate block will begin an implicit animationof the Transform component,causing the entity to move smoothly to the new position.The power of implicit animation really shines when I combine itwith the Object Manipulation API that Amanda introduced.I can use a SwiftUI animation to implement a custom release behavior for Bolts.I’m first going to disable the default release behavior for object manipulationby setting it to .stay.Then, I will subscribe to the WillRelease event for the manipulation interaction.And when the object is about to be released,I will snap Sparky back by setting its transform to identity,which resets the scale, translation, and rotation of the entity.Since I’m modifying Sparky’s transform inside the animate block,Sparky will bounce back to its default position.Now Sparky's animation back to its original position is much more fun!All these built-in RealityKit components support implicit animations,including the Transform, the Audio components,and the Model and Light components, which have color properties!Sparky and Bolts had quite a journey.It's so great to see the power of SwiftUI and RealityKit working together.With this connection,you're also empowered to develop truly exceptional spatial apps,fostering a real connection between the virtual and the physical!Imagine the possibilities as you seamlessly integrate SwiftUI componentsinto your RealityKit scenes,and as entities dynamically drive changes to your SwiftUI state.Just like Sparky and Bolts, we hope you're inspired to connect SwiftUIand RealityKit in ways we haven't even imagined yet.Let's build the future together!

Hi. I'm Amanda, and I'm a RealityKit engineer.And I'm Maks.I'm a SwiftUI engineer.Today we'll share some great enhancements to both SwiftUI and RealityKitthat help them work even better together!Check out this adorable scene!We've got a charming SwiftUI robot, hovering in mid-air, and a groundedRealityKit robot – both yearning for connection.When they get close, sparks fly!But how can they get close enough to truly interact?Maks and I will share how to combine the worldsof traditional UI and interactive 3D content.First, I'll share some enhancements to Model3D.

Then, I'll demonstrate how to transition from using Model3D to using RealityView,and talk about when to choose one versus the other.

I'll tell you about the new Object Manipulation API.

RealityKit gets new Component types, integrating more aspects of SwiftUI.

Information can now flow both ways between SwiftUI and RealityKit - we'll explain.

Coordinate space conversion is easier than ever.

Drive RealityKit component changes with SwiftUI animations.

Let's wire it up!Display 3D models in your apps with just one line of code using Model3D.In visionOS 26, two enhancements let you do even more with Model3D -playing animations, and loading from a ConfigurationCatalog.Since Model3D is a SwiftUI view, it participates in the SwiftUI layout system.I'll use that layout systemto make a little sign that displays the robot's name.

Now the sign says that this robot's name is Sparky.

Sparky also has some sweet dance moves!The artist bundled this animation with the robot model asset.New in visionOS 26 is the Model3DAsset type.Load and control animations on your 3D contentby constructing a Model3D using a Model3DAsset.

The model loads the animations from the asset,and lets you choose which one to play.

"Model" is an overloaded term,especially in this session as we're converging a UI frameworkand a 3D game framework.In UI frameworks, a "model" refers to the data structurethat represents the information your app uses.

The model holds the data and business logic,allowing the view to display this information.In 3D frameworks like RealityKit, a model refers to a 3D objectthat can be placed in a scene.You access it via the ModelComponent, which consists of a mesh resourcethat defines its shape, and materials that determine its appearance.Sometimes that happens.Two worlds collide, bringing their terminologies with them,and sometimes there's overlap.Now, back to Sparky and its animation.

I'm placing the Model3D above a Picker, a Play button, and a time scrubber.In my RobotView, I'm displaying the animated robot itself,and under that, I'm placing a Picker to choose which animation to play,plus the animation playback controls.

First, I'm initializing a Model3DAssetwith the scene name to load from my bundle.

Then, once the asset is present, I pass it to the Model3D initializer.Underneath that in the VStack, I'm presenting a customized Pickerthat lists the animations that are available in this model asset.When an item is chosen from the list,the Picker sets the asset's `selectedAnimation` to the new value.Then the Model3DAsset creates an AnimationPlaybackControllerto control playback of that chosen animation.

The asset vends an `animationPlaybackController`.Use this object to pause, resume, and seek in the animation.

I'm passing that animationController into my RobotAnimationControls view,which we'll also look at shortly.

In visionOS 26, the existing RealityKit class `AnimationPlaybackController`is now Observable.In my SwiftUI view, I observe the `time` propertyto display the animation's progress.

I have a @Bindable property called `controller`,which means I'm using the`AnimationPlaybackController`as my view's data model.

When the controller's isPlaying value changes,SwiftUI will re-evaluate my RobotAnimationControls view.I've got a Slider that shows the current time in the animation,relative to the total duration of the animation.You can drag this slider and it will scrub through the animation.Here's Sparky doing its celebration animation!I can fast forward and rewind using the Slider.Go Sparky, it's your birthday!With its dance moves down, Sparky wants to dress upbefore it heads to the greenhouse to meet the other robot there.I can help it do that with enhancements to RealityKit's ConfigurationCatalog type.This type stores alternatives representations of an entity,such as different mesh geometries, component values, or material properties.

In visionOS 26, you can initialize a Model3Dwith a ConfigurationCatalogand switch between its various representations.

To allow Sparky to try on different outfits,my artist bundled a reality file with several different body types.I load this file as a ConfigurationCatalog from my app's main Bundle.Then, I create my Model3D with the configuration.

This popover presents the configuration options.Choosing from the popover changes Sparky's look.

Dance moves?Check. Outfit?Check.Sparky is ready to meet its new friend in the RealityKit greenhouse.Sparks are going to fly!To make those sparks fly, I'll use a Particle Emitter.But - that’s not something I can do at runtime with the Model3D type.Particle Emitter is a component that I add to a RealityKit entity.More on that in a moment.Importantly, Model3D doesn't support adding components.

So, to add a particle emitter, I'll switch to RealityView.I'll share how to smoothly replace my Model3D with a RealityViewwithout changing the layout.First, I switch the view from Model3D to RealityView.

I load the botanist model from the app bundleinside the `make` closure of the RealityView,creating an entity.

I add that entity to the contents of the RealityViewso Sparky appears on screen.

But... now the name sign is pushed too far over to the side.That wasn't happening before when we were using a Model3D.

It's happening now because, by default,RealityView takes up all the available spacethat the SwiftUI layout system gives it.By contrast, the Model3D sizes itselfbased on the intrinsic size of the underlying model file.I can fix this!I apply the new`.realityViewLayoutBehavior` modifier with `.fixedSize`to make the RealityView tightly wrap the model's initial bounds.

Much better.

RealityView will use the visual bounds of the entitiesin its contents to figure out its size.

This sizing is only evaluated once - right after your `make` closure is run.

The other optionsfor `realityViewLayoutBehavior` are .flexible and .centered.

In all three of these RealityViews,I have the bottom of the Sparky model sitting on the origin of the scene,and I've marked that origin with a gizmo,the little multicolored cross showing the axes and origin.

On the left, with the `.flexible` option,the RealityView acts as if it doesn't have the modifier applied.The origin remains in the center of the view.

The `.centered` option moves the origin of the RealityViewso that the contents are centered in the view.`.fixedSize` makes the RealityView tightly wrap the contents' bounds,and makes your RealityView behave just like Model3D.

None of these options re-position or scale your entitieswith respect to the RealityViewContent;they just re-position the RealityView's own origin point.I've sorted out Sparky's sizing in the RealityView.Next I'll get Sparky animating again.

I'll move from Model3D's new animation APIto a RealityKit animation API directly on the Entity.

For more detail on the many ways of workingwith animation in RealityKit,check out the session "Compose interactive 3D content in Reality Composer Pro".I switched from Model3Dto RealityView so I could give Sparky a ParticleEmitterComponent,because sparks need to fly when these two robots get close to each other.

Particle Emitters let you make effectsthat involve hundreds of tiny particles animating at once,like fireworks, rain, and twinkles.

RealityKit provides preset values for these,and you can adjust those presets to get the effect you're after.You can use Reality Composer Pro to design them, and you can configure them in code.

You add the ParticleEmitter to an entity as a Component.Components are a central part of RealityKit,which is based on the "Entity Component System" paradigm.Each object in your scene is an Entity, and you add components to it to tell itwhat traits and behaviors it has.A Component is the type that holds data about an Entity.A System processes entities that have specific components,performing logic involving that data.There are built-in systems for things like animating particles,for handling physics, for rendering, and many more.You can write your own custom system in RealityKit to do customlogic for your game or app.Watch Dive into RealityKit 2 for a more in-depth lookat the Entity Component System in RealityKit.I'll add a particle emitter to each side of Sparky's head.First I make two invisible entitiesthat serve as containers for the sparks effect.I designed my sparks emitter to point to the right.I'll add it directly to my invisible entity on Sparky's right side.

On the other side, I rotate the entity 180 degreesabout the y axis so it's pointing leftward.

Putting it all together in the RealityView,here's Sparky with its animation,its name sign in the right position, and sparks flying!RealityKit is great for detailed creation like this!If you're making a game or play-oriented experience,or need fine-grained control over the behavior of your 3D content,choose RealityView.

On the other hand, use Model3D to display a self-contained 3D asset on its own.Think of it like SwiftUI's Image view but for 3D assets.

Model3D's new animation and configuration catalogs let you do more with Model3D.And if your design evolves and you need direct access to the entities, components,and systems, transition smoothlyfrom Model3D to RealityView using realityViewLayoutBehavior.Next I'll share details about the new Object ManipulationAPI in visionOS 26, which lets people pick up the virtual objects in your app!Object manipulation works from both SwiftUI and RealityKit.With Object manipulation you move the object with a single hand,rotate it with one or both hands,and scale it by pinching and dragging with both hands.You can even pass the object from one hand to the other.

There are two ways to enable this,depending on whether the object is a RealityKit Entity or SwiftUI View.

In SwiftUI, add the new `manipulable` modifier.

To disallow scaling, but keep the ability to moveand rotate the robot with either hand,I specify what Operations are supported.

To make the robot feel super heavy, I specify that it has high inertia.

The .manipulable modifier works when Sparky is displayed in a Model3D view.It applies to the whole Model3D, or to any View it's attached to.

When Sparky's in a RealityView, I want to enable manipulationon just the robot entity itself, not the whole RealityView.

In visionOS 26, ManipulationComponent is a new type that you can set on an entityto enable Object Manipulation.

The static function `configureEntity` adds the ManipulationComponent to your entity.

It also adds a CollisionComponent so that the interaction system knowswhen you've tapped on this entity.It adds an InputTargetComponent which tells the systemthat this entity responds to gestures.And finally, it adds a HoverEffectComponent which applies a visual effectwhen a person looks at or hovers their mouse over it.

This is the only line you need to enable manipulation of an entity in your scene.To customize the experience further, there are several parameters you can pass.Here, I'm specifying a purple spotlight effect.I'm allowing all types of input: direct touch and indirect gaze and pinch.And I'm supplying collision shapes that define the outer dimensions of the robot.To respond when a person interacts with an object in your app,the object manipulation system raises events at key moments,such as when the interaction starts and stops,gets updated as the entity is moved, rotated, and scaled, when it is released,and when it is handed off from one hand to another.

Subscribe to these events to update your state.By default, standard sounds play when the interaction begins,a handoff occurs, or the object is released.

To apply custom sounds,I first set the audioConfiguration to `none`.That disables the standard sounds.Then I subscribe to the ManipulationEvent DidHandOff,which is delivered when a person passes the robot from one hand to the other.

In that closure, I play my own audio resource.Well, Maks.Sparky's journey has been exciting: animating in Model3D, finding its new homein a RealityView, showing its personality with sparks, and letting people reach outand interact with it.It's come a long way on its path towards the RealityKit greenhouse.It sure has!But for Sparky to truly connect with the robot waiting there,the objects in their virtual space need new capabilities.They need to respond to gestures,present information about themselves,and trigger actions in a way that feels native to SwiftUI.

Sparky's journey toward the RealityKit greenhouseis all about building connection.Deep connection requires rich interactions.That's exactly what the new SwiftUI RealityKit componentsare designed to enable.The new components in visionOS 26 bring powerful,familiar SwiftUI capabilities directly to RealityKit entities.

RealityKit introduces three key components:First, the ViewAttachmentComponent allows youto add SwiftUI views directly to your entities.Next, the GestureComponent makes your entities responsive to touch and gestures.And finally, the PresentationComponent, which presents SwiftUI views,like popovers, from within your RealityKit scene.

visionOS 1 let you declare attachments up frontas part of the RealityView initializer.After evaluating your attachment view builder,the system called your update closure with the results as entities.You could add these entities to your scene and position them in 3D space.

In visionOS 26, this is simplified.Now you create attachments using a RealityKit componentfrom anywhere in your app.

Create your ViewAttachmentComponentby giving it any SwiftUI View.Then, add it to an entity's components collection.

And just like that I moved our NameSign from SwiftUI to RealityKit.Let’s explore gestures next!You can already attach gestures to your RealityViewusing `targetedToEntity`gesture modifiers.

New in visionOS 26 is GestureComponent.Just like ViewAttachmentComponent, you add GestureComponent to your entities directly,passing regular SwiftUI gestures to it.The gesture values are by default reported in the entity’s coordinate space.Super handy!I use GestureComponent with a tap gesture to toggle the name sign on and off.

Check it out.This robot's name is... Bolts!Pro tip: on any entity that's the target of a gesture,also add both InputTargetComponent and CollisionComponent.This advice applies to both GestureComponentand the targeted gestures API.

GestureComponent and ViewAttachmentComponentlet me create a name sign for Bolts.But, Bolts is getting ready for a special visitor: Sparky!Bolts wants to look its absolute best for their meeting in the greenhouse.Time for another outfit change!I'll replace Bolts' name sign with UI to pick what Bolts will wear.Truly, a momentous decision.

To emphasize that, I'll show this UI in a popover,using PresentationComponent, directly from RealityKit.

First, I replace `ViewAttachmentComponent` with `PresentationComponent.

The component takes a boolean binding to control when the popover is presented,and to notify you when someone dismisses the popover.

The `configuration` parameter is the type of presentation to be shown.I'm specifying `popover`.

Inside the popover,I'll present a view with configuration catalog options to dress up Bolts.

Now, I can help Bolts pick its best color for when Sparky comes to visit.

Hey Maks, do you think Bolts is more of a summer?Or an autumn?That's a fashion joke.

Bolts is dressed to impress.But first, it has to go to work.Bolts waters plants in the greenhouse.I'll make a mini map, like on a heads-up display in a game,to track Bolts' position in the greenhouse.For that, I need to observe the robot's Transform component.

In visionOS 26, entities are now observable.They can notify other code when their properties change.To be notified, just read an entity’s "observable" property.

From the “observable” property you can watch for changes to the entity's position,scale, and rotation, its Children collection,and even its Components, including your own custom components!Observe these properties directly using a `withObservationTracking` block.

Or lean on SwiftUI's built-in observation tracking.I’ll use SwiftUI to implement my Minimap.

To learn more about Observation, watch "Discover Observation in SwiftUI".

In this view, I display my entity's position on a MiniMap.I'm accessing this observable value on my entity.This tells SwiftUI that my view depends on this value.

As Bolts moves about the greenhouse,watering the plants, its position will change.Each time it does, SwiftUI will call my View's body again,moving its counterpart symbol in the minimap!For a deeper explanation of SwiftUI's data flow,check out the session "Data Essentials in SwiftUI."Our robot friends are really coming together!That's the dream!I liked your description of the difference between "model" and "model" earlier.And sometimes you need to pass data from your data modelto your 3D object model, and vice versa.In visionOS 26, observable entities give us a new tool to do that.Since the beginning, you could pass information from SwiftUI to RealityKitin the `update` closure of RealityView.Now with entity's `observable` property, you can send information the other way.RealityKit entities can act like model objectsto drive updates to your SwiftUI views!So information can flow both ways now:from SwiftUI to RealityKit and from RealityKit to SwiftUI.But...does this create the potential for an infinite loop?Yes!Let's look at how to avoid creating infinite loopsbetween SwiftUI and RealityKit.

When you read an observable property inside the body of a view,you create a dependency; your view depends on that property.When the property’s value changes, SwiftUI will update the viewand re-run its body.RealityView has some special behavior.Think of its…update closure as an extension of the containing view's body.

SwiftUI will call the closure whenever any of that view's state changes,not only when state that is explicitly observed in that closure changes.

Here in my RealityView's update closure, I'm changing that position.This will write to the position value,which will cause SwiftUI to update the view and re-run its body,causing an infinite loop.

To avoid creating an infinite loop don’t modify your observed statewithin your update closure.

You are free to modify entities that you're not observing.That won't create an infinite loop because changes to themwon't trigger SwiftUI to re-evaluate the view body.

If you do need to modify an observed property,check the existing value of that property and avoid writing that same value back.

This breaks the cycle and avoids an infinite loop.

Note that the RealityView's make closure is special.

When you access an observable property in the make closure,that doesn't create a dependency.It's not included in the containing view's observation scope.

Also, the `make` closure is not re-run on changes.It only runs when the containing view first appears.

You can also update the propertieson an observed entity from within your own custom system.A system's update function is not inside the scopeof the SwiftUI view body evaluation,so this is a good place to change my observed entities' values.

The closures of Gestures are also not inside the scopeof the SwiftUI view body evaluation.They are called in response to user input.

You can modify your observed entities' values here, too.

To recap, it's cool to modifyyour observed entities in some places, and not in others.

If do you find that you have an infinite loop in your app,here's a tip for fixing it:Split up your larger views into smaller,self-contained views, each one having only their own necessary state.That way, a change in some unrelated entitywon't cause your small view to be re-evaluated.This is also great for performance!You know, Maks, you might find that you don't needto use your `update` closure at all anymore.Since your Entity can now be your view's state,you can modify it in the normal places that you're used to modifying state,and forgo the update closure altogether.Yeah!I feel like avoiding infinite loopsis something I have to learn over and over again.But, if I don't use an update closure, I'm less likely to run into one.

I think it's time to bring Bolts and Sparky together.

Bolts is finally done with work - time for its date with Sparky!As I pick up Sparky to bring it over, and the two robots get closer together,I want to make sparks fly as a function of the decreasing distance between them.I'll use our new Unified Coordinate Conversion API to enable this.

Sparky is in a Model3D SwiftUI view now,and Bolts is an Entity in the RealityKit greenhouse.I need to get the absolute distance between these two robots,even though they're in different coordinate spaces.

To solve this, the Spatial framework now defines a `CoordinateSpace3D` protocolthat represents an abstract coordinate space.You can easily convert values between any two typesthat conform to CoordinateSpace3D, even from different frameworks.

RealityKit's `Entity` and `Scene` types conform to CoordinateSpace3D.On the SwiftUI side, GeometryProxy3D has a new .coordinateSpace3D() functionthat gives you its coordinate space.Additionally, several Gesture types can provide their values relativeto any given CoordinateSpace3D.CoordinateSpace3D protocol works by first converting a valuein Sparky’s coordinate space to a coordinate space sharedby both RealityKit and SwiftUI.After that, it converts from the shared space into Bolt’s coordinate space,while taking low-level details like points-to-meter conversionand axis direction into account.In Sparky's Model3D view, whenever the view geometry changes,the system calls my `onGeometryChange3D` function.It passes in a GeometryProxy3D which I use to get its coordinate space.Then, I can convert my view's position to a point in the entity's spaceso I know how far apart my two robots are from each other.Now as Amanda brings Bolts and Sparky together, the sparks increase.As she pulls them apart, the sparks decrease.

Next, I'll teach these robots to move together and to coordinate their actions.I'll use SwiftUI driven animation for RealityKit components.

SwiftUI already comes with great animation APIsto implicitly animate changes to your view properties.

Here, I animate the Model3D view that Sparky is in.I send it over to the left when I toggle,and then it bounces back to the original position when I toggle again.

I’m adding an animation to my `isOffset` binding,and I'm specifying that I want an extra bouncy animation for it.

In visionOS 26, you can now use SwiftUI animationto implicitly animate changes to RealityKit components.

All you need to do is set a supported componenton your entity within a RealityKit animation blockand the framework will take care of the rest.There are two ways to associate an animation with a state change.From within a RealityView, you can use `content.animate()`to set new values for your components inside the animate block.RealityKit will use the animation associated with the SwiftUI transactionthat triggered the `update` closure,which, in this case, is an extra bouncy animation.

The other way is to call the new Entity.animate() function,passing a SwiftUI animation,and a closure that sets new values for your components.Here, whenever the isOffset property changes,I send Sparky left or right using the entity’s position.

Setting the position inside the animate block will begin an implicit animationof the Transform component,causing the entity to move smoothly to the new position.The power of implicit animation really shines when I combine itwith the Object Manipulation API that Amanda introduced.I can use a SwiftUI animation to implement a custom release behavior for Bolts.I’m first going to disable the default release behavior for object manipulationby setting it to .stay.

Then, I will subscribe to the WillRelease event for the manipulation interaction.

And when the object is about to be released,I will snap Sparky back by setting its transform to identity,which resets the scale, translation, and rotation of the entity.Since I’m modifying Sparky’s transform inside the animate block,Sparky will bounce back to its default position.Now Sparky's animation back to its original position is much more fun!All these built-in RealityKit components support implicit animations,including the Transform, the Audio components,and the Model and Light components, which have color properties!Sparky and Bolts had quite a journey.It's so great to see the power of SwiftUI and RealityKit working together.

With this connection,you're also empowered to develop truly exceptional spatial apps,fostering a real connection between the virtual and the physical!Imagine the possibilities as you seamlessly integrate SwiftUI componentsinto your RealityKit scenes,and as entities dynamically drive changes to your SwiftUI state.

Just like Sparky and Bolts, we hope you're inspired to connect SwiftUIand RealityKit in ways we haven't even imagined yet.Let's build the future together!

1:42 -Sparky in Model3D

1:52 -Sparky in Model3D with a name sign

3:18 -Display a model asset in a Model3D and present playback controls​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​

3:34 -Display a model asset in a Model3D and present playback controls​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​

4:03 -Display a model asset in a Model3D and present playback controls​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​

4:32 -Pause, resume, stop, and change the move the play head in the animation​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​

5:41 -Load a Model3D using a ConfigurationCatalog​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​

6:51 -Switching from Model3D to RealityView

7:25 -Switching from Model3D to RealityView with layout behavior

8:48 -Switching from Model3D to RealityView with layout behavior and RealityKit animation

10:34 -Add 2 particle emitters; one to each side of the robot's head

12:30 -Apply the manipulable view modifier

12:33 -Allow translate, 1- and 2-handed rotation, but not scaling

12:41 -The model feels heavy with high inertia

13:18 -Add a ManipulationComponent to an entity​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​

13:52 -Add a ManipulationComponent to an entity​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​ with configuration

14:08 -Manipulation interaction events

14:32 -Replace the standard sounds with custom ones

16:19 -Builder based attachments

16:37 -Attachments created with ViewAttachmentComponent

17:04 -Targeted to entity gesture API

17:10 -Gestures with GestureComponent

## Code Samples

```swift
struct
 
ContentView
: 
View
 {
  
var
 body: 
some
 
View
 {
    
Model3D
(named: 
"sparky"
)
  }
}
```

```swift
struct
 
ContentView
: 
View
 {
  
var
 body: 
some
 
View
 {
    
HStack
 {
      
NameSign
()
      
Model3D
(named: 
"sparky"
)
    }
  }
}
```

```swift
struct
 
RobotView
: 
View
 {
  
@State
 
private
 
var
 asset: 
Model3DAsset
?
  
var
 body: 
some
 
View
 {
    
if
 asset 
==
 
nil
 {
      
ProgressView
().task { asset 
=
 
try?
 
await
 
Model3DAsset
(named: 
"sparky"
) }
    }
  }
}
```

```swift
struct
 
RobotView
: 
View
 {
  
@State
 
private
 
var
 asset: 
Model3DAsset
?
  
var
 body: 
some
 
View
 {
    
if
 asset 
==
 
nil
 {
      
ProgressView
().task { asset 
=
 
try?
 
await
 
Model3DAsset
(named: 
"sparky"
) }
    } 
else
 
if
 
let
 asset {
      
VStack
 {
        
Model3D
(asset: asset)
        
AnimationPicker
(asset: asset)
      }
    }
  }
}
```

```swift
struct
 
RobotView
: 
View
 {
  
@State
 
private
 
var
 asset: 
Model3DAsset
?
  
var
 body: 
some
 
View
 {
    
if
 asset 
==
 
nil
 {
      
ProgressView
().task { asset 
=
 
try?
 
await
 
Model3DAsset
(named: 
"sparky"
) }
    } 
else
 
if
 
let
 asset {
      
VStack
 {
        
Model3D
(asset: asset)
        
AnimationPicker
(asset: asset)
        
if
 
let
 animationController 
=
 asset.animationPlaybackController {
          
RobotAnimationControls
(playbackController: animationController)
        }
      }
    }
  }
}
```

```swift
struct
 
RobotAnimationControls
: 
View
 {
  
@Bindable
 
var
 controller: 
AnimationPlaybackController


  
var
 body: 
some
 
View
 {
    
HStack
 {
      
Button
(controller.isPlaying 
?
 
"Pause"
 : 
"Play"
) {
        
if
 controller.isPlaying { controller.pause() }
        
else
 { controller.resume() }
      }

      
Slider
(
        value: 
$controller
.time,
        in: 
0
...
controller.duration
      ).id(controller)
    }
  }
}
```

```swift
struct
 
ConfigCatalogExample
: 
View
 {
  
@State
 
private
 
var
 configCatalog: 
Entity
.
ConfigurationCatalog
?
  
@State
 
private
 
var
 configurations 
=
 [
String
: 
String
]()
  
@State
 
private
 
var
 showConfig 
=
 
false

  
var
 body: 
some
 
View
 {
    
if
 
let
 configCatalog {
      
Model3D
(from: configCatalog, configurations: configurations)
        .popover(isPresented: 
$showConfig
, arrowEdge: .leading) {
          
ConfigPicker
(
            name: 
"outfits"
,
            configCatalog: configCatalog,
            chosenConfig: 
$configurations
[
"outfits"
])
        }
    } 
else
 {
      
ProgressView
()
        .task {
          
await
 loadConfigurationCatalog()
        }
    }
  }
}
```

```swift
struct
 
RobotView
: 
View
 {
  
let
 url: 
URL
 
=
 
Bundle
.main.url(forResource: 
"sparky"
, withExtension: 
"reality"
)
!


  
var
 body: 
some
 
View
 {
    
HStack
 {
      
NameSign
()
      
RealityView
 { content 
in

        
if
 
let
 sparky 
=
 
try?
 
await
 
Entity
(contentsOf: url) {
          content.add(sparky)
        }
      }
    }
  }
}
```

```swift
struct
 
RobotView
: 
View
 {
  
let
 url: 
URL
 
=
 
Bundle
.main.url(forResource: 
"sparky"
, withExtension: 
"reality"
)
!


  
var
 body: 
some
 
View
 {
    
HStack
 {
      
NameSign
()
      
RealityView
 { content 
in

        
if
 
let
 sparky 
=
 
try?
 
await
 
Entity
(contentsOf: url) {
          content.add(sparky)
        }
      }
      .realityViewLayoutBehavior(.fixedSize)
    }
  }
}
```

```swift
struct
 
RobotView
: 
View
 {
  
let
 url: 
URL
 
=
 
Bundle
.main.url(forResource: 
"sparky"
, withExtension: 
"reality"
)
!


  
var
 body: 
some
 
View
 {
    
HStack
 {
      
NameSign
()
      
RealityView
 { content 
in

        
if
 
let
 sparky 
=
 
try?
 
await
 
Entity
(contentsOf: url) {
          content.add(sparky)
          sparky.playAnimation(getAnimation())
        }
      }
      .realityViewLayoutBehavior(.fixedSize)
    }
  }
}
```

```swift
func
 
setupSparks
(
robotHead
: 
Entity
) {
  
let
 leftSparks 
=
 
Entity
()
  
let
 rightSparks 
=
 
Entity
()

  robotHead.addChild(leftSparks)
  robotHead.addChild(rightSparks)

  rightSparks.components.set(sparksComponent())
  leftSparks.components.set(sparksComponent())

  leftSparks.transform.rotation 
=
 simd_quatf(
Rotation3D
(
    angle: .degrees(
180
),
    axis: .y))

  leftSparks.transform.translation 
=
 leftEarOffset()
  rightSparks.transform.translation 
=
 rightEarOffset()
}


// Create and configure the ParticleEmitterComponent


func
 
sparksComponent
() -> 
ParticleEmitterComponent
 { 
...
 }
```

```swift
struct
 
RobotView
: 
View
 {
  
let
 url: 
URL

  
var
 body: 
some
 
View
 {
    
HStack
 {
      
NameSign
()
      
Model3D
(url: url)
        .manipulable()
    }
  }
}
```

```swift
struct
 
RobotView
: 
View
 {
  
let
 url: 
URL

  
var
 body: 
some
 
View
 {
    
HStack
 {
      
NameSign
()
      
Model3D
(url: url)
        .manipulable(
          operations: [.translation,
                       .primaryRotation,
                       .secondaryRotation]
       )
    }
  }
}
```

```swift
struct
 
RobotView
: 
View
 {
  
let
 url: 
URL

  
var
 body: 
some
 
View
 {
    
HStack
 {
      
NameSign
()
      
Model3D
(url: url)
        .manipulable(inertia: .high)
    }
  }
}
```

```swift
RealityView
 { content 
in

  
let
 sparky 
=
 
await
 loadSparky()
  content.add(sparky)
  
ManipulationComponent
.configureEntity(sparky)
}
```

```swift
RealityView
 { content 
in

  
let
 sparky 
=
 
await
 loadSparky()
  content.add(sparky)
  
ManipulationComponent
.configureEntity(
    sparky,
    hoverEffect: .spotlight(.
init
(color: .purple)),
    allowedInputTypes: .all,
    collisionShapes: myCollisionShapes()
  )
}
```

```swift
public
 
enum
 
ManipulationEvents
 {

  
/// When an interaction is about to begin on a ManipulationComponent's entity

  
public
 
struct
 
WillBegin
: 
Event
 { }
  
  
/// When an entity's transform was updated during a ManipulationComponent

  
public
 
struct
 
DidUpdateTransform
: 
Event
 { }

  
/// When an entity was released

  
public
 
struct
 
WillRelease
: 
Event
 { }

  
/// When the object has reached its destination and will no longer be updated

  
public
 
struct
 
WillEnd
: 
Event
 { }

  
/// When the object is directly handed off from one hand to another

  
public
 
struct
 
DidHandOff
: 
Event
 { }
}
```

```swift
RealityView
 { content 
in

  
let
 sparky 
=
 
await
 loadSparky()
  content.add(sparky)

  
var
 manipulation 
=
 
ManipulationComponent
()
  manipulation.audioConfiguration 
=
 .none
  sparky.components.set(manipulation)

  didHandOff 
=
 content.subscribe(to: 
ManipulationEvents
.
DidHandOff
.
self
) { event 
in

    sparky.playAudio(handoffSound)
  }
}
```

```swift
struct
 
RealityViewAttachments
: 
View
 {
  
var
 body: 
some
 
View
 {
    
RealityView
 { content, attachments 
in

      
let
 bolts 
=
 
await
 loadAndSetupBolts()
      
if
 
let
 nameSign 
=
 attachments.entity(
        for: 
"name-sign"

      ) {
        content.add(nameSign)
        place(nameSign, above: bolts)
      }
      content.add(bolts)
    } attachments: {
      
Attachment
(id: 
"name-sign"
) {
        
NameSign
(
"Bolts"
)
      }
    }
    .realityViewLayoutBehavior(.centered)
  }
}
```

```swift
struct
 
AttachmentComponentAttachments
: 
View
 {
  
var
 body: 
some
 
View
 {
    
RealityView
 { content 
in

      
let
 bolts 
=
 
await
 loadAndSetupBolts()
      
let
 attachment 
=
 
ViewAttachmentComponent
(
          rootView: 
NameSign
(
"Bolts"
))
      
let
 nameSign 
=
 
Entity
(components: attachment)
      place(nameSign, above: bolts)
      content.add(bolts)
      content.add(nameSign)
    }
    .realityViewLayoutBehavior(.centered)
  }
}
```

```swift
struct
 
AttachmentComponentAttachments
: 
View
 {
  
@State
 
private
 
var
 bolts 
=
 
Entity
()
  
@State
 
private
 
var
 nameSign 
=
 
Entity
()

  
var
 body: 
some
 
View
 {
    
RealityView
 { 
...
 }
    .realityViewLayoutBehavior(.centered)
    .gesture(
      
TapGesture
()
        .targetedToEntity(bolts)
        .onEnded { value 
in

          nameSign.isEnabled.toggle()
        }
    )
  }
}
```

```swift
struct
 
AttachmentComponentAttachments
: 
View
 {
  
var
 body: 
some
 
View
 {
    
RealityView
 { content 
in

      
let
 bolts 
=
 
await
 loadAndSetupBolts()
      
let
 attachment 
=
 
ViewAttachmentComponent
(
          rootView: 
NameSign
(
"Bolts"
))
      
let
 nameSign 
=
 
Entity
(components: attachment)
      place(nameSign, above: bolts)
      bolts.components.set(
GestureComponent
(
        
TapGesture
().onEnded {
          nameSign.isEnabled.toggle()
        }
      ))
      content.add(bolts)
      content.add(nameSign)
    }
    .realityViewLayoutBehavior(.centered)
  }
}
```

