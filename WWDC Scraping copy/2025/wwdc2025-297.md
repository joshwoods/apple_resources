# Wwdc2025 297

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Summary

Transcript

Code

Learn about the Apple Projected Media ProfileDive into the Apple Projected Media Profile (APMP) and see how APMP enables 180º/360º and Wide FoV projections in QuickTime and MP4 files using Video Extended Usage signaling. We'll provide guidance on using OS-provided frameworks and tools to convert, read/write, edit, and encode media containing APMP. And we'll review Apple Positional Audio Codec's (APAC) capabilities for creating and delivering spatial audio content for the most immersive experiences.Chapters0:00 -Introduction1:12 -Non-rectilinear video fundamentals3:42 -Apple Projected Media Profile specification5:59 -APMP content capturing and workflows8:33 -Asset conversion capabilities10:45 -APMP video reading11:47 -APMP video editing13:42 -APMP video publishing16:14 -Apple Positional  Audio CodecResourcesApple HEVC Stereo Video Interoperability ProfileAVFoundationConverting projected video to Apple Projected Media ProfileCore MediaHTTP Live StreamingHTTP Live Streaming (HLS) authoring specification for Apple devicesQuickTime and ISO Base Media File Formats and Spatial and Immersive MediaUsing Apple’s HTTP Live Streaming (HLS) ToolsHD VideoSD VideoRelated VideosWWDC25Explore video experiences for visionOSLearn about Apple Immersive Video technologiesSupport immersive video playback in visionOS appsWhat’s new for the spatial webWWDC22What’s new in AVQTWWDC21Evaluate videos with the Advanced Video Quality Tool

Dive into the Apple Projected Media Profile (APMP) and see how APMP enables 180º/360º and Wide FoV projections in QuickTime and MP4 files using Video Extended Usage signaling. We'll provide guidance on using OS-provided frameworks and tools to convert, read/write, edit, and encode media containing APMP. And we'll review Apple Positional Audio Codec's (APAC) capabilities for creating and delivering spatial audio content for the most immersive experiences.

0:00 -Introduction

1:12 -Non-rectilinear video fundamentals

3:42 -Apple Projected Media Profile specification

5:59 -APMP content capturing and workflows

8:33 -Asset conversion capabilities

10:45 -APMP video reading

11:47 -APMP video editing

13:42 -APMP video publishing

16:14 -Apple Positional  Audio Codec

Apple HEVC Stereo Video Interoperability Profile

AVFoundation

Converting projected video to Apple Projected Media Profile

Core Media

HTTP Live Streaming

HTTP Live Streaming (HLS) authoring specification for Apple devices

QuickTime and ISO Base Media File Formats and Spatial and Immersive Media

Using Apple’s HTTP Live Streaming (HLS) Tools

HD VideoSD Video

HD Video

SD Video

Explore video experiences for visionOS

Learn about Apple Immersive Video technologies

Support immersive video playback in visionOS apps

What’s new for the spatial web

What’s new in AVQT

Evaluate videos with the Advanced Video Quality Tool

Search this video…Hello, I’m Jon,an engineer on the Core Media Spatial Technologies team.In this video, I will go over the fundamentalsof how non-rectilinear video projection is represented in QuickTime files.Also, I will present updates we’ve introduced to Core Media,Video Toolbox, and AVFoundation frameworksfor reading, writing, editing, and publishingApple Projected Media Profile, or APMP video.And finally, I will cover using the Apple Positional Audio Codecto add immersive spatial audio to projected video media.Whether you are a camera vendor offering a 180 or 360 degreeor wide field of view camera, a developer of video editing software,or an app developer wanting to work with an exciting new type of media,this video will have something for you.I recommend watching “Explore video experiences for visionOS”for important introductory information about the immersive video profilesavailable on visionOS 26 and the concept of video projections.I’ll start with a review of the non-rectilinear video fundamentalsintroduced in the “Explore video experiences for visionOS” video.New for visionOS 26 is the Apple Projected Media Profilewhich can support 180, 360, and wide FOV videofrom consumer accesible cameras.A key differentiator within the projected media profile is the projection kind.2D, 3D and spatial video use a rectilinear projection.180 degree video uses a half-equirectangular projection,360 degree video uses equirectangular,and wide-FOV video uses a parametric projection.The equirectangular projection,also known as the equidistant cylindrical projection,is widely supported by editing applications such as Final Cut Pro.In the equirectangular projection,the pixel coordinates of an enclosing sphereare expressed as angles of latitude and longitudeand projected equally into the rows and columns of a rectangular video frame.The horizontal axis maps longitude from negative to positive 180 degrees,while the vertical axis maps latitude from negative to positive 90 degrees.The half-equirectangular projection is similarbut the X coordinate within the video frame representsthe range from negative 90 to positive 90 degrees.The ParametricImmersive projectionrepresents the intrinsics and lens distortion parametersassociated with wide-angle or fisheye lenses.Intrinsics represent information such as the focal length and optical centerand the skew of the lens system used for capture.These parameters are interpreted as a 3x3 matrix,denoted as ‘K’ for camera matrix, that represents a transformationfrom 3D world coordinates to 2D coordinates on an image plane.In addition, the ParametricImmersive projectioncan represent lens distortion parameters, for example radial distortion.Radial distortion parameters are used to correct for barrel distortion,where straight lines appear curved proportionally to their distancefrom the optical center due to wide angle lens design.In this image, the fence posts appear curved towards the edge of the lens.Other lens distortion characteristics such as tangential distortion,projection offset, radial angle limit, and lens frame adjustmentscan be specified in the ParametricImmersive projection as well.Now that I’ve covered the fundamentals, I’ll provide an overview of how to useApple’s APIs to interact with Apple Projected Media Profile content.I’ll start by describing how APMP videois carried in QuickTime and MP4 movie files.Apple Projected Media Profile enables signaling of 180, 360 and Wide FOVin QuickTime and MP4 files.QuickTime files are structured as a hierarchyof containers of various types of media data,and can include audio and video tracksas well as data describing the details of each track.The ISO Base Media File Format or ISOBMFF specification for MP4was adapted from QuickTime.The fundamental unit of data organization in an ISOBMFF file is a box.For visionOS 1, we introduced a new Video Extended Usage extension boxwith Stereo view information indicating stereoscopic or monoscopic content.For visionOS 26, we add new boxes to Video Extended Usage also known as vexuto enable signaling of the projected media profile.The projection box signals one of the projection typessuch as equirectangular,half-equirectangular or ParametricImmersive.A lens collection box contains the parameters for intrinsics, extrinsicsand lens distortions for the ParametricImmersive projection.The View packing box contains information about the arrangement of eyesin a frame-packed image, whether side by side or over-under.Here’s an example of the minimal signaling for a monoscopic equirectangular file.The projection box with ProjectionKind indicating equirectangular.A stereoscopic 180 degree file requires a stereo view boxin addition to the projection kind signaling half-equirectangular.With these building blocks it is also possible to signal other combinations,such as stereoscopic 360.Check outthe QuickTime and ISO Base Media File Formats and Spatial and Immersive Media specificationon developer.apple.com, for more information on the projection boxand other boxes supported by Apple Projected Media Profile.Next, I’ll outline some ways to capture APMP contentas well as typical APMP workflows.There are a variety of cameras available to capture APMP-compatible content.For example, Canon’s EOS VR systemto capture and process stereoscopic 180 video.GoPro MAX or Insta360 X5 for 360 video.And recent action cameras like the GoPro HERO 13 and Insta360 Ace Pro 2for wide FOV video.Final Cut Pro already supports reading and writing APMP for 360 degree formats.And, coming later this year,camera video editing software such as Canon’s EOS VR Utility,and the GoPro Player support exporting of MOV or MP4 fileswith Apple Projected Media Profile signaling.For 180 or 360 video, use camera vendor software for operationssuch as stitching, stabilization, and stereo image correction.If the editor is already APMP-enabled,export as an MOV or MP4 file with APMP signaling included.Finally, AirDrop or use iCloud to transfer files to Apple Vision Pro.Otherwise if your camera’s software does not yet support Apple Projected Media,you can export as 180 or 360 using spherical metadata.Then use the avconvert macOS utility, either from the command-line,or through a Finder actionby Ctrl-clicking a selection of one or more video files.Finally, AirDrop or use iCloud to transfer files to Apple Vision Pro.Apple Projected Media Profile is suitable for signaling projected videothrough full media workflows, including capture, editing, and delivery.Here is an example of stereoscopic 180 workflowwhere APMP signaling can be used at each step.Capture the content using HEVC, RAW, or ProRes codecs.Then edit using ProRes.For capture and editing 3D content, you can utilize frame-packed, multiview,separate movie files per eye, and even two video tracks signaled in one movie file.In this example, capture requires two movie files,while editing is performed with side-by-side frame-packed content.Encode and publish using the multiview HEVC or MV-HEVC codecfor efficient delivery and playback on visionOS.Now, that I have covered the APMP specification and typical workflows,I'll review the new capabilities available in macOS and visionOS 26for working with APMP files using existing media APIs.I’ll start with asset conversion capabilities.Developers of media workflow-related apps will need time to adopt APMP signaling,so we’ve added functionality in AVFoundationto recognize compatible assets that use Spherical Metadata V1 or V2 signaling.Compatible 180 or 360 content has an equirectangular projection,and can be frame-packed stereo or monoscopic.Pass the asset creation option ShouldParseExternalSphericalTagsto recognize directly compatible spherical contentand synthesize the appropriate format description extensions.This will allow other system APIs to treat the assetas if it were signaled with Apple Projected Media Profile.Check for the presence of the format description extensionconvertedFromExternalSphericalTagsto determine whether spherical metadata was parsed.visionOS 26 has built-in support for lens projection parametersand popular field of view modes for camera vendors such as GoPro and Insta360.Quick Look prompts to convert such files upon opening.To enable wide-FOV content conversion in your applications, use theParametricImmersiveAssetInfo object from the ImmersiveMediaSupport framework.It generates a video format descriptionthat contains a ParametricImmersive projection with intrinsicsand lens distortion parameters for compatible camera models.Use the isConvertible propertyto determine if metadata from a compatible camera was detected,and replace the video track’s format description,with a newly derived format description.Now, system APIs that use this asset will recognize the content as wide-FOV APMP.Please refer to the “Converting projected video to Apple Projected Media Profile”sample application, to learn how to convert into delivery-ready APMP formats.You can read APMP video using familiar system media APIs.CoreMedia and AVFoundation frameworks have been updatedto support projected media identification and reading.If you need to identify an asset as conforming to an APMP profile,perhaps for the purpose of badgingor preparing a specific playback experience,you can use AVAssetPlaybackAssistantand look for the non-rectilinear projection configuration option.To learn about about building APMP video playback experiences, check outthe “Support immersive video playback in visionOS apps” video.When you need more detail, first examine media characteristicsto determine if the video track indicates a non-rectilinear projection.Then you can examine the projectionKindto determine the exact projection signaled.The viewPackingKind format description extensionidentifies frame-packed content.It supports side-by-side and over-under frame packing.To edit projected media use the AVVideoComposition objectfrom the AVFoundation framework and become familiar with CMTaggedBuffers.CMTaggedDynamicBuffers are used across multiple APIsto handle stereoscopic content including editing APIssuch as AVVideoComposition.CMTaggedDynamicBuffer provides a way to specify certain propertiesof underlying buffers, denoted as CM Tags.Each CM Tag will contain a category and value.Here is an example CMTag representing a StereoView categoryindicating the left eye.CMTaggedDynamicBuffers may be grouped into related buffers,such as in this example showing CVPixelBuffersfor left and right eyes in a stereoscopic video sample.To enable stereoscopic video editing with AVVideoComposition,we have added API for specifying the format of tagged buffersproduced by a compositorand a method to pass tagged buffers to composition requests.The outputBufferDescription specifies what type of CMTaggedBuffersthe compositor will be producing.Define it before starting composition.After constructing a stereoscopic pair of CMTaggedBuffers,call finish and pass the tagged buffers.Now that I’ve described how to convert, read and editApple Projected Media Profile assets,I'll talk about the process of writing them.In this code example for writing monoscopic 360 video,I'm using AVAssetWriter to create the asset.I’m using the CompressionPropertyKey kind to specifythe equirectangular projection kind.Compression properties are passed to AVAssetWriterInputusing the outputSettings dictionary property AVVideoCompressionPropertiesKey.Next, I’ll provide recommendations for APMP content publishing.These are recommended limits for playback on visionOS.The video codec encoding parameters should conform to HEVC Main or Main 10with 4:2:0 chroma subsampling.Rec 709 or P3-D65 color primaries are recommended.Stereo mode may be monoscopic or stereoscopic.The suggested resolution at 10-bits for monoscopic is 7680 by 3840,4320x4320 per eye for stereoscopic.Frame rates vary by resolution and bit-depth, with 30 fps recommendedfor 10-bit monoscopic 8K or stereoscopic 4K.Bitrate encoding settings are content dependentand should be chosen appropriately for your use case,but we recommend not exceeding 150 megabits per second peak.Additional information about the MV-HEVC stereo video profile used by Appleis available in the document“Apple HEVC Stereo Video Interoperability Profile”on developer.apple.com.We have updated the Advanced Video Quality Tool or AVQTto support immersive formats such as 3D, Spatial, and APMP 180 and 360 content,along with algorithmic enhancements for better accuracy.AVQT is useful for assessing the perceptual qualityof compressed video content and fine-tuning video encoder parameters.It is also helpful for HLS tiers bitrate optimization.New features include the ability to calculate quality metricsusing awareness of the equirectangular and half-equirectangular projections.The HTTP Live Streaming specification has been enhanced with supportfor streaming Apple Projected Media Profile,and the latest HLS tools available on the Apple developer websitehave been updated to support publishing APMP.This is an example manifest for a stereoscopic 180 asset.The key change is in the EXT-X-STREAM-INFORMATION tag.The REQ-VIDEO-LAYOUT attribute is specifying stereoand half-equirectangular projection.Note that the map segment must also contain a formatDescription extensionsignaling the half-equirectangular projection and stereo view information.For the latest information on HLS bitrate tier laddersand other HLS authoring guidelines, see the "HLS Authoring Specification"on the Apple developer website.Spatial Audio is as important as videowhen creating a compelling immersive experience.In the real world, sound can come from anywhere.To recreate this experience, a technology capable of representingthe entire sound field is required.We designed the Apple Positional Audio Codec or APAC for this purpose.One important capability of APAC is encoding ambisonic audioin order to capture a high-fidelity representation of the sound field.Ambisonic audio is a technique for recording, mixing,and playing back full-sphere Spatial Audio.Ambisonic recordings are not tied to a specific speaker layoutas the sound field is encoded mathematicallyusing a set of spherical harmonic basis functions.Ambisonic audio capture uses an array of microphonesarranged to take a recording of the 3D sound environment,and then, using digital signal processing,the microphone signals are transformed into signalswith directionality corresponding to spherical harmonic components.The combination of all these signals is an accurate representationof the original sound field.The term order in ambisonics refers to the number of spherical componentsused to represent an audio mix.1st-order is 4 components, or channels,and correspond to 1 omnidirectional channeland 3 channels representing front-back, left-right,and up-down directionally oriented audio.2nd-order ambisonics uses 9 components,while 3rd order ambisonics use 16.The higher order ambisonics provide more spatial resolution.Apple Positional Audio Codec is a high-efficiency spatial audio codecand is recommended for encoding spatial audioincluding ambisonics with APMP video.APAC decodes on all Apple platforms except for watchOS.The built-in APAC encoder accessible via AVAssetWriteron iOS, macOS and visionOS platformssupports 1st, 2nd, and 3rd order ambisonics.This code shows the minimal outputSettings required to encode1st, 2nd or 3rd order ambisonics using AVAssetWriter.Recommended bitrates for ambisonics encoded to APAC for APMPrange from 384 kilobits per second for 1st orderto 768 kilobits per second for 3rd order.APAC audio can be segmented and streamed via HLS.Here’s an example of a monoscopic equirectangular videowith APAC audio encoding a 3rd order ambisonic track.Now that you’ve learned about Apple Projected Media Profile,add support for APMP in your app or serviceto enable immersive user-generated content playback, editing, and sharing.If you are a camera vendor, integrate APMP where appropriateto unlock playback in the Apple ecosystem.Adopt Apple Positional Audio Codec to deliver an immersive audio sound fieldfrom an ambisonic microphone capture together with your immersive video.Thanks for watching!Now, I’m going to go capture some stereoscopic 180 video.

Hello, I’m Jon,an engineer on the Core Media Spatial Technologies team.In this video, I will go over the fundamentalsof how non-rectilinear video projection is represented in QuickTime files.Also, I will present updates we’ve introduced to Core Media,Video Toolbox, and AVFoundation frameworksfor reading, writing, editing, and publishingApple Projected Media Profile, or APMP video.

And finally, I will cover using the Apple Positional Audio Codecto add immersive spatial audio to projected video media.

Whether you are a camera vendor offering a 180 or 360 degreeor wide field of view camera, a developer of video editing software,or an app developer wanting to work with an exciting new type of media,this video will have something for you.

I recommend watching “Explore video experiences for visionOS”for important introductory information about the immersive video profilesavailable on visionOS 26 and the concept of video projections.

I’ll start with a review of the non-rectilinear video fundamentalsintroduced in the “Explore video experiences for visionOS” video.

New for visionOS 26 is the Apple Projected Media Profilewhich can support 180, 360, and wide FOV videofrom consumer accesible cameras.A key differentiator within the projected media profile is the projection kind.2D, 3D and spatial video use a rectilinear projection.180 degree video uses a half-equirectangular projection,360 degree video uses equirectangular,and wide-FOV video uses a parametric projection.

The equirectangular projection,also known as the equidistant cylindrical projection,is widely supported by editing applications such as Final Cut Pro.

In the equirectangular projection,the pixel coordinates of an enclosing sphereare expressed as angles of latitude and longitudeand projected equally into the rows and columns of a rectangular video frame.The horizontal axis maps longitude from negative to positive 180 degrees,while the vertical axis maps latitude from negative to positive 90 degrees.

The half-equirectangular projection is similarbut the X coordinate within the video frame representsthe range from negative 90 to positive 90 degrees.

The ParametricImmersive projectionrepresents the intrinsics and lens distortion parametersassociated with wide-angle or fisheye lenses.Intrinsics represent information such as the focal length and optical centerand the skew of the lens system used for capture.These parameters are interpreted as a 3x3 matrix,denoted as ‘K’ for camera matrix, that represents a transformationfrom 3D world coordinates to 2D coordinates on an image plane.

In addition, the ParametricImmersive projectioncan represent lens distortion parameters, for example radial distortion.Radial distortion parameters are used to correct for barrel distortion,where straight lines appear curved proportionally to their distancefrom the optical center due to wide angle lens design.In this image, the fence posts appear curved towards the edge of the lens.

Other lens distortion characteristics such as tangential distortion,projection offset, radial angle limit, and lens frame adjustmentscan be specified in the ParametricImmersive projection as well.

Now that I’ve covered the fundamentals, I’ll provide an overview of how to useApple’s APIs to interact with Apple Projected Media Profile content.

I’ll start by describing how APMP videois carried in QuickTime and MP4 movie files.

Apple Projected Media Profile enables signaling of 180, 360 and Wide FOVin QuickTime and MP4 files.QuickTime files are structured as a hierarchyof containers of various types of media data,and can include audio and video tracksas well as data describing the details of each track.The ISO Base Media File Format or ISOBMFF specification for MP4was adapted from QuickTime.The fundamental unit of data organization in an ISOBMFF file is a box.

For visionOS 1, we introduced a new Video Extended Usage extension boxwith Stereo view information indicating stereoscopic or monoscopic content.For visionOS 26, we add new boxes to Video Extended Usage also known as vexuto enable signaling of the projected media profile.

The projection box signals one of the projection typessuch as equirectangular,half-equirectangular or ParametricImmersive.

A lens collection box contains the parameters for intrinsics, extrinsicsand lens distortions for the ParametricImmersive projection.The View packing box contains information about the arrangement of eyesin a frame-packed image, whether side by side or over-under.

Here’s an example of the minimal signaling for a monoscopic equirectangular file.The projection box with ProjectionKind indicating equirectangular.

A stereoscopic 180 degree file requires a stereo view boxin addition to the projection kind signaling half-equirectangular.With these building blocks it is also possible to signal other combinations,such as stereoscopic 360.

Check outthe QuickTime and ISO Base Media File Formats and Spatial and Immersive Media specificationon developer.apple.com, for more information on the projection boxand other boxes supported by Apple Projected Media Profile.Next, I’ll outline some ways to capture APMP contentas well as typical APMP workflows.

There are a variety of cameras available to capture APMP-compatible content.For example, Canon’s EOS VR systemto capture and process stereoscopic 180 video.GoPro MAX or Insta360 X5 for 360 video.And recent action cameras like the GoPro HERO 13 and Insta360 Ace Pro 2for wide FOV video.Final Cut Pro already supports reading and writing APMP for 360 degree formats.And, coming later this year,camera video editing software such as Canon’s EOS VR Utility,and the GoPro Player support exporting of MOV or MP4 fileswith Apple Projected Media Profile signaling.For 180 or 360 video, use camera vendor software for operationssuch as stitching, stabilization, and stereo image correction.If the editor is already APMP-enabled,export as an MOV or MP4 file with APMP signaling included.Finally, AirDrop or use iCloud to transfer files to Apple Vision Pro.Otherwise if your camera’s software does not yet support Apple Projected Media,you can export as 180 or 360 using spherical metadata.Then use the avconvert macOS utility, either from the command-line,or through a Finder actionby Ctrl-clicking a selection of one or more video files.Finally, AirDrop or use iCloud to transfer files to Apple Vision Pro.

Apple Projected Media Profile is suitable for signaling projected videothrough full media workflows, including capture, editing, and delivery.Here is an example of stereoscopic 180 workflowwhere APMP signaling can be used at each step.

Capture the content using HEVC, RAW, or ProRes codecs.Then edit using ProRes.For capture and editing 3D content, you can utilize frame-packed, multiview,separate movie files per eye, and even two video tracks signaled in one movie file.In this example, capture requires two movie files,while editing is performed with side-by-side frame-packed content.Encode and publish using the multiview HEVC or MV-HEVC codecfor efficient delivery and playback on visionOS.Now, that I have covered the APMP specification and typical workflows,I'll review the new capabilities available in macOS and visionOS 26for working with APMP files using existing media APIs.I’ll start with asset conversion capabilities.Developers of media workflow-related apps will need time to adopt APMP signaling,so we’ve added functionality in AVFoundationto recognize compatible assets that use Spherical Metadata V1 or V2 signaling.Compatible 180 or 360 content has an equirectangular projection,and can be frame-packed stereo or monoscopic.Pass the asset creation option ShouldParseExternalSphericalTagsto recognize directly compatible spherical contentand synthesize the appropriate format description extensions.This will allow other system APIs to treat the assetas if it were signaled with Apple Projected Media Profile.Check for the presence of the format description extensionconvertedFromExternalSphericalTagsto determine whether spherical metadata was parsed.

visionOS 26 has built-in support for lens projection parametersand popular field of view modes for camera vendors such as GoPro and Insta360.Quick Look prompts to convert such files upon opening.

To enable wide-FOV content conversion in your applications, use theParametricImmersiveAssetInfo object from the ImmersiveMediaSupport framework.It generates a video format descriptionthat contains a ParametricImmersive projection with intrinsicsand lens distortion parameters for compatible camera models.Use the isConvertible propertyto determine if metadata from a compatible camera was detected,and replace the video track’s format description,with a newly derived format description.

Now, system APIs that use this asset will recognize the content as wide-FOV APMP.

Please refer to the “Converting projected video to Apple Projected Media Profile”sample application, to learn how to convert into delivery-ready APMP formats.

You can read APMP video using familiar system media APIs.

CoreMedia and AVFoundation frameworks have been updatedto support projected media identification and reading.

If you need to identify an asset as conforming to an APMP profile,perhaps for the purpose of badgingor preparing a specific playback experience,you can use AVAssetPlaybackAssistantand look for the non-rectilinear projection configuration option.To learn about about building APMP video playback experiences, check outthe “Support immersive video playback in visionOS apps” video.

When you need more detail, first examine media characteristicsto determine if the video track indicates a non-rectilinear projection.Then you can examine the projectionKindto determine the exact projection signaled.

The viewPackingKind format description extensionidentifies frame-packed content.

It supports side-by-side and over-under frame packing.

To edit projected media use the AVVideoComposition objectfrom the AVFoundation framework and become familiar with CMTaggedBuffers.

CMTaggedDynamicBuffers are used across multiple APIsto handle stereoscopic content including editing APIssuch as AVVideoComposition.CMTaggedDynamicBuffer provides a way to specify certain propertiesof underlying buffers, denoted as CM Tags.

Each CM Tag will contain a category and value.

Here is an example CMTag representing a StereoView categoryindicating the left eye.

CMTaggedDynamicBuffers may be grouped into related buffers,such as in this example showing CVPixelBuffersfor left and right eyes in a stereoscopic video sample.

To enable stereoscopic video editing with AVVideoComposition,we have added API for specifying the format of tagged buffersproduced by a compositorand a method to pass tagged buffers to composition requests.

The outputBufferDescription specifies what type of CMTaggedBuffersthe compositor will be producing.Define it before starting composition.After constructing a stereoscopic pair of CMTaggedBuffers,call finish and pass the tagged buffers.

Now that I’ve described how to convert, read and editApple Projected Media Profile assets,I'll talk about the process of writing them.

In this code example for writing monoscopic 360 video,I'm using AVAssetWriter to create the asset.

I’m using the CompressionPropertyKey kind to specifythe equirectangular projection kind.Compression properties are passed to AVAssetWriterInputusing the outputSettings dictionary property AVVideoCompressionPropertiesKey.

Next, I’ll provide recommendations for APMP content publishing.

These are recommended limits for playback on visionOS.The video codec encoding parameters should conform to HEVC Main or Main 10with 4:2:0 chroma subsampling.Rec 709 or P3-D65 color primaries are recommended.

Stereo mode may be monoscopic or stereoscopic.

The suggested resolution at 10-bits for monoscopic is 7680 by 3840,4320x4320 per eye for stereoscopic.

Frame rates vary by resolution and bit-depth, with 30 fps recommendedfor 10-bit monoscopic 8K or stereoscopic 4K.

Bitrate encoding settings are content dependentand should be chosen appropriately for your use case,but we recommend not exceeding 150 megabits per second peak.

Additional information about the MV-HEVC stereo video profile used by Appleis available in the document“Apple HEVC Stereo Video Interoperability Profile”on developer.apple.com.

We have updated the Advanced Video Quality Tool or AVQTto support immersive formats such as 3D, Spatial, and APMP 180 and 360 content,along with algorithmic enhancements for better accuracy.AVQT is useful for assessing the perceptual qualityof compressed video content and fine-tuning video encoder parameters.It is also helpful for HLS tiers bitrate optimization.New features include the ability to calculate quality metricsusing awareness of the equirectangular and half-equirectangular projections.

The HTTP Live Streaming specification has been enhanced with supportfor streaming Apple Projected Media Profile,and the latest HLS tools available on the Apple developer websitehave been updated to support publishing APMP.

This is an example manifest for a stereoscopic 180 asset.The key change is in the EXT-X-STREAM-INFORMATION tag.The REQ-VIDEO-LAYOUT attribute is specifying stereoand half-equirectangular projection.Note that the map segment must also contain a formatDescription extensionsignaling the half-equirectangular projection and stereo view information.

For the latest information on HLS bitrate tier laddersand other HLS authoring guidelines, see the "HLS Authoring Specification"on the Apple developer website.

Spatial Audio is as important as videowhen creating a compelling immersive experience.In the real world, sound can come from anywhere.To recreate this experience, a technology capable of representingthe entire sound field is required.We designed the Apple Positional Audio Codec or APAC for this purpose.

One important capability of APAC is encoding ambisonic audioin order to capture a high-fidelity representation of the sound field.

Ambisonic audio is a technique for recording, mixing,and playing back full-sphere Spatial Audio.

Ambisonic recordings are not tied to a specific speaker layoutas the sound field is encoded mathematicallyusing a set of spherical harmonic basis functions.

Ambisonic audio capture uses an array of microphonesarranged to take a recording of the 3D sound environment,and then, using digital signal processing,the microphone signals are transformed into signalswith directionality corresponding to spherical harmonic components.The combination of all these signals is an accurate representationof the original sound field.

The term order in ambisonics refers to the number of spherical componentsused to represent an audio mix.

1st-order is 4 components, or channels,and correspond to 1 omnidirectional channeland 3 channels representing front-back, left-right,and up-down directionally oriented audio.2nd-order ambisonics uses 9 components,while 3rd order ambisonics use 16.The higher order ambisonics provide more spatial resolution.Apple Positional Audio Codec is a high-efficiency spatial audio codecand is recommended for encoding spatial audioincluding ambisonics with APMP video.

APAC decodes on all Apple platforms except for watchOS.The built-in APAC encoder accessible via AVAssetWriteron iOS, macOS and visionOS platformssupports 1st, 2nd, and 3rd order ambisonics.

This code shows the minimal outputSettings required to encode1st, 2nd or 3rd order ambisonics using AVAssetWriter.

Recommended bitrates for ambisonics encoded to APAC for APMPrange from 384 kilobits per second for 1st orderto 768 kilobits per second for 3rd order.

APAC audio can be segmented and streamed via HLS.Here’s an example of a monoscopic equirectangular videowith APAC audio encoding a 3rd order ambisonic track.

Now that you’ve learned about Apple Projected Media Profile,add support for APMP in your app or serviceto enable immersive user-generated content playback, editing, and sharing.If you are a camera vendor, integrate APMP where appropriateto unlock playback in the Apple ecosystem.Adopt Apple Positional Audio Codec to deliver an immersive audio sound fieldfrom an ambisonic microphone capture together with your immersive video.

Thanks for watching!Now, I’m going to go capture some stereoscopic 180 video.

8:58 -Recognize spherical v1/v2 equirectangular content

9:54 -Convert wide FOV content from supported cameras

10:58 -Recognize Projected & Immersive Video

11:22 -Perform projection or viewPacking processing

12:51 -Specify outputBufferDescription for a stereoscopic pair

13:01 -Finish an asyncVideoCompositionRequest with tagged buffers

0:00 -IntroductionLearn how non-rectilinear video projection is represented in QuickTime files. Discover new APIs for creating, editing, and publishing APMP videos with Spatial Audio. This video is tailored for camera vendors, video editing software developers, and app developers interested in immersive media, particularly for visionOS1:12 -Non-rectilinear video fundamentalsvisionOS 26 introduces the Apple Projected Media Profile, which supports various non-rectilinear video formats beyond the traditional 2D, 3D, and spatial videos. This profile includes half-equirectangular projections for 180-degree videos, equirectangular projections for 360-degree videos, and parametric projections for Wide FOV videos captured with wide-angle or fisheye lenses.

Equirectangular projections map spherical coordinates to a rectangular frame using latitude and longitude angles. Parametric immersive projections, accounts for lens distortion parameters such as focal length, optical center, skew, and radial distortion, which corrects for barrel distortion commonly seen in wide-angle lenses. This allows for more accurate and immersive representation of Wide FOV video content.3:42 -Apple Projected Media Profile specificationApple Projected Media Profile (APMP) enables signaling of 180, 360, and Wide FOV projections. Apple APIs allow you to work with APMP content in QuickTime and MP4 files. 

New boxes within the Video Extended Usage (vexu) box in visionOS 26 specify projection types, lens parameters, and view packing arrangements. These boxes allow for the representation of various immersive media formats, such as monoscopic equirectangular and stereoscopic 180-degree videos.5:59 -APMP content capturing and workflowsVarious cameras and editing software support APMP for capturing and editing 180 and 360-degree video. APMP signaling is used throughout the workflow — capture, editing, and delivery — for efficient playback on visionOS devices. For non-APMP-enabled software, you can use spherical metadata and avconvert utility for conversion before transferring files to Apple Vision Pro.8:33 -Asset conversion capabilitiesmacOS and visionOS 26 introduce new capabilities for working with APMP files. AVFoundation now recognizes compatible spherical assets and synthesizes APMP signaling. visionOS 26 has built-in support for popular camera vendors' lens projection parameters and field of view modes, enabling automatic conversion upon opening. Use new frameworks and objects to convert Wide FOV content to APMP format, making it recognizable by system APIs.10:45 -APMP video readingThe updated CoreMedia and AVFoundation frameworks in visionOS enable you to read and identify APMP videos using system media APIs. 'AVAssetPlaybackAssistant' can be used to check for non-rectilinear projection configurations, and 'viewPackingKind' format description extension supports side-by-side and over-under frame packing for immersive video playback experiences.11:47 -APMP video editingAVFoundation's AVVideoComposition object and CMTaggedDynamicBuffer is used for stereoscopic video editing. CMTaggedDynamicBuffer, which contain CM Tags with categories and values, group related buffers like CVPixelBuffer for the left and right eye. To enable stereoscopic editing, define the outputBufferDescription and pass tagged buffers to composition requests after constructing the stereoscopic pair.13:42 -APMP video publishingAPMP content publishing on visionOS needs to use HEVC Main or Main 10 encoding with specific color primaries and resolutions. Stereo mode can be monoscopic or stereoscopic, with recommended frame rates and bitrates varying by resolution.

The Apple Advanced Video Quality Tool (AVQT) has been updated to support immersive formats and assess video quality. HLS has been enhanced to support APMP streaming, and updated tools and guidelines are available on the Apple Developer website.16:14 -Apple Positional  Audio CodecApple Positional Audio Codec (APAC) is a technology designed to encode ambisonic audio to enable immersive Spatial Audio experiences. Ambisonic audio is a technique for full-sphere sound field recording and playback. It uses mathematical functions and an array of microphones, enabling sound to come from any direction.

APAC supports first, second, and third-order ambisonics, with higher orders providing greater spatial resolution. The codec is highly efficient and recommended for encoding spatial audio with APMP video. APAC decodes on all major Apple platforms except watchOS and allows for segmented and streamed audio via HLS.

0:00 -Introduction

Learn how non-rectilinear video projection is represented in QuickTime files. Discover new APIs for creating, editing, and publishing APMP videos with Spatial Audio. This video is tailored for camera vendors, video editing software developers, and app developers interested in immersive media, particularly for visionOS

Learn how non-rectilinear video projection is represented in QuickTime files. Discover new APIs for creating, editing, and publishing APMP videos with Spatial Audio. This video is tailored for camera vendors, video editing software developers, and app developers interested in immersive media, particularly for visionOS

1:12 -Non-rectilinear video fundamentals

visionOS 26 introduces the Apple Projected Media Profile, which supports various non-rectilinear video formats beyond the traditional 2D, 3D, and spatial videos. This profile includes half-equirectangular projections for 180-degree videos, equirectangular projections for 360-degree videos, and parametric projections for Wide FOV videos captured with wide-angle or fisheye lenses.

Equirectangular projections map spherical coordinates to a rectangular frame using latitude and longitude angles. Parametric immersive projections, accounts for lens distortion parameters such as focal length, optical center, skew, and radial distortion, which corrects for barrel distortion commonly seen in wide-angle lenses. This allows for more accurate and immersive representation of Wide FOV video content.

visionOS 26 introduces the Apple Projected Media Profile, which supports various non-rectilinear video formats beyond the traditional 2D, 3D, and spatial videos. This profile includes half-equirectangular projections for 180-degree videos, equirectangular projections for 360-degree videos, and parametric projections for Wide FOV videos captured with wide-angle or fisheye lenses.

Equirectangular projections map spherical coordinates to a rectangular frame using latitude and longitude angles. Parametric immersive projections, accounts for lens distortion parameters such as focal length, optical center, skew, and radial distortion, which corrects for barrel distortion commonly seen in wide-angle lenses. This allows for more accurate and immersive representation of Wide FOV video content.

3:42 -Apple Projected Media Profile specification

Apple Projected Media Profile (APMP) enables signaling of 180, 360, and Wide FOV projections. Apple APIs allow you to work with APMP content in QuickTime and MP4 files. 

New boxes within the Video Extended Usage (vexu) box in visionOS 26 specify projection types, lens parameters, and view packing arrangements. These boxes allow for the representation of various immersive media formats, such as monoscopic equirectangular and stereoscopic 180-degree videos.

Apple Projected Media Profile (APMP) enables signaling of 180, 360, and Wide FOV projections. Apple APIs allow you to work with APMP content in QuickTime and MP4 files. 

New boxes within the Video Extended Usage (vexu) box in visionOS 26 specify projection types, lens parameters, and view packing arrangements. These boxes allow for the representation of various immersive media formats, such as monoscopic equirectangular and stereoscopic 180-degree videos.

5:59 -APMP content capturing and workflows

Various cameras and editing software support APMP for capturing and editing 180 and 360-degree video. APMP signaling is used throughout the workflow — capture, editing, and delivery — for efficient playback on visionOS devices. For non-APMP-enabled software, you can use spherical metadata and avconvert utility for conversion before transferring files to Apple Vision Pro.

Various cameras and editing software support APMP for capturing and editing 180 and 360-degree video. APMP signaling is used throughout the workflow — capture, editing, and delivery — for efficient playback on visionOS devices. For non-APMP-enabled software, you can use spherical metadata and avconvert utility for conversion before transferring files to Apple Vision Pro.

8:33 -Asset conversion capabilities

macOS and visionOS 26 introduce new capabilities for working with APMP files. AVFoundation now recognizes compatible spherical assets and synthesizes APMP signaling. visionOS 26 has built-in support for popular camera vendors' lens projection parameters and field of view modes, enabling automatic conversion upon opening. Use new frameworks and objects to convert Wide FOV content to APMP format, making it recognizable by system APIs.

macOS and visionOS 26 introduce new capabilities for working with APMP files. AVFoundation now recognizes compatible spherical assets and synthesizes APMP signaling. visionOS 26 has built-in support for popular camera vendors' lens projection parameters and field of view modes, enabling automatic conversion upon opening. Use new frameworks and objects to convert Wide FOV content to APMP format, making it recognizable by system APIs.

10:45 -APMP video reading

The updated CoreMedia and AVFoundation frameworks in visionOS enable you to read and identify APMP videos using system media APIs. 'AVAssetPlaybackAssistant' can be used to check for non-rectilinear projection configurations, and 'viewPackingKind' format description extension supports side-by-side and over-under frame packing for immersive video playback experiences.

The updated CoreMedia and AVFoundation frameworks in visionOS enable you to read and identify APMP videos using system media APIs. 'AVAssetPlaybackAssistant' can be used to check for non-rectilinear projection configurations, and 'viewPackingKind' format description extension supports side-by-side and over-under frame packing for immersive video playback experiences.

11:47 -APMP video editing

AVFoundation's AVVideoComposition object and CMTaggedDynamicBuffer is used for stereoscopic video editing. CMTaggedDynamicBuffer, which contain CM Tags with categories and values, group related buffers like CVPixelBuffer for the left and right eye. To enable stereoscopic editing, define the outputBufferDescription and pass tagged buffers to composition requests after constructing the stereoscopic pair.

AVFoundation's AVVideoComposition object and CMTaggedDynamicBuffer is used for stereoscopic video editing. CMTaggedDynamicBuffer, which contain CM Tags with categories and values, group related buffers like CVPixelBuffer for the left and right eye. To enable stereoscopic editing, define the outputBufferDescription and pass tagged buffers to composition requests after constructing the stereoscopic pair.

13:42 -APMP video publishing

APMP content publishing on visionOS needs to use HEVC Main or Main 10 encoding with specific color primaries and resolutions. Stereo mode can be monoscopic or stereoscopic, with recommended frame rates and bitrates varying by resolution.

The Apple Advanced Video Quality Tool (AVQT) has been updated to support immersive formats and assess video quality. HLS has been enhanced to support APMP streaming, and updated tools and guidelines are available on the Apple Developer website.

APMP content publishing on visionOS needs to use HEVC Main or Main 10 encoding with specific color primaries and resolutions. Stereo mode can be monoscopic or stereoscopic, with recommended frame rates and bitrates varying by resolution.

The Apple Advanced Video Quality Tool (AVQT) has been updated to support immersive formats and assess video quality. HLS has been enhanced to support APMP streaming, and updated tools and guidelines are available on the Apple Developer website.

16:14 -Apple Positional  Audio Codec

Apple Positional Audio Codec (APAC) is a technology designed to encode ambisonic audio to enable immersive Spatial Audio experiences. Ambisonic audio is a technique for full-sphere sound field recording and playback. It uses mathematical functions and an array of microphones, enabling sound to come from any direction.

APAC supports first, second, and third-order ambisonics, with higher orders providing greater spatial resolution. The codec is highly efficient and recommended for encoding spatial audio with APMP video. APAC decodes on all major Apple platforms except watchOS and allows for segmented and streamed audio via HLS.

Apple Positional Audio Codec (APAC) is a technology designed to encode ambisonic audio to enable immersive Spatial Audio experiences. Ambisonic audio is a technique for full-sphere sound field recording and playback. It uses mathematical functions and an array of microphones, enabling sound to come from any direction.

APAC supports first, second, and third-order ambisonics, with higher orders providing greater spatial resolution. The codec is highly efficient and recommended for encoding spatial audio with APMP video. APAC decodes on all major Apple platforms except watchOS and allows for segmented and streamed audio via HLS.

## Code Samples

```swift
// Convert spherical v1/v2 RFC 180/360 equirectangular content



import
 AVFoundation


func
 
wasConvertedFromSpherical
(
url
: 
URL
) -> 
Bool
 {
	
let
 assetOptions 
=
 [
AVURLAssetShouldParseExternalSphericalTagsKey
: 
true
]
	
let
 urlAsset 
=
 
AVURLAsset
(url: url, options: assetOptions)
	
	
// simplified for sample, assume first video track

	
let
 track 
=
 
try
 
await
 urlAsset.loadTracks(withMediaType: .video).first
!

	
	
// Retrieve formatDescription from video track, simplified for sample assume first format description

	
let
 formatDescription 
=
 
try
 
await
 videoTrack.load(.formatDescriptions).first
	
	
// Detect if formatDescription includes extensions synthesized from spherical

	
let
 wasConvertedFromSpherical 
=
 formatDescription.extensions[.convertedFromExternalSphericalTags]
	
	
return
 wasConvertedFromSpherical
}
```

```swift
// Convert wide-FOV content from recognized camera models


import
 ImmersiveMediaSupport


func
 
upliftIntoParametricImmersiveIfPossible
(
url
: 
URL
) -> 
AVMutableMovie
 {
	
let
 movie 
=
 
AVMutableMovie
(url: url)

	
let
 assetInfo 
=
 
try
 
await
 
ParametricImmersiveAssetInfo
(asset: movie)
	
if
 (assetInfo.isConvertible) {
		
guard
 
let
 newDescription 
=
 assetInfo.requiredFormatDescription 
else
 {
			
fatalError
(
"no format description for convertible asset"
)
		}
		
let
 videoTracks 
=
 
try
 
await
 movie.loadTracks(withMediaType: .video)
		
guard
 
let
 videoTrack 
=
 videoTracks.first,
			  
let
 currentDescription 
=
 
try
 
await
 videoTrack.load(.formatDescriptions).first
		
else
 {
      
fatalError
(
"missing format description for video track"
)
		}
		
// presumes that format already compatible for intended use case (delivery or production)

    
// for delivery then if not already HEVC should transcode for example

		videoTrack.replaceFormatDescription(currentDescription, with: newDescription)
	}
  
return
 movie
}
```

```swift
// Determine if an asset contains any tracks with nonRectilinearVideo and if so, whether any are AIV


import
 AVFoundation


func
 
classifyProjectedMedia
( 
movieURL
: 
URL
 ) 
async
 -> (containsNonRectilinearVideo: 
Bool
, containsAppleImmersiveVideo: 
Bool
) {
	
	
let
 asset 
=
 
AVMovie
(url: movieURL)
	
let
 assistant 
=
 
AVAssetPlaybackAssistant
(asset: asset)
	
let
 options 
=
 
await
 assistant.playbackConfigurationOptions
	
// Note contains(.nonRectilinearProjection) is true for both APMP & AIV, while contains(.appleImmersiveVideo) is true only for AIV

	
return
 (options.contains(.nonRectilinearProjection), options.contains(.appleImmersiveVideo))
}
```

```swift
import
 AVFoundation

import
 CoreMedia


// Perform projection or viewPacking specific processing


func
 
handleProjectionAndViewPackingKind
(
_
 
movieURL
: 
URL
) 
async
 
throws
 {
	
	
let
 movie 
=
 
AVMovie
(url: movieURL)
	
let
 track 
=
 
try
 
await
 movie.loadTracks(withMediaType: .video).first
!

	
let
 mediaCharacteristics 
=
 
try
 
await
 track.load(.mediaCharacteristics)
	
	
// Check for presence of non-rectilinear projection

	
if
 mediaCharacteristics.contains(.indicatesNonRectilinearProjection) {
		
let
 formatDescriptions 
=
 
try
 
await
 track.load(.formatDescriptions)
		
for
 formatDesc 
in
 formatDescriptions {
			
if
 
let
 projectionKind 
=
 formatDesc.extensions[.projectionKind] {
				
if
 projectionKind 
==
 .projectionKind(.equirectangular) {
					
// handle equirectangular (360) video

				} 
else
 
if
 projectionKind 
==
 .projectionKind(.halfEquirectangular) {
					
// handle 180 video

				} 
else
 
if
 projectionKind 
==
 .projectionKind(.parametricImmersive) {
					
// handle parametric wfov video

				} 
else
 
if
 projectionKind 
==
 .projectionKind(.appleImmersiveVideo) {
					
// handle AIV

				}
			}
			
if
 
let
 viewPackingKind 
=
 formatDesc.extensions[.viewPackingKind] {
				
if
 viewPackingKind 
==
 .viewPackingKind(.sideBySide) {
					
// handle side by side

				} 
else
 
if
 viewPackingKind 
==
 .viewPackingKind(.overUnder) {
					
// handle over under

				}
			}
		}
	}
}
```

```swift
var
 config 
=
 
try
 
await
 
AVVideoComposition
.
Configuration
(for: asset)
	
	config.outputBufferDescription 
=
 [[.stereoView(.leftEye)], [.stereoView(.rightEye)]]

	
let
 videoComposition 
=
 
AVVideoComposition
(configuration: config)
```

```swift
func
 
startRequest
(
_
 
asyncVideoCompositionRequest
: 
AVAsynchronousVideoCompositionRequest
) {
	
var
 taggedBuffers: [
CMTaggedDynamicBuffer
] 
=
 []
	
let
 
MVHEVCLayerIDs
 
=
 [
0
, 
1
]
	
let
 eyes: [
CMStereoViewComponents
] 
=
 [.leftEye, .rightEye]
	
	
for
 (layerID, eye) 
in
 
zip
(
MVHEVCLayerIDs
, eyes) {
		
// take a monoscopic image and convert it to a z=0 stereo image with identical content for each eye

		
let
 pixelBuffer 
=
 asyncVideoCompositionRequest.sourceReadOnlyPixelBuffer(byTrackID: 
0
)
		
		
let
 tags: [
CMTag
] 
=
 [.videoLayerID(
Int64
(layerID)), .stereoView(eye)]
		
let
 buffer 
=
 
CMTaggedDynamicBuffer
(tags: tags, content: .pixelBuffer(pixelBuffer
!
))
		taggedBuffers.append(buffer)
	}
	asyncVideoCompositionRequest.finish(withComposedTaggedBuffers: taggedBuffers)
}
```

