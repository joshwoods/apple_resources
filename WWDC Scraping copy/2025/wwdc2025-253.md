# Wwdc2025 253

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Summary

Transcript

Code

Enhancing your camera experience with capture controlsLearn how to customize capture controls in your camera experiences. We'll show you how to take photos with all physical capture controls, including new AirPods support, and how to adjust settings with Camera Control.Chapters0:00 -Introduction2:51 -Physical capture3:01 -Event handling6:39 -AirPods remote capture10:00 -Camera controlResourcesAccessing the camera while multitaskingAVFoundationAVMultiCamPiP: Capturing from Multiple CamerasCapture setupCapturing Photos with DepthCreating a camera experience for the Lock ScreenCreating a camera extension with Core Media I/ODockKitForum: Photos & CameraScanning data with the cameraSupporting Continuity Camera in your macOS appSupporting Continuity Camera in your tvOS appHD VideoSD VideoRelated VideosWWDC24Build a great Lock Screen camera capture experienceWWDC23Discover Continuity Camera for tvOSSupport external cameras in your iPadOS app

Learn how to customize capture controls in your camera experiences. We'll show you how to take photos with all physical capture controls, including new AirPods support, and how to adjust settings with Camera Control.

0:00 -Introduction

2:51 -Physical capture

3:01 -Event handling

6:39 -AirPods remote capture

10:00 -Camera control

Accessing the camera while multitasking

AVFoundation

AVMultiCamPiP: Capturing from Multiple Cameras

Capture setup

Capturing Photos with Depth

Creating a camera experience for the Lock Screen

Creating a camera extension with Core Media I/O

DockKit

Forum: Photos & Camera

Scanning data with the camera

Supporting Continuity Camera in your macOS app

Supporting Continuity Camera in your tvOS app

HD VideoSD Video

HD Video

SD Video

Build a great Lock Screen camera capture experience

Discover Continuity Camera for tvOS

Support external cameras in your iPadOS app

Search this video…Hi, my name is Vitaliy.I'm an engineer on the AVKit team,and welcome to 'Enhancing your camera experience with capture controls'.In this session,we will explore powerful new ways to improve user interactions in your app.With Capture Controls,you can programmatically map physical button gestures to camera actions,giving users the familiar feel of the native iOS Camera app.We'll also be showcasing an amazing new feature introduced in iOS 26,and we think you are going to love it.To illustrate the power of Capture Controls,let’s use the camera app on iOS.Instead of taking a photo with the UI button on the screen,let’s use physical buttons on the phone.With just a simple click of the volume up button,we can initiate capture.But this API isn't just about simple clicks.It also supports long presses.For example, in camera mode,if we press and hold the volume down button,it'll kick off video recording.Just like that, with Capture Controls,we have given our usersmore flexibility in using the camerato capture unforgettable moments with their iPhones.In this session,We will first cover what physical capture isand what physical buttons are supported.Then, we’ll show you how to effectively use the API in your appto handle user interactionsand build a robust and responsive camera interfacelike we saw in the demoWe will also go over an exciting new feature in iOS 26Remote Camera Control with AirPods.Finally, my colleague, Nick,will present an overview of Camera Control on iPhone 16.Before exploring the API's core functionalitieslet's review the key frameworks involvedin creating a great camera experience in iOSAt the heart of our camera app's architecture is AVFoundation,providing the low-level capture functionality through APIslike AVCapturePhotoOutput for photoand AVCaptureMovieFileOutput for video.To combine the UI layer with AVFoundation,we have AVKit that sits on top of these frameworks.One capture-specific API that AVKit includes is AVCaptureEventInteraction.Now, let’s dive into the key featuresand capabilities of AVCaptureEventInteraction.This API lets you override the default behaviorof physical buttons like the volume upand down or the Camera Control introduced with iPhone 16.Another button that AVCaptureEventInteraction supportsis the Action button.Please check out our session from last yearon how to configure the Action button for capture.Every physical button press triggers a capture event notification.This notification includes an event phase,giving you precise control over the entire press lifecycle.There are three possible phases:began: The moment the button is pressed.Perfect for preparing your app’s camera object.cancelled: For those moments when the app goes into backgroundor capture is not available.ended: The moment when the button is released.This is the phase when the camera object should initiate capture.AVCaptureEventInteraction also gives you the abilityto distinguish button presses between primary and secondary actions.A button press triggers a capture notificationthat is sent only to its designated handler.Primary actions are triggered by the volume down button,Action button, and the Camera Control,while the secondary action is triggered by the volume up button.The same button cannot trigger both handlersas they are designed to represent distinct actions.The secondary handler is optional,if it's not specified,the volume up click will trigger the primary action.Such design introduces modularity into your appand gives you more flexibilityin designing a great camera experience for your users.This API is intended for capture use cases.The system sends capture events only to apps that actively use the camera.Adopting this API overrides default physical button behavior,so apps must always respond appropriately to any events received.Failing to handle events results in a nonfunctional buttonthat provides a poor user experience.Backgrounded capture apps,or those that do not have an active AVCaptureSession running,won’t receive any events.Any button press will trigger its default system action,such as adjusting volume or launching the camera.Although AVCaptureEventInteraction is an API designed specifically for UIKit,SwiftUI developers can also access its functionalitythrough the onCameraCaptureEvent view modifier.Adopting either of them will result in the same behavior.Now that we understand AVCaptureEventInteraction,let's build a simple camera appthat uses the SwiftUI onCameraCaptureEvent view modifierto allow photo capture with physical buttons.First, let’s begin by crafting the user interface for our camera app.We will include the CameraView that is responsiblefor displaying the camera output on the screen.Next, let's add a button to the view, giving the user a way to capture photos.We'll connect this button to our CameraModel,so tapping it triggers the photo capture.Now, we have a functional view that takes pictureswhen the user presses on the on-screen button.However, pressing on any physical buttons will trigger system actions.So, let’s make these hardware button presses camera specificwith the API we discussed in this session.First, we will import the AVKit framework.Then, we would attach the onCameraCaptureEventview modifier to the CameraViewand if the event phase is ended, we take a picture.It’s as simple as that!With only 6 additional lines of code,we've enabled the same intuitive physical button interactions for photo captureas you find in the built-in Camera app.That's the power of leveraging the Capture Controls API.This year, we're excited to announce that AVCaptureEventInteractionwill also support primary actions triggered by clicks on either AirPod stem,specifically from AirPods equipped with the H2 chip.This will allow the user to remotely capture unforgettable momentswithout needing to interact with their phones.For those who have already adopted the API,this feature will come for free with iOS 26.Now, let's checkout this new feature in action.First, I'll put in my right AirPod, then the left one.Looking sharp!Then, to configure the AirPods for capture,let's go into settingsand scroll down to the “Remote Camera Capture” section.There, we can choose which type of click will trigger capture.I’ll choose the “Press once” option.Now, let’s go back into the camera app.I’ll step back a few steps,and take a photo by clicking on one of the stems.Great, now I can control my camera without touching the device.Because AirPods are used for remote camera control,audio feedback is crucial,as users may not be viewing the screen during capturebut need confirmation that their command has been recognized.Therefore, we're introducing a new API for controlling sound playback,specifically for AirPod stem clicks.If a capture event is triggered by something other than an AirPod click,the AVCaptureEventInteraction object will not allow control of sound feedback.To provide this new feature to AVCaptureEventInteractionusers without requiring additional work,we've added a default tone for AirPods clicks.However, this sound may not be optimal for your use case.So, you can customize the sound playbackby providing your own sound within the app’s bundle.Let’s go back to our camera app,specifically the .onCameraCaptureEvent view modifier.In iOS 26, if we leave this code unchanged,when the user clicks their AirPod stem, they will hear the default sound.Since we are building an app specifically for taking photos,the default sound may not be appropriate for our app.To tune the capture sound for our specific scenario,we first disable the default sound using defaultSoundDisabled parameter.If audio feedback is needed,we play the cameraShutter sound using the playSound method on the event object.Note, that the shouldPlaySound propertywill only be true if the capture action was triggered by an AirPod stem click.To sum up, the AVCaptureEventInteraction API significantly simplifiesthe process of building high-quality camera experiences for your apps.We reviewed the API's key features and best practices for implementation,including this year's update:a way to control camera capture with AirPod stem clicks.Now, over to Nick to talk about AVCaptureControls.Hi, I’m Nick Lupinetti.I’m a software engineer on the Camera Experience team,and I’m excited to introduce you to AVCaptureControl,a powerful way to turn Camera Control on iPhone 16into a physical piece of hardware for your camera interface.Camera Control is a versatile capture tool,which can click to launch a camera app, act as a shutter button,and make quick adjustments, all while keeping your finger in one place.Let’s take those three functions in order, starting with launch.In order to launch your app, Camera Control needs access to iteven when the device is locked.That means creating a Lock Screen capture extension.For more details, check out last year’s sessionon building a Lock Screen camera experience.Next, it’s easy to use Camera Control as a shutter button for your app.Just set up one of the capture event APIs that Vitaliy showed you earlier,and you’re done!Camera Control will run the same primary event callbackyou already provided for the volume buttons.Now, my favorite part: adjusting camera settings.Camera Control supports a light press gesture,like on a traditional camera shutter,which activates a clean preview to focus on composing,and presents a setting to adjust by sliding the control.Other settings are available for selection by light pressing twice,then sliding on Camera Control and light pressing again to confirm.At the highest level, there are two kinds of controls:sliders for numeric values, and pickers for items in a list.Sliders, in turn, come in two flavors: continuous, and discrete.Continuous sliders can select any numeric value within a specified range.For example, iOS comes out of the box with a continuous zoom slider.It supports the entire gamut of zoom factorsbetween the recommended minimumand maximum for the device it operates.Discrete sliders also select numeric values:either from an explicit set you provide,or by fixed steps from one value to another.iOS offers a built-in discrete slider to drive exposure bias,with one-third increments between plus or minus two,which are manageable units from traditional photography.Pickers, LIKE discrete sliders, allow finite selections,but are instead indexed.Each index maps to a named state for the control,like “On” and “Off” for Flash,or the names of Photographic Styles in Camera app.Okay, now that we understand the various types of controls,let’s take a look at the classes that implement them.Controls are defined in the AVFoundation framework.AVCaptureControl is the abstract base class the others inherit from.There are two system-defined subclasses,which allow any app to adopt the same behavior as the built-in camera appfor zoom or exposure bias.Finally, there are two generic subclasses, for sliders,both continuous and discrete,and pickers, whose behaviors you can define yourself.To adopt controls in your app, you add them to an AVCaptureSession.Usually, the first step in configuring a session is creating a capture device,and adding it to the session wrapped as an input.Next, any system-defined controls are created,with a reference to that same devicefor them to drive its properties on your behalf.Any controls with app-defined behaviorsare also created at this time,and finally, each control is added to the session.For your app to respond as a person interacts with Camera Control,there are a couple of flows to consider.System-provided controls apply values from Camera Controldirectly to the configured device.You don’t need to set videoZoomFactor or exposureTargetBias yourself.But you may have models or UI that need to be updated.If you already use Key-Value Observing, or KVO,to know when the relevant property changes,you can continue using that mechanism to update your interface.If you’re not using KVO,create the system control with an action handler,which will be called on the main thread as the value changes,so you can update your UI directly.App-defined controls are always created with an action callback,run on a queue that you specify.If your control needs to drive capture settings,you can do that synchronously by specifying the queuewhere you manage that state.Then, you can update your UI on the main queue.Great, now we’re ready to adoptCamera Control in the app I’m making with Vitaliy.Since he’s added a capture interaction,the control already works as a shutter button to take photos,without any additional code.But I’d also like to support zooming in and out with Camera Control,so let’s add that next.Here’s where we’re currently configuring our capture session to use a device.First we’ll check if controls are actually supported,since they’re only available on devices with a Camera Control,like iPhone 16.System-provided controls are so easy to create, it takes a single line of code.Before adding your control, make sure the session will accept it.For example, you can’t add a system-provided zoom control for a devicethat’s already associated with a zoom control.Finally, we add the control to the session.That’s all it took to make zoom work perfectly in our app using Camera Control!But there’s one problem.I also support a pinch gesture to zoom in.After I zoomed in with Camera Control,the UI wasn’t up to date,so the pinch gesture jumped back to a stale zoom factor.But that’s an easy fix.We’ll just create the slider with an action closure,which receives the zoom factor as an argument.The new value gets applied to the UIwith a delegate callback or an observable model property.Once the model behind the pinch gesture is in sync,I can zoom with Camera Control,and the pinch always starts from the right zoom factor.Now I’d like to add a control that isn’t available right out of the box.But before we create our own,let’s give some thought to what makes a great control.Camera Control, true to its name,is meant to be used with the iPhone Camera,so it should affect the appearance or experience of capturing.That makes it confusing if your controls affect unrelated areas of your app.And never introduce a capture session just to adopt Camera Control,since running the camera has power and privacy requirementsbest reserved for a capture experience.A great example of a custom control is a Pickerthat iterates through the effects or filters your app supports.Such a control typically needs to operate closer to capture than to UI.So let’s checkout what that looks like.Starting with the code we just wrote,we’ll make the zoom control one of many.Note that there’s a limit to how many controls you can add,reported by the session’s maxControlsCount property.canAddControl will return false once you reach the limit.Now we can define our effects.Your effects might be rendered on a dedicated queueusing video sample buffers, but for our app,I’ll use the reaction effects introduced with iOS 17.Here we’ve built an ordered list of effects as well as their namesout of the available unordered set.The Picker gets initialized with the effect names to showas it iterates through the options.The Picker also needs a name of its own,as well as an SF Symbol image, to distinguish it from the zoom control.It’s a great idea to disable your control when it’s not currently supported,rather than omitting it.That way, it’s still visible, just not interactive.Otherwise, another control will be selected as a fallback,and people might wonder what happened.As the user changes the Picker’s selected index,we’ll perform the corresponding reaction.We’re targeting the action to the session queue,since that’s our isolation context for managing the device.And all that’s left, is to add the picker to the array of controls.Let’s checkout how we did!As I interact with Camera Control,note how zoom is disabled in this new configuration.Let’s change the control.I’ll swipe out on the overlay, and now I can checkout my picker in the list.I’ll scroll over to it and press lightly to display its options.Now I can select each effect in turn, and watch as they play in the preview.Rock on!So, with what Vitaliy and I showed you in this session,you’ve got a ton of great tools to build a world-class camera app.We’ve seen how to capture media with physical buttons on iOS devices,including how easily this works with AirPods interactions on iOS 26.And we learned how to harness Camera Control as a convenient toolto supercharge your app’s capture interactions.And there’s even more great resources at developer.apple.com,including Human Interface Guidelines for Camera Control,guidance on setting up AirPods to test new features,and articles and sample codewith more information on how to adopt Camera Control.I can’t wait for you to build a camera that willcapture peoples’ attention.Thanks for watching!

Hi, my name is Vitaliy.I'm an engineer on the AVKit team,and welcome to 'Enhancing your camera experience with capture controls'.In this session,we will explore powerful new ways to improve user interactions in your app.With Capture Controls,you can programmatically map physical button gestures to camera actions,giving users the familiar feel of the native iOS Camera app.We'll also be showcasing an amazing new feature introduced in iOS 26,and we think you are going to love it.

To illustrate the power of Capture Controls,let’s use the camera app on iOS.Instead of taking a photo with the UI button on the screen,let’s use physical buttons on the phone.

With just a simple click of the volume up button,we can initiate capture.

But this API isn't just about simple clicks.It also supports long presses.For example, in camera mode,if we press and hold the volume down button,it'll kick off video recording.

Just like that, with Capture Controls,we have given our usersmore flexibility in using the camerato capture unforgettable moments with their iPhones.

In this session,We will first cover what physical capture isand what physical buttons are supported.Then, we’ll show you how to effectively use the API in your appto handle user interactionsand build a robust and responsive camera interfacelike we saw in the demoWe will also go over an exciting new feature in iOS 26Remote Camera Control with AirPods.

Finally, my colleague, Nick,will present an overview of Camera Control on iPhone 16.

Before exploring the API's core functionalitieslet's review the key frameworks involvedin creating a great camera experience in iOSAt the heart of our camera app's architecture is AVFoundation,providing the low-level capture functionality through APIslike AVCapturePhotoOutput for photoand AVCaptureMovieFileOutput for video.

To combine the UI layer with AVFoundation,we have AVKit that sits on top of these frameworks.

One capture-specific API that AVKit includes is AVCaptureEventInteraction.

Now, let’s dive into the key featuresand capabilities of AVCaptureEventInteraction.This API lets you override the default behaviorof physical buttons like the volume upand down or the Camera Control introduced with iPhone 16.

Another button that AVCaptureEventInteraction supportsis the Action button.Please check out our session from last yearon how to configure the Action button for capture.

Every physical button press triggers a capture event notification.This notification includes an event phase,giving you precise control over the entire press lifecycle.There are three possible phases:began: The moment the button is pressed.Perfect for preparing your app’s camera object.

cancelled: For those moments when the app goes into backgroundor capture is not available.

ended: The moment when the button is released.This is the phase when the camera object should initiate capture.

AVCaptureEventInteraction also gives you the abilityto distinguish button presses between primary and secondary actions.A button press triggers a capture notificationthat is sent only to its designated handler.

Primary actions are triggered by the volume down button,Action button, and the Camera Control,while the secondary action is triggered by the volume up button.The same button cannot trigger both handlersas they are designed to represent distinct actions.The secondary handler is optional,if it's not specified,the volume up click will trigger the primary action.Such design introduces modularity into your appand gives you more flexibilityin designing a great camera experience for your users.This API is intended for capture use cases.The system sends capture events only to apps that actively use the camera.Adopting this API overrides default physical button behavior,so apps must always respond appropriately to any events received.Failing to handle events results in a nonfunctional buttonthat provides a poor user experience.

Backgrounded capture apps,or those that do not have an active AVCaptureSession running,won’t receive any events.Any button press will trigger its default system action,such as adjusting volume or launching the camera.

Although AVCaptureEventInteraction is an API designed specifically for UIKit,SwiftUI developers can also access its functionalitythrough the onCameraCaptureEvent view modifier.Adopting either of them will result in the same behavior.

Now that we understand AVCaptureEventInteraction,let's build a simple camera appthat uses the SwiftUI onCameraCaptureEvent view modifierto allow photo capture with physical buttons.First, let’s begin by crafting the user interface for our camera app.We will include the CameraView that is responsiblefor displaying the camera output on the screen.

Next, let's add a button to the view, giving the user a way to capture photos.We'll connect this button to our CameraModel,so tapping it triggers the photo capture.

Now, we have a functional view that takes pictureswhen the user presses on the on-screen button.However, pressing on any physical buttons will trigger system actions.So, let’s make these hardware button presses camera specificwith the API we discussed in this session.First, we will import the AVKit framework.Then, we would attach the onCameraCaptureEventview modifier to the CameraViewand if the event phase is ended, we take a picture.

It’s as simple as that!With only 6 additional lines of code,we've enabled the same intuitive physical button interactions for photo captureas you find in the built-in Camera app.That's the power of leveraging the Capture Controls API.This year, we're excited to announce that AVCaptureEventInteractionwill also support primary actions triggered by clicks on either AirPod stem,specifically from AirPods equipped with the H2 chip.This will allow the user to remotely capture unforgettable momentswithout needing to interact with their phones.

For those who have already adopted the API,this feature will come for free with iOS 26.

Now, let's checkout this new feature in action.First, I'll put in my right AirPod, then the left one.

Looking sharp!Then, to configure the AirPods for capture,let's go into settingsand scroll down to the “Remote Camera Capture” section.

There, we can choose which type of click will trigger capture.I’ll choose the “Press once” option.

Now, let’s go back into the camera app.

I’ll step back a few steps,and take a photo by clicking on one of the stems.

Great, now I can control my camera without touching the device.

Because AirPods are used for remote camera control,audio feedback is crucial,as users may not be viewing the screen during capturebut need confirmation that their command has been recognized.

Therefore, we're introducing a new API for controlling sound playback,specifically for AirPod stem clicks.If a capture event is triggered by something other than an AirPod click,the AVCaptureEventInteraction object will not allow control of sound feedback.

To provide this new feature to AVCaptureEventInteractionusers without requiring additional work,we've added a default tone for AirPods clicks.

However, this sound may not be optimal for your use case.So, you can customize the sound playbackby providing your own sound within the app’s bundle.

Let’s go back to our camera app,specifically the .onCameraCaptureEvent view modifier.In iOS 26, if we leave this code unchanged,when the user clicks their AirPod stem, they will hear the default sound.Since we are building an app specifically for taking photos,the default sound may not be appropriate for our app.

To tune the capture sound for our specific scenario,we first disable the default sound using defaultSoundDisabled parameter.

If audio feedback is needed,we play the cameraShutter sound using the playSound method on the event object.Note, that the shouldPlaySound propertywill only be true if the capture action was triggered by an AirPod stem click.

To sum up, the AVCaptureEventInteraction API significantly simplifiesthe process of building high-quality camera experiences for your apps.We reviewed the API's key features and best practices for implementation,including this year's update:a way to control camera capture with AirPod stem clicks.Now, over to Nick to talk about AVCaptureControls.Hi, I’m Nick Lupinetti.I’m a software engineer on the Camera Experience team,and I’m excited to introduce you to AVCaptureControl,a powerful way to turn Camera Control on iPhone 16into a physical piece of hardware for your camera interface.Camera Control is a versatile capture tool,which can click to launch a camera app, act as a shutter button,and make quick adjustments, all while keeping your finger in one place.

Let’s take those three functions in order, starting with launch.In order to launch your app, Camera Control needs access to iteven when the device is locked.That means creating a Lock Screen capture extension.For more details, check out last year’s sessionon building a Lock Screen camera experience.

Next, it’s easy to use Camera Control as a shutter button for your app.Just set up one of the capture event APIs that Vitaliy showed you earlier,and you’re done!Camera Control will run the same primary event callbackyou already provided for the volume buttons.Now, my favorite part: adjusting camera settings.Camera Control supports a light press gesture,like on a traditional camera shutter,which activates a clean preview to focus on composing,and presents a setting to adjust by sliding the control.Other settings are available for selection by light pressing twice,then sliding on Camera Control and light pressing again to confirm.

At the highest level, there are two kinds of controls:sliders for numeric values, and pickers for items in a list.

Sliders, in turn, come in two flavors: continuous, and discrete.

Continuous sliders can select any numeric value within a specified range.For example, iOS comes out of the box with a continuous zoom slider.It supports the entire gamut of zoom factorsbetween the recommended minimumand maximum for the device it operates.Discrete sliders also select numeric values:either from an explicit set you provide,or by fixed steps from one value to another.iOS offers a built-in discrete slider to drive exposure bias,with one-third increments between plus or minus two,which are manageable units from traditional photography.

Pickers, LIKE discrete sliders, allow finite selections,but are instead indexed.Each index maps to a named state for the control,like “On” and “Off” for Flash,or the names of Photographic Styles in Camera app.Okay, now that we understand the various types of controls,let’s take a look at the classes that implement them.Controls are defined in the AVFoundation framework.AVCaptureControl is the abstract base class the others inherit from.

There are two system-defined subclasses,which allow any app to adopt the same behavior as the built-in camera appfor zoom or exposure bias.Finally, there are two generic subclasses, for sliders,both continuous and discrete,and pickers, whose behaviors you can define yourself.To adopt controls in your app, you add them to an AVCaptureSession.Usually, the first step in configuring a session is creating a capture device,and adding it to the session wrapped as an input.Next, any system-defined controls are created,with a reference to that same devicefor them to drive its properties on your behalf.Any controls with app-defined behaviorsare also created at this time,and finally, each control is added to the session.

For your app to respond as a person interacts with Camera Control,there are a couple of flows to consider.System-provided controls apply values from Camera Controldirectly to the configured device.You don’t need to set videoZoomFactor or exposureTargetBias yourself.

But you may have models or UI that need to be updated.If you already use Key-Value Observing, or KVO,to know when the relevant property changes,you can continue using that mechanism to update your interface.

If you’re not using KVO,create the system control with an action handler,which will be called on the main thread as the value changes,so you can update your UI directly.

App-defined controls are always created with an action callback,run on a queue that you specify.If your control needs to drive capture settings,you can do that synchronously by specifying the queuewhere you manage that state.

Then, you can update your UI on the main queue.

Great, now we’re ready to adoptCamera Control in the app I’m making with Vitaliy.Since he’s added a capture interaction,the control already works as a shutter button to take photos,without any additional code.

But I’d also like to support zooming in and out with Camera Control,so let’s add that next.

Here’s where we’re currently configuring our capture session to use a device.First we’ll check if controls are actually supported,since they’re only available on devices with a Camera Control,like iPhone 16.

System-provided controls are so easy to create, it takes a single line of code.

Before adding your control, make sure the session will accept it.For example, you can’t add a system-provided zoom control for a devicethat’s already associated with a zoom control.Finally, we add the control to the session.

That’s all it took to make zoom work perfectly in our app using Camera Control!But there’s one problem.I also support a pinch gesture to zoom in.After I zoomed in with Camera Control,the UI wasn’t up to date,so the pinch gesture jumped back to a stale zoom factor.

But that’s an easy fix.We’ll just create the slider with an action closure,which receives the zoom factor as an argument.The new value gets applied to the UIwith a delegate callback or an observable model property.

Once the model behind the pinch gesture is in sync,I can zoom with Camera Control,and the pinch always starts from the right zoom factor.

Now I’d like to add a control that isn’t available right out of the box.But before we create our own,let’s give some thought to what makes a great control.Camera Control, true to its name,is meant to be used with the iPhone Camera,so it should affect the appearance or experience of capturing.That makes it confusing if your controls affect unrelated areas of your app.And never introduce a capture session just to adopt Camera Control,since running the camera has power and privacy requirementsbest reserved for a capture experience.A great example of a custom control is a Pickerthat iterates through the effects or filters your app supports.

Such a control typically needs to operate closer to capture than to UI.So let’s checkout what that looks like.Starting with the code we just wrote,we’ll make the zoom control one of many.Note that there’s a limit to how many controls you can add,reported by the session’s maxControlsCount property.canAddControl will return false once you reach the limit.Now we can define our effects.Your effects might be rendered on a dedicated queueusing video sample buffers, but for our app,I’ll use the reaction effects introduced with iOS 17.Here we’ve built an ordered list of effects as well as their namesout of the available unordered set.The Picker gets initialized with the effect names to showas it iterates through the options.The Picker also needs a name of its own,as well as an SF Symbol image, to distinguish it from the zoom control.It’s a great idea to disable your control when it’s not currently supported,rather than omitting it.That way, it’s still visible, just not interactive.Otherwise, another control will be selected as a fallback,and people might wonder what happened.

As the user changes the Picker’s selected index,we’ll perform the corresponding reaction.We’re targeting the action to the session queue,since that’s our isolation context for managing the device.And all that’s left, is to add the picker to the array of controls.Let’s checkout how we did!As I interact with Camera Control,note how zoom is disabled in this new configuration.Let’s change the control.I’ll swipe out on the overlay, and now I can checkout my picker in the list.I’ll scroll over to it and press lightly to display its options.Now I can select each effect in turn, and watch as they play in the preview.

Rock on!So, with what Vitaliy and I showed you in this session,you’ve got a ton of great tools to build a world-class camera app.

We’ve seen how to capture media with physical buttons on iOS devices,including how easily this works with AirPods interactions on iOS 26.And we learned how to harness Camera Control as a convenient toolto supercharge your app’s capture interactions.And there’s even more great resources at developer.apple.com,including Human Interface Guidelines for Camera Control,guidance on setting up AirPods to test new features,and articles and sample codewith more information on how to adopt Camera Control.I can’t wait for you to build a camera that willcapture peoples’ attention.Thanks for watching!

5:35 -Initial PhotoCapture view setup

5:37 -Connecting a button to the camera model

6:10 -Importing AVKit

6:14 -Setting up onCameraCaptureEvent view modifier

8:53 -Default sound for onCameraCaptureEvent view modifier

9:13 -Play photo shutter sound on AirPod stem click

14:46 -Add a build-in zoom slider to Camera Control

15:40 -Modifying the built-in zoom slider to receive updates when zoom changes

16:46 -Adding a custom reaction-effects picker alongside zoom slider

0:00 -IntroductionLearn about using AVKit and AVFoundation to improve user interactions in camera apps. You can now programmatically map physical button gestures, such as volume "up" and "down", to camera actions, enabling people to take photos and start video recording using the phone's buttons. 

A new feature in iOS 26 is Remote Camera Control with AirPods. Learn the supported physical buttons, how to use the AVCaptureEventInteraction API, and an overview of Camera Control on the iPhone 16.2:51 -Physical captureAVCaptureEventInteraction supports the Action button; configuration details can be found in last year's session “Extend your app’s controls across the system”.3:01 -Event handlingAVCaptureEventInteraction is an API that enables you to control the lifecycle of physical button presses within camera apps. It provides three event phases: 'began', 'cancelled', and 'ended'. The 'ended' phase is when the camera object should initiate capture. 

The API distinguishes between primary and secondary actions, triggered by specific buttons (volume down, Action, Camera Control for primary; volume up for secondary). This design allows for modularity and flexibility in app design.

SwiftUI developers can access this functionality through the 'onCameraCaptureEvent' view modifier. By importing the AVKit framework and attaching this modifier to the CameraView, you can enable photo capture with physical buttons in just a few lines of code, mimicking the behavior of the built-in Camera app.6:39 -AirPods remote captureStarting in iOS 26, AirPods equipped with the H2 chip will enable remote camera control through stem clicks. Users can configure this feature in the settings app and choose which stem click action triggers photo capture. 

A new API has been introduced to provide audio feedback for these stem clicks, with a default tone included, though you can customize this sound for your specific apps. This enhancement allows people to capture moments hands-free and ensures confirmation of their commands through audio cues.10:00 -Camera controlAVCaptureControl is a new feature in iPhone 16 that enables you to create physical hardware controls for your camera interfaces. It allows people to launch camera apps and adjust camera setting quickly, and it can act as a shutter button.

The control supports two main types of adjustments: sliders (continuous and discrete) for numeric values, and pickers for items in a list. These controls are defined in the AVFoundation framework.

You can add system-defined controls for zoom or exposure bias, or create custom controls with app-defined behaviors. These controls are then added to an AVCaptureSession, and the app responds to interactions through action handlers or Key-Value Observing (KVO), updating the UI accordingly.

To enhance the camera app functionality, utilize Camera Control to easily add system-provided controls such as zoom, which requires only a single line of code. However, it is crucial to ensure the session accepts the control and to synchronize the UI with the control's state to avoid issues like stale zoom factors.

When creating custom controls, consider that Camera Control is specifically designed for capturing experiences. A good example of a custom control is a Picker that allows people to select effects or filters. This control needs to operate closer to the capture session than the UI.

You can define effects, initialize the Picker with effect names, and disable the control when not supported. By targeting the action to the session queue, you can ensure proper isolation and management of the device. 

With Capture Controls, you can build world-class camera apps that capture people's attention.

0:00 -Introduction

Learn about using AVKit and AVFoundation to improve user interactions in camera apps. You can now programmatically map physical button gestures, such as volume "up" and "down", to camera actions, enabling people to take photos and start video recording using the phone's buttons. 

A new feature in iOS 26 is Remote Camera Control with AirPods. Learn the supported physical buttons, how to use the AVCaptureEventInteraction API, and an overview of Camera Control on the iPhone 16.

Learn about using AVKit and AVFoundation to improve user interactions in camera apps. You can now programmatically map physical button gestures, such as volume "up" and "down", to camera actions, enabling people to take photos and start video recording using the phone's buttons. 

A new feature in iOS 26 is Remote Camera Control with AirPods. Learn the supported physical buttons, how to use the AVCaptureEventInteraction API, and an overview of Camera Control on the iPhone 16.

2:51 -Physical capture

AVCaptureEventInteraction supports the Action button; configuration details can be found in last year's session “Extend your app’s controls across the system”.

AVCaptureEventInteraction supports the Action button; configuration details can be found in last year's session “Extend your app’s controls across the system”.

3:01 -Event handling

AVCaptureEventInteraction is an API that enables you to control the lifecycle of physical button presses within camera apps. It provides three event phases: 'began', 'cancelled', and 'ended'. The 'ended' phase is when the camera object should initiate capture. 

The API distinguishes between primary and secondary actions, triggered by specific buttons (volume down, Action, Camera Control for primary; volume up for secondary). This design allows for modularity and flexibility in app design.

SwiftUI developers can access this functionality through the 'onCameraCaptureEvent' view modifier. By importing the AVKit framework and attaching this modifier to the CameraView, you can enable photo capture with physical buttons in just a few lines of code, mimicking the behavior of the built-in Camera app.

AVCaptureEventInteraction is an API that enables you to control the lifecycle of physical button presses within camera apps. It provides three event phases: 'began', 'cancelled', and 'ended'. The 'ended' phase is when the camera object should initiate capture. 

The API distinguishes between primary and secondary actions, triggered by specific buttons (volume down, Action, Camera Control for primary; volume up for secondary). This design allows for modularity and flexibility in app design.

SwiftUI developers can access this functionality through the 'onCameraCaptureEvent' view modifier. By importing the AVKit framework and attaching this modifier to the CameraView, you can enable photo capture with physical buttons in just a few lines of code, mimicking the behavior of the built-in Camera app.

6:39 -AirPods remote capture

Starting in iOS 26, AirPods equipped with the H2 chip will enable remote camera control through stem clicks. Users can configure this feature in the settings app and choose which stem click action triggers photo capture. 

A new API has been introduced to provide audio feedback for these stem clicks, with a default tone included, though you can customize this sound for your specific apps. This enhancement allows people to capture moments hands-free and ensures confirmation of their commands through audio cues.

Starting in iOS 26, AirPods equipped with the H2 chip will enable remote camera control through stem clicks. Users can configure this feature in the settings app and choose which stem click action triggers photo capture. 

A new API has been introduced to provide audio feedback for these stem clicks, with a default tone included, though you can customize this sound for your specific apps. This enhancement allows people to capture moments hands-free and ensures confirmation of their commands through audio cues.

10:00 -Camera control

AVCaptureControl is a new feature in iPhone 16 that enables you to create physical hardware controls for your camera interfaces. It allows people to launch camera apps and adjust camera setting quickly, and it can act as a shutter button.

The control supports two main types of adjustments: sliders (continuous and discrete) for numeric values, and pickers for items in a list. These controls are defined in the AVFoundation framework.

You can add system-defined controls for zoom or exposure bias, or create custom controls with app-defined behaviors. These controls are then added to an AVCaptureSession, and the app responds to interactions through action handlers or Key-Value Observing (KVO), updating the UI accordingly.

To enhance the camera app functionality, utilize Camera Control to easily add system-provided controls such as zoom, which requires only a single line of code. However, it is crucial to ensure the session accepts the control and to synchronize the UI with the control's state to avoid issues like stale zoom factors.

When creating custom controls, consider that Camera Control is specifically designed for capturing experiences. A good example of a custom control is a Picker that allows people to select effects or filters. This control needs to operate closer to the capture session than the UI.

You can define effects, initialize the Picker with effect names, and disable the control when not supported. By targeting the action to the session queue, you can ensure proper isolation and management of the device. 

With Capture Controls, you can build world-class camera apps that capture people's attention.

AVCaptureControl is a new feature in iPhone 16 that enables you to create physical hardware controls for your camera interfaces. It allows people to launch camera apps and adjust camera setting quickly, and it can act as a shutter button.

The control supports two main types of adjustments: sliders (continuous and discrete) for numeric values, and pickers for items in a list. These controls are defined in the AVFoundation framework.

You can add system-defined controls for zoom or exposure bias, or create custom controls with app-defined behaviors. These controls are then added to an AVCaptureSession, and the app responds to interactions through action handlers or Key-Value Observing (KVO), updating the UI accordingly.

To enhance the camera app functionality, utilize Camera Control to easily add system-provided controls such as zoom, which requires only a single line of code. However, it is crucial to ensure the session accepts the control and to synchronize the UI with the control's state to avoid issues like stale zoom factors.

When creating custom controls, consider that Camera Control is specifically designed for capturing experiences. A good example of a custom control is a Picker that allows people to select effects or filters. This control needs to operate closer to the capture session than the UI.

You can define effects, initialize the Picker with effect names, and disable the control when not supported. By targeting the action to the session queue, you can ensure proper isolation and management of the device. 

With Capture Controls, you can build world-class camera apps that capture people's attention.

## Code Samples

```swift
import
 SwiftUI


struct
 
PhotoCapture
: 
View
 {
    
var
 body: 
some
 
View
 {
        
VStack
 {
            
CameraView
()
        }
    }
}
```

```swift
import
 SwiftUI


struct
 
PhotoCapture
: 
View
 {
    
let
 camera 
=
 
CameraModel
()
    
var
 body: 
some
 
View
 {
        
VStack
 {
            
CameraView
()
            
Button
(action: camera.capturePhoto) {
                
Text
(
"Take a photo"
)
            }
        }
    }
}
```

```swift
import
 AVKit

import
 SwiftUI


struct
 
PhotoCapture
: 
View
 {
    
let
 camera 
=
 
CameraModel
()
    
var
 body: 
some
 
View
 {
        
VStack
 {
            
CameraView
()
            
Button
(action: camera.capturePhoto) {
                
Text
(
"Take a photo"
)
            }
        }
    }
}
```

```swift
import
 AVKit

import
 SwiftUI


struct
 
PhotoCapture
: 
View
 {
    
let
 camera 
=
 
CameraModel
()
    
var
 body: 
some
 
View
 {
        
VStack
 {
            
CameraView
()
            .onCameraCaptureEvent { event 
in

                
if
 event.phase 
==
 .ended {
                   camera.capturePhoto()
                }
            }
            
Button
(action: camera.capturePhoto) {
                
Text
(
"Take a photo"
)
            }
        }
    }
}
```

```swift
.onCameraCaptureEvent { event
	
if
 event.phase 
==
 .ended {
   	camera.capturePhoto() 
  }
}
```

```swift
.onCameraCaptureEvent(defaultSoundDisabled: 
true
) { event 
in

    
if
 event.phase 
==
 .ended {a
        
if
 event.shouldPlaySound {d
            event.play(.cameraShutter)
        }
    }
    camera.capturePhoto()
 }
```

```swift
captureSession.beginConfiguration()


// configure device inputs and outputs



if
 captureSession.supportsControls {
    
let
 zoomControl 
=
 
AVCaptureSystemZoomSlider
(device: device)

    
if
 captureSession.canAddControl(zoomControl) {
        captureSession.addControl(zoomControl)
    }
}

captureSession.commitConfiguration()
```

```swift
let
 zoomControl 
=
 
AVCaptureSystemZoomSlider
(device: device) { [
weak
 
self
] zoomFactor 
in

    
self
?
.updateUI(zoomFactor: zoomFactor)
}
```

```swift
let
 reactions 
=
 device.availableReactionTypes.sorted { 
$0
.rawValue 
<
 
$1
.rawValue }

let
 titles 
=
 reactions.map { localizedTitle(reaction: 
$0
) }

let
 picker 
=
 
AVCaptureIndexPicker
(“
Reactions
", symbolName: “face.smiling.inverted”,
    localizedIndexTitles: titles)

picker.isEnabled = device.canPerformReactionEffects
picker.setActionQueue(sessionQueue) { index in
    device.performEffect(for: reactions[index])
}

let controls: [AVCaptureControl] = [zoomControl, picker]

for control in controls {
    if captureSession.canAddControl(control) {
        captureSession.addControl(control)
    }
}
```

