# Wwdc2025 308

## Transcript

More Videos

Streaming is available in most browsers,and in the Developer app.

About

Summary

Transcript

Code

Optimize CPU performance with InstrumentsLearn how to optimize your app for Apple silicon with two new hardware-assisted tools in Instruments. We'll start by covering how to profile your app, then dive deeper by showing every single function called with Processor Trace. We'll also discuss how to use CPU Counters' modes to analyze your code for CPU bottlenecks.Chapters0:00 -Introduction & Agenda2:28 -Performance mindset8:50 -Profilers13:20 -Span14:05 -Processor Trace19:51 -Bottleneck analysis31:33 -Recap32:13 -Next stepsResourcesAnalyzing CPU usage with the Processor Trace instrumentApple Silicon CPU Optimization GuidePerformance and metricsTuning your code’s performance for Apple siliconHD VideoSD VideoRelated VideosWWDC25Improve memory usage and performance with SwiftOptimize SwiftUI performance with InstrumentsProfile and optimize power usage in your appWWDC24Explore Swift performanceWWDC23Analyze hangs with InstrumentsWWDC22Visualize and optimize Swift concurrency

Learn how to optimize your app for Apple silicon with two new hardware-assisted tools in Instruments. We'll start by covering how to profile your app, then dive deeper by showing every single function called with Processor Trace. We'll also discuss how to use CPU Counters' modes to analyze your code for CPU bottlenecks.

0:00 -Introduction & Agenda

2:28 -Performance mindset

8:50 -Profilers

13:20 -Span

14:05 -Processor Trace

19:51 -Bottleneck analysis

31:33 -Recap

32:13 -Next steps

Analyzing CPU usage with the Processor Trace instrument

Apple Silicon CPU Optimization Guide

Performance and metrics

Tuning your code’s performance for Apple silicon

HD VideoSD Video

HD Video

SD Video

Improve memory usage and performance with Swift

Optimize SwiftUI performance with Instruments

Profile and optimize power usage in your app

Explore Swift performance

Analyze hangs with Instruments

Visualize and optimize Swift concurrency

Search this video…Hi, I'm Matt and I'm an OS kernel engineer.Today, I'll teach you how to use Instruments to optimize code for Apple silicon CPUs.Using CPU resources efficiently can avoid noticeable wait timeswhen your app needs to process large amounts of data or quickly respondto an interaction.But predicting how software performs is difficult for two reasons.The first is the layers of abstraction between your Swift source codeand what actually ends up running.The source code you write for your app is compiled into machine instructionsthat eventually execute on a CPU.But your code doesn't run in isolation:it's augmented by compiler-generated support code, the Swift runtime,and other system frameworks, all of which may invokekernel system calls to handle privileged operations on behalf of your app.This makes it hard to know the cost of software abstractions your code relies on.The second reason that your code's performance is difficult to predictis how the CPU carries out the instructions it's given.Within a single CPU, its functional units work in parallelto execute instructions efficiently.To support this, instructions will be executed out-of-order,only giving the appearance of in-order execution.And CPUs also benefit from several layers of memory cachesthat ensure quick access to data.These characteristics dramatically accelerate common coding patterns,such as linear scans through memory or defensive checkslike early exits for rare conditions.But some data structures, algorithms, or implementation approachesare difficult for the CPU to execute efficiently without careful optimizationor even significant restructuring.We're going to cover the right path to optimizing your code for the CPU.We'll start by reviewing how to approach performance investigations,guided by data to focus on the biggest potential speed-ups first.Then, we'll consider the traditional profiling approachesthat are a great first step in identifying excessive CPU usage in your code.To dive deeper and fill in the gaps from profiling,we'll use Processor Traceto record every instruction and measure the costs of software abstractions.And finally, we'll use the improved CPU Counters instrument to analyzeCPU bottlenecks and understand how to micro-optimize our algorithm.Let's get started by putting ourselves in the right mindsetto approach performance investigations.The first step is to keep an open mind:sources of slowdowns can be surprising and unexpected.Collect data to test assumptions and verify that your mental modelfor how the code executes is accurate.For instance, consider other causes of slowdownsin addition to single-threaded CPU performance.Aside from executing on a CPU,threads and tasks can also spend time blocked,waiting on resources like files or access to shared mutable state.The "Visualize and optimize Swift concurrency" session covers toolsfor understanding why your tasks might be off-CPU.When your threads are unblocked and on-CPU,it's possible there's an API that's being misused,like applying the wrong quality of service class to your codeor implicitly creating too many threads.Read the documentation on Tuning your code's performance for more details.But if it's efficiency that's the problem, you either need to change the algorithmand its associated data structuresor the implementation, which is how the algorithmis expressed in the programming language.Use tools to determine which branches of this tree you should focus on first.Try Xcode's built-in CPU Gauge to detect if CPUs are heavily usedwhile interacting with your app.To analyze the blocking behaviors between threads and which threads will ultimatelyunblock them, use the System Trace instrument.And for issues impacting the UI or your app's main thread,use the specialized Hangs instrument.The Analyzing hangs with Instruments session contains more detailson how to confirm that your app's CPU usage needs to be optimized.But even with guidance from tools, be careful about the kinds of optimizationsyou implement.Ruthless micro-optimization can make your code more difficultto extend and reason about.And it often relies on compiler optimizations that can be fragile,like auto-vectorization or reference count elision.Before you commit to intrusive micro-optimizations, seek out alternativesthat might avoid the slow operations altogether.Consider why the code is running in the first place.Maybe you can avoid doing the work at all and just delete the code.Thanks for watching this session and...just kidding.But seriously,This usually is impossible, but it's a good check on your assumptionsabout how important the results of the work are.You could also try to do the work later outside of the critical path,or only when the results will be visible to someone.In the same vein, precomputing values can also hide the timeit takes to complete the work.This might even involve baking in values at build time.However, these approaches could consume power unnecessarilyor increase the download size of your app.For repeated operations with the same inputs, caching is another solution,but one that often comes with its own set of hard problems,like cache invalidation or dealing with increased memory usage.Once you've exhausted attempts to avoid doing the workwhen its performance is noticeable,you need the CPU to do the work faster.That's what we'll be focusing on today.Prioritize your optimization efforts on code that will havethe largest impact on user experience.Usually, this is code involved in the critical path of someone interactingwith your app, where they'll notice performance issues,but can also be longer-running operations that consume power.For this session, we'll focus on searching through prepared lists of integersbecause it's on my app's critical path.My app is already using binary search, a classic algorithm that uses a sorted arrayto find an element by successively halving the search space.In this example, there are 16 elements in the array and we're searchingfor the element holding the number 5.5 is less than the element in the middle of the array, 20,so its element must be in the first half.5 is also smaller than the element in the middle of the first half, 9,so it must be in the first quarter of the array.We narrow in on the matching element after comparing with 3, in only 4 steps.This is the binary search implementation from a framework used by my app.It's a standalone functionthat uses the metaphor of finding a needle in a haystackto name its parameters.It supports searching for a Comparable needle through the haystack Collection.The algorithm tracks two variables:the start of our current search area in startand the number of elements remaining to search in length.While there's remaining elements to search,it checks the middle value of the search space.If the needle is less than the value, then it just halves the search space,leaving start intact.If the needle is equal, then the element has been foundand the middle index is returned.Otherwise, the start position needs to be adjustedto just after the middle element and the search space reduced by half.We'll prepare to optimize this algorithm incrementally,confirming at each step that we're making progressby comparing search throughput,or the number of searches the algorithm can complete each second.Don't feel limited to only taking huge leaps forward every time you make a change:some optimizations might be hard to quantify, but small improvements will add up over time.To help us optimize continuously,I've written an automated test to measure search throughput.We don't need a particularly robust setup: we just want an estimate of performance.This repeat-while loop calls the search closureuntil the specified duration has elapsed.I'm using an OS signpost interval around the calls to the search closureso tools can narrow in on the portion of the test we're optimizing.And I chose the points of interest category since Instruments includes it by default.The timing itself uses a ContinuousClock, which unlike Date,can't go backwards and has low-overhead.It's a simple, but effective approach to gathering some rough dataon the performance of the algorithm.My test is called searchCollection and simulates how my app uses binary search.We'll run the searches for one second with a descriptive name for the signposts,in case we run multiple tests in a single recording.A loop inside the closure will invoke the binary search function to amortizethe cost of checking the time.Let's run this test under an Instruments profiler to analyzethe CPU performance of binary search.There are two CPU-focused profilers to choose from:the Time Profiler and the CPU Profiler.The classic Time Profiler instrument periodically samples what's runningon the system's CPUs, based on a timer.In this example, we have some work happening on two CPUs.At each sample point, Time Profiler captures the user space call stacksof each thread running on the CPU.Instruments can then visualize those samples as a call tree or flame graphin its detail view to give an approximation of what codeis important to optimize for CPU performance.This is helpful for analyzing how work is distributed over time or which threadsare active at the same time.But using a timer to sample call stacks suffers from a problem called aliasing.Aliasing is when some periodic work on the system happens at the same cadenceas the sampling timer.Here, the blue regions are responsible for most of the CPU time,but the orange functions happen to be running whenever the sampler collects a call stack.This makes orange unfairly over-represented in the Instruments call tree.To avoid this problem, we can switch to the CPU Profiler.It samples CPUs independently, based on the clock frequency of each CPU.You should prefer CPU Profiler over Time Profiler for CPU optimizationbecause it's more accurate and more fairly weights software consuming CPU resources.These bells represent when a CPU's cycle counter samples the running call stack.Apple silicon CPUs are asymmetric and some of them will run at a slower,but more power-efficient clock frequency than others.Individual CPUs that scale up their frequency will be sampled more often,without Time Profiler's bias against faster-running CPUs.We'll use CPU Profiler to find out which parts of my binary search functionconsume the most CPU cycles.In Xcode's test navigatoryou can quickly launch Instruments from a unit testby secondary-clicking on the test nameand selecting its Profile item.In this case, we'll select Profile searchCollection.This opens Instruments and presents the template chooser.I'll choose CPU Profiler.In the recorder settings, we'll switch to deferred modefor lower overhead and start recording.The default immediate mode for profilers can be useful to confirmyour interactions with an app are captured.But for an automated test on the same machine as Instruments,we want to minimize any overhead that might be added by our toolsby waiting until the recording stops to analyze it.New documents in Instruments are often daunting.The window is divided into two halves.The top holds the tracks that show activity on the timeline.Each track can contain multiple lanes with charts to indicate levels or regions.Below the timeline is the detail view with summary informationabout the timeline range being inspected.Any extended details are shown on the right side.To get oriented, we'll find the region where searches are happeningin the Points of Interest track.Secondary-clicking on the region will offer to set the inspection range,limiting the detail view below to only the data captured in the signpost interval.We can click on the test runner process trackto show the CPU profile in the detail view, below the timeline.This view shows a call tree of the functions in the testthat were sampled by each CPU's cycle counter.Holding down option and clicking on the chevron next to the first function listedwill expand the tree until the first pointwhere the sample counts diverge significantly,which is close to our binary search function.We'll focus on our binary search function by clicking the arrow next to its nameand selecting the Focus on Subtree item.Each function is weighted by the sample countmultiplied by the number of cycles between each sample.This call tree is showing a lot of samples taken in functions called by binary searchto deal with the Collection type.This protocol witness is showing up in about a quarter of our samples.There are allocations and even Array's checks for Objective-C types.We can avoid the overheads of Arrayand the generics if we switch to a container typethat better matches the kind of data we're searching through.Let's try the new Span type.Span can be used instead of Collection when elements are stored contiguously in memory,which is common for many kinds of data structures.It's effectively a base address and count.But it also prevents escaping, or leaking, the memory referenceoutside of the functions it's used in.For more details on Span, watchthe Improve memory usage and performance with Swift session.Adopting Span only requires changing the haystack and return types to Span,the algorithm itself is unchanged.This small change makes searching four times faster.But this version of binary search is still impacting my appand I'd like to investigate whether Span's bounds checkingis contributing to the overhead.To dig in deeper, we'll switch to a new tool called Processor Trace.Starting in Instruments 16.3, Processor Trace can collect a complete traceof all the instructions your app's process executes in user space.This is a fundamental shift in how you can measure software performance:there's no sampling biasand only a negligible 1% impact to your app's performance.Processor Trace requires specialized CPU featuresthat are only available on Mac and iPad Pro with M4 or iPhone with A18.Before we jump in, we'll need to set up our device for processor tracing.On a Mac, turn on the setting under Privacy & Security and Developer Tools.For iPhone or iPad, the setting is located in the Developer section.For the best experience with Processor Trace,try to limit your tracing to only a few seconds.Unlike sampling with CPU Profiler, you don't need to batch up your work:even a single instance of the code you want to optimize can be enough.Let's run Processor Trace on the Span version of binary search.Our test now only needs a handful of iterations.To profile this test, I'll secondary-click the test icon in the line number gutter.This shows the same menu as the one we used before, but can be more convenientthan switching navigators.I'll select the Processor Trace templateand start recording.Processor Trace has to deal with a lot of data,so capturing and analyzing it can take some time.Processor Trace sets up the CPU to record every branching decision.The cycle count and the current time are also recorded to track how long the CPUspends in each function.Instruments then uses your app's and system framework's executable binariesto reconstruct an execution path and annotate function callswith the cycles elapsed and time durations.We limit the time spent tracing because even though the CPU is recordingas little information as possible,it can still be gigabytes of data per secondfor a multi-threaded application.Now that the document is ready, let's zoom in so we can inspectour binary search function calls.Searching now only takes up a tiny fraction of the full recording,so we'll find it in the Regions of Interest Listin the details view below the timelineand then secondary-click its row and select Set Inspection Range and Zoom.To locate the thread that's executing binary search,we'll secondary-click the Start Thread cell and select Pin Thread in Timeline.Processor Trace adds a new function call flame graph to each thread track,so I'll drag the pin's divider up to make space for it.Processor Trace shows execution visually as a flame graph.A flame graph is a graphical representation of function costs and relationships:the width of the bars represents how much time the function took to executeand the rows represent nested call stacks.But most flame graphs show data from sampling and their cost is onlyan estimate based on a sample count.Processor Trace's timeline flame graph is different:it shows the calls made over timeexactly as they would have been executed on the CPU.The colors of each bar represent the kinds of binaries they're from:brown for system frameworks,magenta for the Swift runtime and standard library,and blue for code compiled into your app's binary or any custom frameworks.The first part of this trace is showing the overhead of emitting the signpost,so let's zoom in further on the binary search code near the end of the range.I'll hold down option, click, and drag across the timeline to zoom in.I can select any binary search function call from the 10 iterationsand set the inspection range and zoom with a secondary click.This is the power of Processor Tracewe can see all the calls made by a single functionthat only runs for a few hundred nanoseconds.We could zoom in even further,but let's use the Function Calls summary below the timeline.This shows the same information as the timeline, but as a table,with the full names of functions being called for short periods of time.I'll sort this table by cycles.My initial assumption that bounds checks were causing slowdowns was wrong.This implementation of binary search is still dealing with protocol metadata overheadsand can't inline the number comparisons, which ends up being a sizablepercentage of the search's total cycle count.This is because the generic Comparable parameter is not specialized for the typeof elements being used.Since my code is in a framework linked by my app,the Swift compiler cannot producea specialized version of binary search, just for the types passed by callers.When this causes overheads in code from a framework,you should add the inlinable annotation to the framework's functionto generate specialized implementationswithin the framework client's binary executables.But inlining can make code harder to analyzebecause it gets mixed with any callers.I'd like to avoid inlining into the test harness, so for this function,I'll just manually specialize it for the Int type used by the appand test, with a new function name.The code loses a lot of generality while becoming about 1.7 times faster.We need to keep optimizingbecause binary search is still contributing to slowdowns in the app.It's pretty odd to spend so much time on optimizing a single functionyou'll periodically reassess and collect more data to notice a different partof your code responsible for the inefficiency.Our specialized Span binary search isn't showing any unexpected function callsin the Processor Trace either, so we should understand how the code is runningon the CPU to make further progress.With the CPU Counters instrument, we can find out what bottlenecksthe code is hitting when running on CPU.Before we start using Instruments again,we need to build a mental model of how a CPU works.At a basic level, a CPU is just following a list of instructions,modifying registers and memory, and interacting with peripheral devices.When a CPU is executing, it has to follow a series of steps,which are broadly categorized into two phases.The first is Instruction Delivery to ensure the CPU has instructions to execute.Then, Instruction Processing is responsible for executing them.In Instruction Delivery, instructions are fetched and decoded into micro-operationsthat are easier for the CPU to execute.Most instructions decode into a single micro-operation, but some instructionsdo multiple things, like issuing a memory request and incrementing an index value.To process a micro-operation, it's sent to the map and schedule units,which route and dispatch the operation.From there, the operation is assigned to an execution unit, or the load-store unitif the operation needs to access memory.It would be pretty slow if the CPU had to run these phases seriallybefore it can start to fetch again, so Apple silicon processors are pipelined.Once one unit is finished with work, it can move onto the next operation,keeping each unit busy.Pipelining and making extra copies of the execution unitssupports instruction-level parallelism.It's different from process or thread-level parallelismthat you would access with Swift Concurrency or Grand Central Dispatch,where multiple CPUs execute different operating system threads.Instruction-level parallelism allows a single CPU to take advantage of timeswhen units might otherwise be idle and use hardware resources efficiently,keeping all parts of the pipeline busy.Your Swift source code is not directly in control of this parallelism,but instead must help the compiler generate an amenable sequence of instructions.Unfortunately parallelizable instruction sequences are not immediately intuitivedue to the interaction between units in a CPU.Every arrow between the units shows where operations can stall in the pipeline,limiting the available parallelism.We call these bottlenecks.To find out which bottleneck is relevant to our workload,Apple silicon CPUs can count interesting events in each unitand other characteristics of the instructions being executed.The CPU Counters instrument reads these counters to buildhigher-level metrics out of them.This year, we've added preset modes for these counters to make them much easier to use.Instruments uses them in a guided, iterative methodology for analyzingthe performance of your code, called bottleneck analysis.Let's use it to find out why our binary search is still slow, despite not havingany apparent function call overheads.The CPU Counters instrument relies on sampling the workload, so we need toreturn to the test harness we used with CPU Profiler to measure throughput again.Let's profile a test of the specialized Span implementation with Instruments.We'll select the CPU Counters template.There's now a guided configuration with curated modes to measure with.If you're curious about what each mode does, there's documentation availableby clicking on the info icon next to the mode selection.Let's get counting.This initial CPU Bottlenecks mode breaks up the work done by the CPUinto four broad categories that account for all of the CPU's potential performance.Instruments shows these categories as a colored stacked bar chartand a summary table in the detail view.During recording, Instruments will collect CPU counter data for the threadsused in our test and convert them into bottleneck percentages.We'll use the Points of Interest to orient ourselves, like before,to zoom and select our searches.We'll then pin the thread that is running our binary search implementation to the timeline.Hovering over its CPU Bottleneck lane shows a high percentagein the discarded bottleneck.The details view below shows the metric aggregations in the inspection range.Selecting the Discarded Bottleneck row shows a description in the extended detailview on the right.Instruments also presents a remark above the chart in the timeline.Clicking that remark shows more details below.This is useful, but I still don't knowwhat part of search is responsible for the bottleneck.Secondary-clicking the Discarded Sampling cellunder the Suggested Next Mode columnoffers the option to profile the workload againwith a different mode.Let's try it out.This mode is a little different from CPU Bottlenecks.It's still gathering counter data, but it's also setting up the countersto trigger sampling.The sample data is just limited to the instructionthat's generating discarded work.To show those, let's orient ourselves again with Points of Interest.Then, we'll select the test process trackand navigate to Instruction Samples below the timeline.This isn't a call stack, but the exact instruction that's causing the problem.We can open the Source Viewer by clicking on the arrow next to the function nameto show the source code that was sampledbecause the CPU followed the wrong branch direction.Here, the comparisons being donebetween the needle and the middle value are incorrectly predicted.To understand why these source lines are responsible for so many bad predictions,we need to learn a bit more about CPUs.CPUs are sneaky and execute instructions out-of-order.It only appears like instructions execute sequentiallythanks to an extra re-ordering step as instructions complete.This means CPUs look ahead and make predictionsabout which instructions will execute next.The branch predictors responsible are usually accurate, but can take wrong pathswhen there's no consistent pattern from prior executionabout whether a branch will be taken or not.The loop in our binary search algorithm has two kinds of branches.The first loop condition is usually taken until the end of the loop,so is well-predicted and didn't show up in sampling.But the check for the needle is effectively a random branch,so it's no wonder the predictors have trouble with it.I've rewritten the loop body to avoid difficult to predictbranches that affect control flow.The if statement's body is only assigning a value based on the condition.This allows the Swift compiler to generate a conditional move instruction,and avoid branching to a different instruction.Returning from a function or breaking a loop based on a conditionmust be implemented with a branch, so I also had to remove the early return.I've used unchecked arithmetic to avoid branches that would terminate the program.This is one of the areas where micro-optimization becomes fragileand easy to disrupt, not to mention less safe and understandable.When we make a change like this, you should go back to the initialCPU Bottlenecks mode to check how it impacts the rest of the bottlenecks.I've already collected a trace of our new, branchless binary search,which is now about twice as fast as the version with branches.It's now almost completely bottlenecked on Instruction Processing.Instruments indicates that we should re-run the workloadwith the Instruction Processing mode.That mode had remarks that recommended running the L1D Cache Miss Sampling mode.The cache miss samples show that accessing memory from the array is the reasonthe CPU isn't able to execute instructions efficiently.Let's learn more about CPUs and memory to find out why.CPUs access memory through a hierarchy of caches that make repeated accessesto the same address, or even predictable access patterns, much faster.It starts with the L1 caches that are located within each CPU.These can't store much data, but offer the fastest access to memory.A slower L2 cache sits outside of the CPUs and provides a lot more headroom.And finally, requests that miss both caches and need to access main memorybecome 50 times slower than the fast path.These caches also group memory into 64- or 128 byte segments called cache lines:even if an instruction only requests 4 bytes, the caches will pull in more datawith the expectation that subsequent instructionswill need to access other bytes nearby.Let's consider how this impacts our binary search algorithm.In this example, the blue lines are elements in the array,while the grey capsules are the cache lines that the CPU caches operate on.The array starts completely out of the cache.The first comparison brings in a cache line and several elements into the L1 data cache.But the next comparison experiences a cache miss.And subsequent iterations keep missing in the cache.This continues until the search narrows in on a cache-line sized region.Binary search turns out to be a pathological casefor the CPU's memory hierarchy.But if we can tolerate reordering the elementsto be more friendly to the cache,then we can put the search points on the same cache line.This is called an Eytzinger layout, after a 16th century Austrian genealogistwho organized family trees this way.This isn't a general optimization we can make without significant consequences.This improves search speed at the cost of in-order traversal speed,making that operation now miss in the cache.Let's go back to the first example of binary search to show how to re-arrangea sorted array to an Eytzinger layout.Starting with the middle element as the root,we'll model the binary search operation as a tree,where midpoints are descendant nodes.An Eytzinger layout is arranged as the breadth-first traversal of that tree.The elements closer to the root of the tree are arranged more densely,and are more likely to share cache lines.Searching for 5 again now spends the first three steps in the same cache line.The leaf nodes are sorted at the end of the array,which will incur an unavoidable cache miss.I've recorded a CPU Bottlenecks trace of the Eytzinger binary search,which shows it's two times faster again than the branchless search.But this example highlights something interesting,it's still technically bottlenecked on Instruction Processing.We've made our implementation more cache-friendly, but the workload is stillinherently memory-bound.You should monitor performance to know when to stop and optimize other codein your app,because our search is now no longer impactingcritical path performance.During this process, we've significantly improved search throughput.First, with CPU Profiler, we were able to get a significant speedup when switchingfrom Collection to Span.Then, Processor Trace showed us the overheads of unspecialized generics.And finally, we dialed up the performance significantlywith some micro-optimizations, guided by Bottleneck Analysis.Overall, we made our search function about 25 times faster with Instruments.To achieve these speedups, we started in the correct mindset,using tools to confirm our guesses and develop an intuitionabout the cost of abstractions.We successively applied more detailed tools to find unexpected overheads.These were as easy to address as they are to overlook if you don't actually measure.And then, once software overheads were addressed,we looked at CPU bottleneck-focused optimizations.We became more aware of and maybe even sympathetic to features of the CPUthat are taken for granted.This order was important: we have to ensure the CPU-focused toolsaren't confused by extra software runtime overheads.To apply this to your own apps,collect data and follow leads with the performance mindset.Write performance tests so you can repeatedly measure with these Instruments.Provide feedback or ask questions about using the tools on the forums.Watch the sessions I mentioned earlier and the WWDC24 session on Swift performance,which will help you build a more accurate mental model of the costsof Swift's powerful abstractions.And to better understand how CPUs execute your code,read the Apple Silicon CPU Optimization Guide.Thanks for watchingand I hope you have fun using Instruments to find optimization needlesin your code's haystacks.

Hi, I'm Matt and I'm an OS kernel engineer.Today, I'll teach you how to use Instruments to optimize code for Apple silicon CPUs.Using CPU resources efficiently can avoid noticeable wait timeswhen your app needs to process large amounts of data or quickly respondto an interaction.But predicting how software performs is difficult for two reasons.

The first is the layers of abstraction between your Swift source codeand what actually ends up running.

The source code you write for your app is compiled into machine instructionsthat eventually execute on a CPU.

But your code doesn't run in isolation:it's augmented by compiler-generated support code, the Swift runtime,and other system frameworks, all of which may invokekernel system calls to handle privileged operations on behalf of your app.

This makes it hard to know the cost of software abstractions your code relies on.

The second reason that your code's performance is difficult to predictis how the CPU carries out the instructions it's given.Within a single CPU, its functional units work in parallelto execute instructions efficiently.

To support this, instructions will be executed out-of-order,only giving the appearance of in-order execution.And CPUs also benefit from several layers of memory cachesthat ensure quick access to data.These characteristics dramatically accelerate common coding patterns,such as linear scans through memory or defensive checkslike early exits for rare conditions.

But some data structures, algorithms, or implementation approachesare difficult for the CPU to execute efficiently without careful optimizationor even significant restructuring.We're going to cover the right path to optimizing your code for the CPU.We'll start by reviewing how to approach performance investigations,guided by data to focus on the biggest potential speed-ups first.Then, we'll consider the traditional profiling approachesthat are a great first step in identifying excessive CPU usage in your code.To dive deeper and fill in the gaps from profiling,we'll use Processor Traceto record every instruction and measure the costs of software abstractions.And finally, we'll use the improved CPU Counters instrument to analyzeCPU bottlenecks and understand how to micro-optimize our algorithm.Let's get started by putting ourselves in the right mindsetto approach performance investigations.The first step is to keep an open mind:sources of slowdowns can be surprising and unexpected.Collect data to test assumptions and verify that your mental modelfor how the code executes is accurate.

For instance, consider other causes of slowdownsin addition to single-threaded CPU performance.Aside from executing on a CPU,threads and tasks can also spend time blocked,waiting on resources like files or access to shared mutable state.

The "Visualize and optimize Swift concurrency" session covers toolsfor understanding why your tasks might be off-CPU.

When your threads are unblocked and on-CPU,it's possible there's an API that's being misused,like applying the wrong quality of service class to your codeor implicitly creating too many threads.Read the documentation on Tuning your code's performance for more details.But if it's efficiency that's the problem, you either need to change the algorithmand its associated data structuresor the implementation, which is how the algorithmis expressed in the programming language.Use tools to determine which branches of this tree you should focus on first.Try Xcode's built-in CPU Gauge to detect if CPUs are heavily usedwhile interacting with your app.To analyze the blocking behaviors between threads and which threads will ultimatelyunblock them, use the System Trace instrument.

And for issues impacting the UI or your app's main thread,use the specialized Hangs instrument.The Analyzing hangs with Instruments session contains more detailson how to confirm that your app's CPU usage needs to be optimized.But even with guidance from tools, be careful about the kinds of optimizationsyou implement.Ruthless micro-optimization can make your code more difficultto extend and reason about.And it often relies on compiler optimizations that can be fragile,like auto-vectorization or reference count elision.Before you commit to intrusive micro-optimizations, seek out alternativesthat might avoid the slow operations altogether.Consider why the code is running in the first place.Maybe you can avoid doing the work at all and just delete the code.

Thanks for watching this session and...just kidding.But seriously,This usually is impossible, but it's a good check on your assumptionsabout how important the results of the work are.

You could also try to do the work later outside of the critical path,or only when the results will be visible to someone.

In the same vein, precomputing values can also hide the timeit takes to complete the work.This might even involve baking in values at build time.However, these approaches could consume power unnecessarilyor increase the download size of your app.For repeated operations with the same inputs, caching is another solution,but one that often comes with its own set of hard problems,like cache invalidation or dealing with increased memory usage.Once you've exhausted attempts to avoid doing the workwhen its performance is noticeable,you need the CPU to do the work faster.That's what we'll be focusing on today.Prioritize your optimization efforts on code that will havethe largest impact on user experience.Usually, this is code involved in the critical path of someone interactingwith your app, where they'll notice performance issues,but can also be longer-running operations that consume power.For this session, we'll focus on searching through prepared lists of integersbecause it's on my app's critical path.

My app is already using binary search, a classic algorithm that uses a sorted arrayto find an element by successively halving the search space.In this example, there are 16 elements in the array and we're searchingfor the element holding the number 5.

5 is less than the element in the middle of the array, 20,so its element must be in the first half.

5 is also smaller than the element in the middle of the first half, 9,so it must be in the first quarter of the array.We narrow in on the matching element after comparing with 3, in only 4 steps.

This is the binary search implementation from a framework used by my app.It's a standalone functionthat uses the metaphor of finding a needle in a haystackto name its parameters.It supports searching for a Comparable needle through the haystack Collection.The algorithm tracks two variables:the start of our current search area in startand the number of elements remaining to search in length.While there's remaining elements to search,it checks the middle value of the search space.If the needle is less than the value, then it just halves the search space,leaving start intact.If the needle is equal, then the element has been foundand the middle index is returned.Otherwise, the start position needs to be adjustedto just after the middle element and the search space reduced by half.

We'll prepare to optimize this algorithm incrementally,confirming at each step that we're making progressby comparing search throughput,or the number of searches the algorithm can complete each second.Don't feel limited to only taking huge leaps forward every time you make a change:some optimizations might be hard to quantify, but small improvements will add up over time.

To help us optimize continuously,I've written an automated test to measure search throughput.We don't need a particularly robust setup: we just want an estimate of performance.This repeat-while loop calls the search closureuntil the specified duration has elapsed.

I'm using an OS signpost interval around the calls to the search closureso tools can narrow in on the portion of the test we're optimizing.And I chose the points of interest category since Instruments includes it by default.

The timing itself uses a ContinuousClock, which unlike Date,can't go backwards and has low-overhead.It's a simple, but effective approach to gathering some rough dataon the performance of the algorithm.

My test is called searchCollection and simulates how my app uses binary search.We'll run the searches for one second with a descriptive name for the signposts,in case we run multiple tests in a single recording.

A loop inside the closure will invoke the binary search function to amortizethe cost of checking the time.

Let's run this test under an Instruments profiler to analyzethe CPU performance of binary search.There are two CPU-focused profilers to choose from:the Time Profiler and the CPU Profiler.

The classic Time Profiler instrument periodically samples what's runningon the system's CPUs, based on a timer.

In this example, we have some work happening on two CPUs.At each sample point, Time Profiler captures the user space call stacksof each thread running on the CPU.

Instruments can then visualize those samples as a call tree or flame graphin its detail view to give an approximation of what codeis important to optimize for CPU performance.This is helpful for analyzing how work is distributed over time or which threadsare active at the same time.But using a timer to sample call stacks suffers from a problem called aliasing.Aliasing is when some periodic work on the system happens at the same cadenceas the sampling timer.Here, the blue regions are responsible for most of the CPU time,but the orange functions happen to be running whenever the sampler collects a call stack.This makes orange unfairly over-represented in the Instruments call tree.

To avoid this problem, we can switch to the CPU Profiler.

It samples CPUs independently, based on the clock frequency of each CPU.You should prefer CPU Profiler over Time Profiler for CPU optimizationbecause it's more accurate and more fairly weights software consuming CPU resources.

These bells represent when a CPU's cycle counter samples the running call stack.Apple silicon CPUs are asymmetric and some of them will run at a slower,but more power-efficient clock frequency than others.Individual CPUs that scale up their frequency will be sampled more often,without Time Profiler's bias against faster-running CPUs.

We'll use CPU Profiler to find out which parts of my binary search functionconsume the most CPU cycles.

In Xcode's test navigatoryou can quickly launch Instruments from a unit testby secondary-clicking on the test nameand selecting its Profile item.

In this case, we'll select Profile searchCollection.

This opens Instruments and presents the template chooser.I'll choose CPU Profiler.

In the recorder settings, we'll switch to deferred modefor lower overhead and start recording.The default immediate mode for profilers can be useful to confirmyour interactions with an app are captured.But for an automated test on the same machine as Instruments,we want to minimize any overhead that might be added by our toolsby waiting until the recording stops to analyze it.

New documents in Instruments are often daunting.The window is divided into two halves.The top holds the tracks that show activity on the timeline.Each track can contain multiple lanes with charts to indicate levels or regions.

Below the timeline is the detail view with summary informationabout the timeline range being inspected.Any extended details are shown on the right side.

To get oriented, we'll find the region where searches are happeningin the Points of Interest track.

Secondary-clicking on the region will offer to set the inspection range,limiting the detail view below to only the data captured in the signpost interval.We can click on the test runner process trackto show the CPU profile in the detail view, below the timeline.This view shows a call tree of the functions in the testthat were sampled by each CPU's cycle counter.Holding down option and clicking on the chevron next to the first function listedwill expand the tree until the first pointwhere the sample counts diverge significantly,which is close to our binary search function.We'll focus on our binary search function by clicking the arrow next to its nameand selecting the Focus on Subtree item.

Each function is weighted by the sample countmultiplied by the number of cycles between each sample.This call tree is showing a lot of samples taken in functions called by binary searchto deal with the Collection type.This protocol witness is showing up in about a quarter of our samples.There are allocations and even Array's checks for Objective-C types.We can avoid the overheads of Arrayand the generics if we switch to a container typethat better matches the kind of data we're searching through.Let's try the new Span type.Span can be used instead of Collection when elements are stored contiguously in memory,which is common for many kinds of data structures.It's effectively a base address and count.But it also prevents escaping, or leaking, the memory referenceoutside of the functions it's used in.For more details on Span, watchthe Improve memory usage and performance with Swift session.

Adopting Span only requires changing the haystack and return types to Span,the algorithm itself is unchanged.

This small change makes searching four times faster.But this version of binary search is still impacting my appand I'd like to investigate whether Span's bounds checkingis contributing to the overhead.To dig in deeper, we'll switch to a new tool called Processor Trace.Starting in Instruments 16.3, Processor Trace can collect a complete traceof all the instructions your app's process executes in user space.This is a fundamental shift in how you can measure software performance:there's no sampling biasand only a negligible 1% impact to your app's performance.Processor Trace requires specialized CPU featuresthat are only available on Mac and iPad Pro with M4 or iPhone with A18.

Before we jump in, we'll need to set up our device for processor tracing.On a Mac, turn on the setting under Privacy & Security and Developer Tools.For iPhone or iPad, the setting is located in the Developer section.For the best experience with Processor Trace,try to limit your tracing to only a few seconds.Unlike sampling with CPU Profiler, you don't need to batch up your work:even a single instance of the code you want to optimize can be enough.Let's run Processor Trace on the Span version of binary search.Our test now only needs a handful of iterations.To profile this test, I'll secondary-click the test icon in the line number gutter.

This shows the same menu as the one we used before, but can be more convenientthan switching navigators.

I'll select the Processor Trace templateand start recording.

Processor Trace has to deal with a lot of data,so capturing and analyzing it can take some time.

Processor Trace sets up the CPU to record every branching decision.The cycle count and the current time are also recorded to track how long the CPUspends in each function.Instruments then uses your app's and system framework's executable binariesto reconstruct an execution path and annotate function callswith the cycles elapsed and time durations.We limit the time spent tracing because even though the CPU is recordingas little information as possible,it can still be gigabytes of data per secondfor a multi-threaded application.Now that the document is ready, let's zoom in so we can inspectour binary search function calls.

Searching now only takes up a tiny fraction of the full recording,so we'll find it in the Regions of Interest Listin the details view below the timelineand then secondary-click its row and select Set Inspection Range and Zoom.To locate the thread that's executing binary search,we'll secondary-click the Start Thread cell and select Pin Thread in Timeline.

Processor Trace adds a new function call flame graph to each thread track,so I'll drag the pin's divider up to make space for it.

Processor Trace shows execution visually as a flame graph.A flame graph is a graphical representation of function costs and relationships:the width of the bars represents how much time the function took to executeand the rows represent nested call stacks.But most flame graphs show data from sampling and their cost is onlyan estimate based on a sample count.

Processor Trace's timeline flame graph is different:it shows the calls made over timeexactly as they would have been executed on the CPU.The colors of each bar represent the kinds of binaries they're from:brown for system frameworks,magenta for the Swift runtime and standard library,and blue for code compiled into your app's binary or any custom frameworks.The first part of this trace is showing the overhead of emitting the signpost,so let's zoom in further on the binary search code near the end of the range.I'll hold down option, click, and drag across the timeline to zoom in.

I can select any binary search function call from the 10 iterationsand set the inspection range and zoom with a secondary click.This is the power of Processor Tracewe can see all the calls made by a single functionthat only runs for a few hundred nanoseconds.We could zoom in even further,but let's use the Function Calls summary below the timeline.This shows the same information as the timeline, but as a table,with the full names of functions being called for short periods of time.I'll sort this table by cycles.

My initial assumption that bounds checks were causing slowdowns was wrong.This implementation of binary search is still dealing with protocol metadata overheadsand can't inline the number comparisons, which ends up being a sizablepercentage of the search's total cycle count.

This is because the generic Comparable parameter is not specialized for the typeof elements being used.

Since my code is in a framework linked by my app,the Swift compiler cannot producea specialized version of binary search, just for the types passed by callers.

When this causes overheads in code from a framework,you should add the inlinable annotation to the framework's functionto generate specialized implementationswithin the framework client's binary executables.

But inlining can make code harder to analyzebecause it gets mixed with any callers.I'd like to avoid inlining into the test harness, so for this function,I'll just manually specialize it for the Int type used by the appand test, with a new function name.

The code loses a lot of generality while becoming about 1.7 times faster.We need to keep optimizingbecause binary search is still contributing to slowdowns in the app.It's pretty odd to spend so much time on optimizing a single functionyou'll periodically reassess and collect more data to notice a different partof your code responsible for the inefficiency.Our specialized Span binary search isn't showing any unexpected function callsin the Processor Trace either, so we should understand how the code is runningon the CPU to make further progress.

With the CPU Counters instrument, we can find out what bottlenecksthe code is hitting when running on CPU.

Before we start using Instruments again,we need to build a mental model of how a CPU works.

At a basic level, a CPU is just following a list of instructions,modifying registers and memory, and interacting with peripheral devices.

When a CPU is executing, it has to follow a series of steps,which are broadly categorized into two phases.The first is Instruction Delivery to ensure the CPU has instructions to execute.Then, Instruction Processing is responsible for executing them.

In Instruction Delivery, instructions are fetched and decoded into micro-operationsthat are easier for the CPU to execute.Most instructions decode into a single micro-operation, but some instructionsdo multiple things, like issuing a memory request and incrementing an index value.

To process a micro-operation, it's sent to the map and schedule units,which route and dispatch the operation.From there, the operation is assigned to an execution unit, or the load-store unitif the operation needs to access memory.

It would be pretty slow if the CPU had to run these phases seriallybefore it can start to fetch again, so Apple silicon processors are pipelined.

Once one unit is finished with work, it can move onto the next operation,keeping each unit busy.

Pipelining and making extra copies of the execution unitssupports instruction-level parallelism.

It's different from process or thread-level parallelismthat you would access with Swift Concurrency or Grand Central Dispatch,where multiple CPUs execute different operating system threads.Instruction-level parallelism allows a single CPU to take advantage of timeswhen units might otherwise be idle and use hardware resources efficiently,keeping all parts of the pipeline busy.Your Swift source code is not directly in control of this parallelism,but instead must help the compiler generate an amenable sequence of instructions.

Unfortunately parallelizable instruction sequences are not immediately intuitivedue to the interaction between units in a CPU.Every arrow between the units shows where operations can stall in the pipeline,limiting the available parallelism.We call these bottlenecks.

To find out which bottleneck is relevant to our workload,Apple silicon CPUs can count interesting events in each unitand other characteristics of the instructions being executed.The CPU Counters instrument reads these counters to buildhigher-level metrics out of them.This year, we've added preset modes for these counters to make them much easier to use.Instruments uses them in a guided, iterative methodology for analyzingthe performance of your code, called bottleneck analysis.Let's use it to find out why our binary search is still slow, despite not havingany apparent function call overheads.

The CPU Counters instrument relies on sampling the workload, so we need toreturn to the test harness we used with CPU Profiler to measure throughput again.

Let's profile a test of the specialized Span implementation with Instruments.

We'll select the CPU Counters template.

There's now a guided configuration with curated modes to measure with.

If you're curious about what each mode does, there's documentation availableby clicking on the info icon next to the mode selection.Let's get counting.

This initial CPU Bottlenecks mode breaks up the work done by the CPUinto four broad categories that account for all of the CPU's potential performance.Instruments shows these categories as a colored stacked bar chartand a summary table in the detail view.During recording, Instruments will collect CPU counter data for the threadsused in our test and convert them into bottleneck percentages.We'll use the Points of Interest to orient ourselves, like before,to zoom and select our searches.

We'll then pin the thread that is running our binary search implementation to the timeline.

Hovering over its CPU Bottleneck lane shows a high percentagein the discarded bottleneck.

The details view below shows the metric aggregations in the inspection range.Selecting the Discarded Bottleneck row shows a description in the extended detailview on the right.

Instruments also presents a remark above the chart in the timeline.Clicking that remark shows more details below.

This is useful, but I still don't knowwhat part of search is responsible for the bottleneck.Secondary-clicking the Discarded Sampling cellunder the Suggested Next Mode columnoffers the option to profile the workload againwith a different mode.Let's try it out.This mode is a little different from CPU Bottlenecks.It's still gathering counter data, but it's also setting up the countersto trigger sampling.The sample data is just limited to the instructionthat's generating discarded work.To show those, let's orient ourselves again with Points of Interest.

Then, we'll select the test process trackand navigate to Instruction Samples below the timeline.

This isn't a call stack, but the exact instruction that's causing the problem.We can open the Source Viewer by clicking on the arrow next to the function nameto show the source code that was sampledbecause the CPU followed the wrong branch direction.Here, the comparisons being donebetween the needle and the middle value are incorrectly predicted.To understand why these source lines are responsible for so many bad predictions,we need to learn a bit more about CPUs.

CPUs are sneaky and execute instructions out-of-order.It only appears like instructions execute sequentiallythanks to an extra re-ordering step as instructions complete.

This means CPUs look ahead and make predictionsabout which instructions will execute next.The branch predictors responsible are usually accurate, but can take wrong pathswhen there's no consistent pattern from prior executionabout whether a branch will be taken or not.

The loop in our binary search algorithm has two kinds of branches.The first loop condition is usually taken until the end of the loop,so is well-predicted and didn't show up in sampling.But the check for the needle is effectively a random branch,so it's no wonder the predictors have trouble with it.

I've rewritten the loop body to avoid difficult to predictbranches that affect control flow.The if statement's body is only assigning a value based on the condition.This allows the Swift compiler to generate a conditional move instruction,and avoid branching to a different instruction.

Returning from a function or breaking a loop based on a conditionmust be implemented with a branch, so I also had to remove the early return.

I've used unchecked arithmetic to avoid branches that would terminate the program.This is one of the areas where micro-optimization becomes fragileand easy to disrupt, not to mention less safe and understandable.When we make a change like this, you should go back to the initialCPU Bottlenecks mode to check how it impacts the rest of the bottlenecks.

I've already collected a trace of our new, branchless binary search,which is now about twice as fast as the version with branches.It's now almost completely bottlenecked on Instruction Processing.

Instruments indicates that we should re-run the workloadwith the Instruction Processing mode.

That mode had remarks that recommended running the L1D Cache Miss Sampling mode.The cache miss samples show that accessing memory from the array is the reasonthe CPU isn't able to execute instructions efficiently.

Let's learn more about CPUs and memory to find out why.

CPUs access memory through a hierarchy of caches that make repeated accessesto the same address, or even predictable access patterns, much faster.

It starts with the L1 caches that are located within each CPU.These can't store much data, but offer the fastest access to memory.A slower L2 cache sits outside of the CPUs and provides a lot more headroom.And finally, requests that miss both caches and need to access main memorybecome 50 times slower than the fast path.

These caches also group memory into 64- or 128 byte segments called cache lines:even if an instruction only requests 4 bytes, the caches will pull in more datawith the expectation that subsequent instructionswill need to access other bytes nearby.

Let's consider how this impacts our binary search algorithm.In this example, the blue lines are elements in the array,while the grey capsules are the cache lines that the CPU caches operate on.

The array starts completely out of the cache.The first comparison brings in a cache line and several elements into the L1 data cache.But the next comparison experiences a cache miss.And subsequent iterations keep missing in the cache.This continues until the search narrows in on a cache-line sized region.Binary search turns out to be a pathological casefor the CPU's memory hierarchy.

But if we can tolerate reordering the elementsto be more friendly to the cache,then we can put the search points on the same cache line.This is called an Eytzinger layout, after a 16th century Austrian genealogistwho organized family trees this way.

This isn't a general optimization we can make without significant consequences.This improves search speed at the cost of in-order traversal speed,making that operation now miss in the cache.Let's go back to the first example of binary search to show how to re-arrangea sorted array to an Eytzinger layout.Starting with the middle element as the root,we'll model the binary search operation as a tree,where midpoints are descendant nodes.An Eytzinger layout is arranged as the breadth-first traversal of that tree.

The elements closer to the root of the tree are arranged more densely,and are more likely to share cache lines.Searching for 5 again now spends the first three steps in the same cache line.The leaf nodes are sorted at the end of the array,which will incur an unavoidable cache miss.

I've recorded a CPU Bottlenecks trace of the Eytzinger binary search,which shows it's two times faster again than the branchless search.

But this example highlights something interesting,it's still technically bottlenecked on Instruction Processing.

We've made our implementation more cache-friendly, but the workload is stillinherently memory-bound.

You should monitor performance to know when to stop and optimize other codein your app,because our search is now no longer impactingcritical path performance.

During this process, we've significantly improved search throughput.First, with CPU Profiler, we were able to get a significant speedup when switchingfrom Collection to Span.

Then, Processor Trace showed us the overheads of unspecialized generics.

And finally, we dialed up the performance significantlywith some micro-optimizations, guided by Bottleneck Analysis.

Overall, we made our search function about 25 times faster with Instruments.To achieve these speedups, we started in the correct mindset,using tools to confirm our guesses and develop an intuitionabout the cost of abstractions.We successively applied more detailed tools to find unexpected overheads.These were as easy to address as they are to overlook if you don't actually measure.And then, once software overheads were addressed,we looked at CPU bottleneck-focused optimizations.We became more aware of and maybe even sympathetic to features of the CPUthat are taken for granted.

This order was important: we have to ensure the CPU-focused toolsaren't confused by extra software runtime overheads.

To apply this to your own apps,collect data and follow leads with the performance mindset.Write performance tests so you can repeatedly measure with these Instruments.Provide feedback or ask questions about using the tools on the forums.Watch the sessions I mentioned earlier and the WWDC24 session on Swift performance,which will help you build a more accurate mental model of the costsof Swift's powerful abstractions.And to better understand how CPUs execute your code,read the Apple Silicon CPU Optimization Guide.

Thanks for watchingand I hope you have fun using Instruments to find optimization needlesin your code's haystacks.

6:37 -Binary search in Collection

7:49 -Throughput benchmark

13:46 -Binary search in Span

15:09 -Throughput benchmark for binary search in Span

19:17 -Binary search in Span

23:04 -Throughput benchmark for binary search in Span

26:34 -Branchless binary search

27:20 -Throughput benchmark for branchless binary search

29:27 -Eytzinger binary search

30:34 -Throughput benchmark for Eytzinger binary search

0:00 -Introduction & AgendaOptimizing code for Apple silicon CPUs is complex due to the layers of abstraction between Swift source code and machine instructions, as well as the complex ways CPUs execute instructions out-of-order and utilize memory caches. 

Instruments helps developers navigate these complexities and enables performance investigations, profiles system performance to identify excessive CPU usage. Use Processor Trace and CPU Counters instruments to record instructions, measure costs, and analyze bottlenecks, ultimately leading to more efficient code and improved app performance.2:28 -Performance mindsetWhen you investigate performance issues in your apps, it's crucial to maintain an open mind and collect data to validate assumptions. Slowdowns can result from various factors, such as blocked threads waiting on resources, misused APIs, or inefficient algorithms.

Tools like CPU Gauge in Xcode as well as System Trace and Hangs instruments in Instruments are invaluable for identifying CPU usage patterns, blocking behaviors, and UI unresponsiveness. Before diving into micro-optimizations, which can make code harder to maintain, it's best to explore alternative approaches.

These alternatives include avoiding unnecessary work, delaying that work with concurrency, precomputing values, and caching state computed by complex operations. If these strategies are exhausted, optimizing the CPU-bound code becomes necessary.

Focus on code that significantly impacts user experience, such as the critical path of user interactions. Incremental optimization is recommended, with progress measured using automated tests and performance metrics in both Xcode and Instruments.8:50 -ProfilersTo analyze the CPU performance of the binary search example in this session, two profilers in Instruments are available: Time Profiler and CPU Profiler. 

Time Profiler periodically samples CPU activity but can suffer from aliasing, where periodic work distorts the representation of CPU usage. CPU Profiler, on the other hand, samples CPUs independently based on their clock frequency, making it more accurate and suitable for CPU optimization.

For this analysis, the CPU Profiler instrument is chosen and launched from Xcode's test navigator, then the recording in Instruments is set to Deferred Mode to minimize overhead. The areas within Instruments is introduced, including the timeline view, its tracks and lanes, and the detail view which shows the profiled results.

By examining the Points of Interest track and the Process track for the 'xctest' process, the specific region where binary searches occur in the example app is identified. The call tree in the detail view shows that functions related to the 'Collection' protocol consume significant CPU time. To optimize performance, switching to a more efficient container type, such as 'Span', is suggested to avoid overheads associated with 'Array'—which has copy-on-write semantics—and generics.13:20 -SpanSwift 6.2 introduces 'Span', a memory-efficient data structure that represents a contiguous memory range with a base address and count. Using 'Span' for binary search input and output types improves performance by 400% without altering the algorithm. Next, to further optimize performance, the Processor Trace instrument is used to investigate bounds checking overhead.14:05 -Processor TraceInstruments 16.3 introduced a significant new instrument called Processor Trace. This tool allows you to capture a comprehensive trace of all the instructions executed by your app's process in user space on Mac and iPad Pro with M4 chips and later, or iPhone with A18 chips and later. 

Processor Trace requires specific device settings to be enabled and is most effective when used for short tracing sessions, as it can generate substantial amounts of data. By recording every branching decision, cycle count, and current time, Instruments reconstructs the exact execution path of the app.

The data is presented visually in a flame graph, which shows the time taken by each function call over time. Unlike traditional flame graphs that use sampling, Processor Trace's flame graph provides an exact representation of how the CPU executed the code. This allows you to identify performance bottlenecks with unprecedented precision.

Through analysis of the trace data, it's clear that protocol metadata overheads and the inability to inline number comparisons are causing significant slowdowns in a specific binary search function. To address this, the function is manually specialized for the Int type, resulting in a substantial performance improvement of about 170%. However, further optimization is still needed because the app's binary search implementation continues to contribute to overall app slowdowns.19:51 -Bottleneck analysisApple Silicon CPUs execute instructions in two phases: Instruction Delivery and Instruction Processing, which are pipelined to enable instruction-level parallelism. This allows multiple operations to be processed simultaneously, maximizing efficiency. However, bottlenecks can occur in the pipeline, stalling operations and limiting parallelism.

The CPU Counters instrument helps identify these bottlenecks by counting events in each CPU unit. It uses preset modes to measure CPU performance and break down work into broad categories. When you analyze the sampled data, they can pinpoint specific instructions causing issues, such as mispredicted branch directions, which can lead to wasted cycles and performance degradation.

CPUs execute instructions out-of-order using branch predictors to enhance performance. However, random branches can mislead these predictors. To mitigate this, the code is rewritten to avoid difficult-to-predict branches, resulting in a branchless binary search that is about twice as fast.

The focus of the app's optimization then shifts to memory access, as CPUs utilize a hierarchy of caches to speed up data retrieval. The binary search algorithm's access pattern was pathological for this hierarchy, leading to frequent cache misses. By rearranging the array elements using an Eytzinger layout, cache locality is improved, and the binary search became another 200% faster.

Despite these significant optimizations, the code is still technically bottlenecked on instruction processing, but the overall search function has been made about 2500% faster through various profiling and micro-optimization techniques.31:33 -RecapBy measuring and optimizing software overheads first, then focusing on CPU bottlenecks, the binary search app's performance was improved by addressing easily overlooked issues and becoming more attuned to CPU architecture.32:13 -Next stepsTo optimize apps, use Instruments to collect data, run performance tests, and analyze results. You can also watch sessions on Swift performance and read the "Apple Silicon CPU Optimization Guide" in developer documentation. Ask questions or provide feedback on the Developer Forums.

0:00 -Introduction & Agenda

Optimizing code for Apple silicon CPUs is complex due to the layers of abstraction between Swift source code and machine instructions, as well as the complex ways CPUs execute instructions out-of-order and utilize memory caches. 

Instruments helps developers navigate these complexities and enables performance investigations, profiles system performance to identify excessive CPU usage. Use Processor Trace and CPU Counters instruments to record instructions, measure costs, and analyze bottlenecks, ultimately leading to more efficient code and improved app performance.

Optimizing code for Apple silicon CPUs is complex due to the layers of abstraction between Swift source code and machine instructions, as well as the complex ways CPUs execute instructions out-of-order and utilize memory caches. 

Instruments helps developers navigate these complexities and enables performance investigations, profiles system performance to identify excessive CPU usage. Use Processor Trace and CPU Counters instruments to record instructions, measure costs, and analyze bottlenecks, ultimately leading to more efficient code and improved app performance.

2:28 -Performance mindset

When you investigate performance issues in your apps, it's crucial to maintain an open mind and collect data to validate assumptions. Slowdowns can result from various factors, such as blocked threads waiting on resources, misused APIs, or inefficient algorithms.

Tools like CPU Gauge in Xcode as well as System Trace and Hangs instruments in Instruments are invaluable for identifying CPU usage patterns, blocking behaviors, and UI unresponsiveness. Before diving into micro-optimizations, which can make code harder to maintain, it's best to explore alternative approaches.

These alternatives include avoiding unnecessary work, delaying that work with concurrency, precomputing values, and caching state computed by complex operations. If these strategies are exhausted, optimizing the CPU-bound code becomes necessary.

Focus on code that significantly impacts user experience, such as the critical path of user interactions. Incremental optimization is recommended, with progress measured using automated tests and performance metrics in both Xcode and Instruments.

When you investigate performance issues in your apps, it's crucial to maintain an open mind and collect data to validate assumptions. Slowdowns can result from various factors, such as blocked threads waiting on resources, misused APIs, or inefficient algorithms.

Tools like CPU Gauge in Xcode as well as System Trace and Hangs instruments in Instruments are invaluable for identifying CPU usage patterns, blocking behaviors, and UI unresponsiveness. Before diving into micro-optimizations, which can make code harder to maintain, it's best to explore alternative approaches.

These alternatives include avoiding unnecessary work, delaying that work with concurrency, precomputing values, and caching state computed by complex operations. If these strategies are exhausted, optimizing the CPU-bound code becomes necessary.

Focus on code that significantly impacts user experience, such as the critical path of user interactions. Incremental optimization is recommended, with progress measured using automated tests and performance metrics in both Xcode and Instruments.

8:50 -Profilers

To analyze the CPU performance of the binary search example in this session, two profilers in Instruments are available: Time Profiler and CPU Profiler. 

Time Profiler periodically samples CPU activity but can suffer from aliasing, where periodic work distorts the representation of CPU usage. CPU Profiler, on the other hand, samples CPUs independently based on their clock frequency, making it more accurate and suitable for CPU optimization.

For this analysis, the CPU Profiler instrument is chosen and launched from Xcode's test navigator, then the recording in Instruments is set to Deferred Mode to minimize overhead. The areas within Instruments is introduced, including the timeline view, its tracks and lanes, and the detail view which shows the profiled results.

By examining the Points of Interest track and the Process track for the 'xctest' process, the specific region where binary searches occur in the example app is identified. The call tree in the detail view shows that functions related to the 'Collection' protocol consume significant CPU time. To optimize performance, switching to a more efficient container type, such as 'Span', is suggested to avoid overheads associated with 'Array'—which has copy-on-write semantics—and generics.

To analyze the CPU performance of the binary search example in this session, two profilers in Instruments are available: Time Profiler and CPU Profiler. 

Time Profiler periodically samples CPU activity but can suffer from aliasing, where periodic work distorts the representation of CPU usage. CPU Profiler, on the other hand, samples CPUs independently based on their clock frequency, making it more accurate and suitable for CPU optimization.

For this analysis, the CPU Profiler instrument is chosen and launched from Xcode's test navigator, then the recording in Instruments is set to Deferred Mode to minimize overhead. The areas within Instruments is introduced, including the timeline view, its tracks and lanes, and the detail view which shows the profiled results.

By examining the Points of Interest track and the Process track for the 'xctest' process, the specific region where binary searches occur in the example app is identified. The call tree in the detail view shows that functions related to the 'Collection' protocol consume significant CPU time. To optimize performance, switching to a more efficient container type, such as 'Span', is suggested to avoid overheads associated with 'Array'—which has copy-on-write semantics—and generics.

13:20 -Span

Swift 6.2 introduces 'Span', a memory-efficient data structure that represents a contiguous memory range with a base address and count. Using 'Span' for binary search input and output types improves performance by 400% without altering the algorithm. Next, to further optimize performance, the Processor Trace instrument is used to investigate bounds checking overhead.

Swift 6.2 introduces 'Span', a memory-efficient data structure that represents a contiguous memory range with a base address and count. Using 'Span' for binary search input and output types improves performance by 400% without altering the algorithm. Next, to further optimize performance, the Processor Trace instrument is used to investigate bounds checking overhead.

14:05 -Processor Trace

Instruments 16.3 introduced a significant new instrument called Processor Trace. This tool allows you to capture a comprehensive trace of all the instructions executed by your app's process in user space on Mac and iPad Pro with M4 chips and later, or iPhone with A18 chips and later. 

Processor Trace requires specific device settings to be enabled and is most effective when used for short tracing sessions, as it can generate substantial amounts of data. By recording every branching decision, cycle count, and current time, Instruments reconstructs the exact execution path of the app.

The data is presented visually in a flame graph, which shows the time taken by each function call over time. Unlike traditional flame graphs that use sampling, Processor Trace's flame graph provides an exact representation of how the CPU executed the code. This allows you to identify performance bottlenecks with unprecedented precision.

Through analysis of the trace data, it's clear that protocol metadata overheads and the inability to inline number comparisons are causing significant slowdowns in a specific binary search function. To address this, the function is manually specialized for the Int type, resulting in a substantial performance improvement of about 170%. However, further optimization is still needed because the app's binary search implementation continues to contribute to overall app slowdowns.

Instruments 16.3 introduced a significant new instrument called Processor Trace. This tool allows you to capture a comprehensive trace of all the instructions executed by your app's process in user space on Mac and iPad Pro with M4 chips and later, or iPhone with A18 chips and later. 

Processor Trace requires specific device settings to be enabled and is most effective when used for short tracing sessions, as it can generate substantial amounts of data. By recording every branching decision, cycle count, and current time, Instruments reconstructs the exact execution path of the app.

The data is presented visually in a flame graph, which shows the time taken by each function call over time. Unlike traditional flame graphs that use sampling, Processor Trace's flame graph provides an exact representation of how the CPU executed the code. This allows you to identify performance bottlenecks with unprecedented precision.

Through analysis of the trace data, it's clear that protocol metadata overheads and the inability to inline number comparisons are causing significant slowdowns in a specific binary search function. To address this, the function is manually specialized for the Int type, resulting in a substantial performance improvement of about 170%. However, further optimization is still needed because the app's binary search implementation continues to contribute to overall app slowdowns.

19:51 -Bottleneck analysis

Apple Silicon CPUs execute instructions in two phases: Instruction Delivery and Instruction Processing, which are pipelined to enable instruction-level parallelism. This allows multiple operations to be processed simultaneously, maximizing efficiency. However, bottlenecks can occur in the pipeline, stalling operations and limiting parallelism.

The CPU Counters instrument helps identify these bottlenecks by counting events in each CPU unit. It uses preset modes to measure CPU performance and break down work into broad categories. When you analyze the sampled data, they can pinpoint specific instructions causing issues, such as mispredicted branch directions, which can lead to wasted cycles and performance degradation.

CPUs execute instructions out-of-order using branch predictors to enhance performance. However, random branches can mislead these predictors. To mitigate this, the code is rewritten to avoid difficult-to-predict branches, resulting in a branchless binary search that is about twice as fast.

The focus of the app's optimization then shifts to memory access, as CPUs utilize a hierarchy of caches to speed up data retrieval. The binary search algorithm's access pattern was pathological for this hierarchy, leading to frequent cache misses. By rearranging the array elements using an Eytzinger layout, cache locality is improved, and the binary search became another 200% faster.

Despite these significant optimizations, the code is still technically bottlenecked on instruction processing, but the overall search function has been made about 2500% faster through various profiling and micro-optimization techniques.

Apple Silicon CPUs execute instructions in two phases: Instruction Delivery and Instruction Processing, which are pipelined to enable instruction-level parallelism. This allows multiple operations to be processed simultaneously, maximizing efficiency. However, bottlenecks can occur in the pipeline, stalling operations and limiting parallelism.

The CPU Counters instrument helps identify these bottlenecks by counting events in each CPU unit. It uses preset modes to measure CPU performance and break down work into broad categories. When you analyze the sampled data, they can pinpoint specific instructions causing issues, such as mispredicted branch directions, which can lead to wasted cycles and performance degradation.

CPUs execute instructions out-of-order using branch predictors to enhance performance. However, random branches can mislead these predictors. To mitigate this, the code is rewritten to avoid difficult-to-predict branches, resulting in a branchless binary search that is about twice as fast.

The focus of the app's optimization then shifts to memory access, as CPUs utilize a hierarchy of caches to speed up data retrieval. The binary search algorithm's access pattern was pathological for this hierarchy, leading to frequent cache misses. By rearranging the array elements using an Eytzinger layout, cache locality is improved, and the binary search became another 200% faster.

Despite these significant optimizations, the code is still technically bottlenecked on instruction processing, but the overall search function has been made about 2500% faster through various profiling and micro-optimization techniques.

31:33 -Recap

By measuring and optimizing software overheads first, then focusing on CPU bottlenecks, the binary search app's performance was improved by addressing easily overlooked issues and becoming more attuned to CPU architecture.

By measuring and optimizing software overheads first, then focusing on CPU bottlenecks, the binary search app's performance was improved by addressing easily overlooked issues and becoming more attuned to CPU architecture.

32:13 -Next steps

To optimize apps, use Instruments to collect data, run performance tests, and analyze results. You can also watch sessions on Swift performance and read the "Apple Silicon CPU Optimization Guide" in developer documentation. Ask questions or provide feedback on the Developer Forums.

To optimize apps, use Instruments to collect data, run performance tests, and analyze results. You can also watch sessions on Swift performance and read the "Apple Silicon CPU Optimization Guide" in developer documentation. Ask questions or provide feedback on the Developer Forums.

## Code Samples

```swift
public
 
func
 
binarySearch
<
E
, 
C
>(
    
needle
: 
E
,
    
haystack
: 
C

) -> 
C
.
Index
 
where
 
E
: 
Comparable
, 
C
: 
Collection
<
E
> {
    
var
 start 
=
 haystack.startIndex
    
var
 length 
=
 haystack.count

    
while
 length 
>
 
0
 {
        
let
 half 
=
 length 
/
 
2

        
let
 middle 
=
 haystack.index(start, offsetBy: half)
        
let
 middleValue 
=
 haystack[middle]
        
if
 needle 
<
 middleValue {
            length 
=
 half
        } 
else
 
if
 needle 
==
 middleValue {
            
return
 middle
        } 
else
 {
            start 
=
 haystack.index(after: middle)
            length 
-=
 half 
+
 
1

        }
    }

    
return
 start
}
```

```swift
import
 Testing

import
 OSLog


let
 signposter 
=
 
OSSignposter
(
    subsystem: 
"com.example.apple-samplecode.MyBinarySearch"
,
    category: .pointsOfInterest
)


func
 
search
(
    
name
: 
StaticString
,
    
duration
: 
Duration
,
    
_
 
search
: () -> 
Void

) {
    
var
 now 
=
 
ContinuousClock
.now
    
var
 outerIterations 
=
 
0

    
    
let
 interval 
=
 signposter.beginInterval(name)
    
let
 start 
=
 
ContinuousClock
.now
    
repeat
 {
        search()
        outerIterations 
+=
 
1

        now 
=
 .now
    } 
while
 (start.duration(to: now) 
<
 duration)
    
let
 elapsed 
=
 start.duration(to: now)
    
let
 seconds 
=
 
Double
(elapsed.components.seconds) 
+

            
Double
(elapsed.components.attoseconds) 
/
 
1e18

    
let
 throughput 
=
 
Double
(outerIterations) 
/
 seconds
    signposter.endInterval(name, interval, 
"
\(throughput)
 ops/s"
)
    
print
(
"
\(name)
: 
\(throughput)
 ops/s"
)
}


let
 arraySize 
=
 
8
 
<<
 
20


let
 arrayCount 
=
 arraySize 
/
 
MemoryLayout
<
Int
>.size

let
 searchCount 
=
 
10_000



struct
 
MyBinarySearchTests
 {
    
let
 sortedArray: [
Int
]
    
let
 randomElements: [
Int
]
    
    
init
() {
        
let
 sortedArray: [
Int
] 
=
 (
0
..<
arrayCount).map { 
_
 
in

                .random(in: 
0
..<
arrayCount)
        }.sorted()
        
self
.randomElements 
=
 (
0
..<
searchCount).map { 
_
 
in

            sortedArray.randomElement()
!

        }
        
self
.sortedArray 
=
 sortedArray
    }

    
@Test
 
func
 
searchCollection
() 
throws
 {
        search(name: 
"Collection"
, duration: .seconds(
1
)) {
            
for
 element 
in
 randomElements {
                
_
 
=
 binarySearch(needle: element, haystack: sortedArray)
            }
        }
    }
}
```

```swift
public
 
func
 
binarySearch
<
E
: 
Comparable
>(
    
needle
: 
E
,
    
haystack
: 
Span
<
E
>
) -> 
Span
<
E
>.
Index
 {
    
var
 start 
=
 haystack.indices.startIndex
    
var
 length 
=
 haystack.count

    
while
 length 
>
 
0
 {
        
let
 half 
=
 length 
/
 
2

        
let
 middle 
=
 haystack.indices.index(start, offsetBy: half)
        
let
 middleValue 
=
 haystack[middle]
        
if
 needle 
<
 middleValue {
            length 
=
 half
        } 
else
 
if
 needle 
==
 middleValue {
            
return
 middle
        } 
else
 {
            start 
=
 haystack.indices.index(after: middle)
            length 
-=
 half 
+
 
1

        }
    }

    
return
 start
}
```

```swift
extension
 
MyBinarySearchTests
 {
    
@Test
 
func
 
searchSpan
() 
throws
 {
        
let
 span 
=
 sortedArray.span
        search(name: 
"Span"
, duration: .seconds(
1
)) {
            
for
 element 
in
 randomElements {
                
_
 
=
 binarySearch(needle: element, haystack: span)
            }
        }
    }

    
@Test
 
func
 
searchSpanForProcessorTrace
() 
throws
 {
        
let
 span 
=
 sortedArray.span
        signposter.withIntervalSignpost(
"Span"
) {
            
for
 element 
in
 randomElements[
0
..<
10
] {
                
_
 
=
 binarySearch(needle: element, haystack: span)
            }
        }
    }
}
```

```swift
public
 
func
 
binarySearchInt
(
    
needle
: 
Int
,
    
haystack
: 
Span
<
Int
>
) -> 
Span
<
Int
>.
Index
 {
    
var
 start 
=
 haystack.indices.startIndex
    
var
 length 
=
 haystack.count

    
while
 length 
>
 
0
 {
        
let
 half 
=
 length 
/
 
2

        
let
 middle 
=
 haystack.indices.index(start, offsetBy: half)
        
let
 middleValue 
=
 haystack[middle]
        
if
 needle 
<
 middleValue {
            length 
=
 half
        } 
else
 
if
 needle 
==
 middleValue {
            
return
 middle
        } 
else
 {
            start 
=
 haystack.indices.index(after: middle)
            length 
-=
 half 
+
 
1

        }
    }
    
return
 start
}
```

```swift
extension
 
MyBinarySearchTests
 {
    
@Test
 
func
 
searchSpanInt
() 
throws
 {
        
let
 span 
=
 sortedArray.span
        search(name: 
"Span<Int>"
, duration: .seconds(
1
)) {
            
for
 element 
in
 randomElements {
                
_
 
=
 binarySearchInt(needle: element, haystack: span)
            }
        }
    }
}
```

```swift
public
 
func
 
binarySearchBranchless
(
    
needle
: 
Int
,
    
haystack
: 
Span
<
Int
>
) -> 
Span
<
Int
>.
Index
 {
    
var
 start 
=
 haystack.indices.startIndex
    
var
 length 
=
 haystack.count

    
while
 length 
>
 
0
 {
        
let
 remainder 
=
 length 
%
 
2

        length 
/=
 
2

        
let
 middle 
=
 start 
&+
 length
        
let
 middleValue 
=
 haystack[middle]
        
if
 needle 
>
 middleValue {
            start 
=
 middle 
&+
 remainder
        }
    }

    
return
 start
}
```

```swift
extension
 
MyBinarySearchTests
 {
    
@Test
 
func
 
searchBranchless
() 
throws
 {
        
let
 span 
=
 sortedArray.span
        search(name: 
"Branchless"
, duration: .seconds(
1
)) {
            
for
 element 
in
 randomElements {
                
_
 
=
 binarySearchBranchless(needle: element, haystack: span)
            }
        }
    }
}
```

```swift
public
 
func
 
binarySearchEytzinger
(
    
needle
: 
Int
,
    
haystack
: 
Span
<
Int
>
) -> 
Span
<
Int
>.
Index
 {
    
var
 start 
=
 haystack.indices.startIndex.advanced(by: 
1
)
    
let
 length 
=
 haystack.count

    
while
 start 
<
 length {
        
let
 value 
=
 haystack[start]
        start 
*=
 
2

        
if
 value 
<
 needle {
            start 
+=
 
1

        }
    }
    
    
return
 start 
>>
 ((
~
start).trailingZeroBitCount 
+
 
1
)
}
```

```swift
struct
 
MyBinarySearchEytzingerTests
 {
    
let
 eytzingerArray: [
Int
]
    
let
 randomElements: [
Int
]

    
static
 
func
 
reorderEytzinger
(
_
 
input
: [
Int
], 
array
: 
inout
 [
Int
], 
sourceIndex
: 
Int
, 
resultIndex
: 
Int
) -> 
Int
 {
        
var
 sourceIndex 
=
 sourceIndex
        
if
 resultIndex 
<
 array.count {
            sourceIndex 
=
 reorderEytzinger(input, array: 
&
array, sourceIndex: sourceIndex, resultIndex: 
2
 
*
 resultIndex)
            array[resultIndex] 
=
 input[sourceIndex]
            sourceIndex 
=
 reorderEytzinger(input, array: 
&
array, sourceIndex: sourceIndex 
+
 
1
, resultIndex: 
2
 
*
 resultIndex 
+
 
1
)
        }
        
return
 sourceIndex
    }

    
init
() {
        
let
 sortedArray: [
Int
] 
=
 (
0
..<
arrayCount).map { 
_
 
in

            .random(in: 
0
..<
arrayCount)
        }.sorted()
        
var
 eytzingerArray: [
Int
] 
=
 
Array
(repeating: 
0
, count: arrayCount 
+
 
1
)
        
_
 
=
 
Self
.reorderEytzinger(sortedArray, array: 
&
eytzingerArray, sourceIndex: 
0
, resultIndex: 
1
)
        
self
.randomElements 
=
 (
0
..<
searchCount).map { 
_
 
in

            sortedArray.randomElement()
!

        }
        
self
.eytzingerArray 
=
 eytzingerArray
    }

    
@Test
 
func
 
searchEytzinger
() 
throws
 {
        
let
 span 
=
 eytzingerArray.span
        search(name: 
"Eytzinger"
, duration: .seconds(
1
)) {
            
for
 element 
in
 randomElements {
                
_
 
=
 binarySearchEytzinger(needle: element, haystack: span)
            }
        }
    }
}
```

