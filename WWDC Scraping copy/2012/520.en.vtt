WEBVTT

00:00:11.440 --> 00:00:12.400
Good morning.

00:00:12.400 --> 00:00:14.600
Good morning and welcome to session 520.

00:00:14.630 --> 00:00:18.230
If you're interested in cameras
or capture or new stuff,

00:00:18.230 --> 00:00:20.680
you've come to the right place.

00:00:20.900 --> 00:00:23.200
And looks like we've got
a really good crowd here.

00:00:23.200 --> 00:00:25.550
Thank you all for coming and
thank you for all of your apps

00:00:25.550 --> 00:00:26.960
and all the work that you do.

00:00:26.960 --> 00:00:30.160
You're making our platforms a very
vibrant place to be right now.

00:00:32.080 --> 00:00:34.540
We're going to cover a lot today,
so if I sound like an auctioneer,

00:00:34.540 --> 00:00:36.500
it's not because I'm nervous,
it's just because we've

00:00:36.500 --> 00:00:37.800
got a lot to go through.

00:00:37.890 --> 00:00:41.550
What you're going to learn today
is five main topics to cover.

00:00:41.780 --> 00:00:45.600
Performance improvements
in Mac OS X 10.8.

00:00:45.680 --> 00:00:48.830
A bit about the camera
ecosystem as a whole.

00:00:48.940 --> 00:00:52.720
New AV Foundation capture
features in iOS 6.

00:00:52.850 --> 00:00:54.500
That's the bulk of it.

00:00:54.590 --> 00:00:56.780
And then we have a section
on solutions for performance

00:00:56.930 --> 00:00:58.860
problems in your capture apps.

00:00:58.950 --> 00:01:02.060
And finally, a kind of a neat demo at the
end where we'll talk about

00:01:02.200 --> 00:01:05.520
synchronizing motion data with video.

00:01:06.410 --> 00:01:11.320
What you will not learn, however,
is AV Foundation and Core Media Basics.

00:01:11.480 --> 00:01:14.700
I've given this talk two or
three years in a row now.

00:01:14.740 --> 00:01:17.950
I think you've seen the
block diagrams enough,

00:01:17.950 --> 00:01:21.980
you've seen the class hierarchies,
that we're not going to do that again.

00:01:21.980 --> 00:01:25.730
But you do have all of the videos
available to you on developer.apple.com,

00:01:25.820 --> 00:01:28.270
and we encourage you to go
back there and view those as

00:01:28.280 --> 00:01:29.800
prerequisites to this session.

00:01:31.430 --> 00:01:35.720
We have five pieces of sample
code for today's session.

00:01:35.900 --> 00:01:38.860
I just looked before the session,
and most of them are up there.

00:01:38.920 --> 00:01:43.400
I anticipate that the rest will be up
later today or tomorrow at the latest.

00:01:43.490 --> 00:01:44.400
And there are the URLs.

00:01:47.400 --> 00:03:29.300
[Transcript missing]

00:03:29.770 --> 00:03:32.980
Also, there is an optimization in
there for writing movies,

00:03:32.980 --> 00:03:35.190
which is we detect duplicate frames.

00:03:35.200 --> 00:03:39.650
If there's been no movement in the frame,
we won't give you those duplicate frames.

00:03:39.740 --> 00:03:44.100
But for screen-grabbing apps
that are doing effects or such,

00:03:44.150 --> 00:03:45.250
you want all of the frames.

00:03:45.260 --> 00:03:46.330
You want a constant frame rate.

00:03:46.460 --> 00:03:49.160
So you can opt out of
that behavior in 10.8,

00:03:49.160 --> 00:03:51.300
the duplicate frame removal.

00:03:51.420 --> 00:03:55.500
See the updated AVScreenShack
sample code for examples on how

00:03:55.500 --> 00:03:57.660
to use all of these new APIs.

00:03:57.800 --> 00:03:59.390
Next up,

00:03:59.690 --> 00:04:03.900
is support for hardware-accelerated
H.264 encoding.

00:04:04.070 --> 00:04:08.330
2011 and newer Macs that have
the Sandy Bridge or the latest

00:04:08.460 --> 00:04:12.830
MacBook Pro that has the
Ivy Bridge chipset all have special

00:04:12.830 --> 00:04:16.110
hardware for doing H.264 encode.

00:04:16.450 --> 00:04:19.300
Up to 1920 by 1080.

00:04:19.480 --> 00:04:22.460
So if you have something
larger than that,

00:04:22.460 --> 00:04:25.760
it will fall back to software,
and that happens transparently.

00:04:26.020 --> 00:04:28.730
It just--if you can get the hardware,
you'll get it.

00:04:28.800 --> 00:04:31.140
If not, you'll get the software.

00:04:31.270 --> 00:04:36.900
This is both movie file output and asset
writer in real-time mode are eligible

00:04:37.450 --> 00:04:42.840
for getting real-time H.264 encoding,
and no code change is required.

00:04:42.980 --> 00:04:45.200
Everybody just gets it automatically.

00:04:45.330 --> 00:04:49.850
The difference is you'll see
dramatically decreased CPU usage and

00:04:49.850 --> 00:04:53.480
a lot more time for doing processing
or other things in your app.

00:04:54.810 --> 00:04:59.460
Next up,
support for just-in-time compression.

00:04:59.630 --> 00:05:03.140
So we know that there's a
mix of consumer-based and

00:05:03.140 --> 00:05:06.070
pro apps on the desktop,
and we have a feature in

00:05:06.240 --> 00:05:09.870
AV Foundation on the desktop
that's kind of just for pro apps,

00:05:10.190 --> 00:05:13.440
which is that you can do
frame-accurate starting and stopping.

00:05:13.470 --> 00:05:17.050
This helps if you want to, say,
switch between one recorded movie

00:05:17.050 --> 00:05:20.990
to another recorded movie on the
fly without dropping any frames.

00:05:20.990 --> 00:05:24.340
That's a neat feature,
but it does come at a price,

00:05:24.370 --> 00:05:27.970
because in order to do that,
to guarantee that we can switch on any

00:05:28.000 --> 00:05:30.950
frame boundary to a new movie file,
we have to compress all

00:05:30.950 --> 00:05:34.550
the time in the background,
even when you're just previewing.

00:05:34.580 --> 00:05:38.800
So that comes with a big power
cost to doing that all the time.

00:05:38.810 --> 00:05:43.840
In Mac OS X 10.8, you must opt in for the
frame-accurate start behavior.

00:05:43.840 --> 00:05:47.230
We think that most clients
would rather have the power win

00:05:47.240 --> 00:05:49.280
and not a frame-accurate start.

00:05:49.280 --> 00:05:53.880
So if you implement a delegate
to the AVCapture file output,

00:05:53.880 --> 00:05:53.880
you can just do that.

00:05:54.020 --> 00:05:57.820
You can choose whether you want
frame-accurate start or not.

00:05:57.840 --> 00:06:01.910
If you want frame-accurate start,
then in capture output should provide

00:06:01.910 --> 00:06:04.890
sample-accurate recording start,
you'll answer yes,

00:06:05.150 --> 00:06:07.750
and it will compress all the time,
and you'll still burn a lot of power,

00:06:07.750 --> 00:06:10.760
but you will have the
feature that you depend upon.

00:06:11.150 --> 00:06:15.500
This feature is great for lowering
power consumption when previewing.

00:06:15.720 --> 00:06:18.730
And please see the updated
AV Recorder sample code for

00:06:18.730 --> 00:06:20.350
an example of how to do this.

00:06:20.470 --> 00:06:22.510
This is up there right now.

00:06:24.170 --> 00:06:28.150
Okay, lastly, with 10.8,
we have the newly published

00:06:28.320 --> 00:06:30.340
CoreMediaIO DAO SDK.

00:06:30.580 --> 00:06:33.110
A lot of times on the developer forums,
we're asked,

00:06:33.110 --> 00:06:34.950
"How do I write a video device driver?

00:06:34.950 --> 00:06:37.370
Is there a replacement
for sequence grabber?

00:06:37.370 --> 00:06:42.220
What do I use?" And last year,
we made the CoreMediaIO framework

00:06:42.220 --> 00:06:47.220
available with support for writing
DAO or device abstraction layer drivers.

00:06:47.370 --> 00:06:53.370
That's if you want to write a native
64-bit device driver for a video card or,

00:06:53.400 --> 00:06:56.420
you know, a little gadget on OS X.

00:06:56.530 --> 00:07:01.240
But we didn't make it very easy for
you because we didn't release an SDK.

00:07:01.240 --> 00:07:04.610
This year,
we've just released to the public,

00:07:04.700 --> 00:07:08.500
not just WWC, but to everyone,
a sample SDK that

00:07:08.500 --> 00:07:12.610
includes a sample device,
both an assistant and a plug-in,

00:07:12.630 --> 00:07:16.600
so it shows how to make it a
shareable device across processes.

00:07:16.770 --> 00:07:20.920
And if you're interested in doing this,
please come and see us in the labs and

00:07:21.120 --> 00:07:24.950
talk to us about how to use this SDK.

00:07:25.050 --> 00:07:25.960
That's the URL right there.

00:07:28.650 --> 00:07:31.400
All right,
let's move on to the camera ecosystem.

00:07:31.400 --> 00:07:35.200
I know you're all eager
to get into the APIs.

00:07:35.200 --> 00:07:36.700
I am, too.

00:07:36.700 --> 00:07:38.890
As developers,
we tend to kind of obsess over the

00:07:38.890 --> 00:07:42.180
minute details of these APIs and
how we can use them for our

00:07:42.180 --> 00:07:44.340
nefarious purposes in our apps.

00:07:44.450 --> 00:07:46.950
But I'd like to start by
taking a step back and just

00:07:47.230 --> 00:07:50.700
looking at the bigger picture,
because your capture apps,

00:07:50.780 --> 00:07:53.890
especially on iOS,
are part of a bigger ecosystem.

00:07:54.000 --> 00:07:57.420
There are a lot of concerns
that you need to be aware of,

00:07:57.730 --> 00:08:01.600
chief among them being privacy
and sensitivity of data.

00:08:01.680 --> 00:08:05.610
So I wanted to just talk about the
bigger picture before we dive deep.

00:08:06.480 --> 00:08:12.600
Apple's Camera app saves photos
and videos to a central library.

00:08:12.710 --> 00:08:16.730
You know, before iOS 4 and before we gave
you access to this library,

00:08:16.730 --> 00:08:20.900
this was the place where only the
Camera app could write photos to.

00:08:20.900 --> 00:08:25.930
It's called the Assets Library,
and as of iOS 4, you have read/write

00:08:25.940 --> 00:08:27.350
access to it in your apps.

00:08:27.420 --> 00:08:31.050
That means that you get access to
the photos that people have taken

00:08:31.050 --> 00:08:33.040
on the device in their camera roll.

00:08:33.040 --> 00:08:37.130
You have access to synced
assets that came in from iTunes.

00:08:37.130 --> 00:08:41.520
You have access to saved assets
that they took from your app

00:08:41.520 --> 00:08:43.720
or mail or some other source.

00:08:43.720 --> 00:08:46.360
And now we have photo streams as well.

00:08:46.360 --> 00:08:50.200
All of this data is there for
your reading and for writing.

00:08:51.630 --> 00:08:55.290
But all of this data is
sensitive and personal,

00:08:55.310 --> 00:08:58.440
and it's a big deal,
just as location of the device

00:08:58.550 --> 00:09:00.440
is a big deal to people.

00:09:00.440 --> 00:09:03.670
They want to have confidence
that their photos are not being

00:09:03.670 --> 00:09:08.160
used behind their backs or for
purposes that they're not aware of.

00:09:08.360 --> 00:09:11.910
So as of iOS 6,
devices now prompt the user to

00:09:12.110 --> 00:09:14.610
grant access to the library.

00:09:14.890 --> 00:09:19.210
So when you take a picture
the first time in your app,

00:09:19.870 --> 00:09:22.800
I think it's a good thing, too.

00:09:22.800 --> 00:09:27.400
The first time your app tries
to write to the assets library,

00:09:27.530 --> 00:09:31.220
the user will be told about it
and will grant access or not.

00:09:31.550 --> 00:09:36.550
What that means is your app may now
fail in places where it didn't before.

00:09:36.640 --> 00:09:40.790
Don't just assume that you have
automatic access to the assets library,

00:09:40.890 --> 00:09:42.900
so please handle errors.

00:09:43.510 --> 00:09:44.270
Great.

00:09:44.430 --> 00:09:45.760
Now we're on to the good stuff.

00:09:45.940 --> 00:09:49.470
AV Foundation capture features in iOS 6.

00:09:49.790 --> 00:09:54.120
We've got three main areas
of feature improvements that

00:09:54.120 --> 00:09:54.940
we'd like to talk about today.

00:09:54.940 --> 00:09:57.010
The first is video stabilization.

00:09:57.180 --> 00:10:01.480
We'll talk about real-time face
detection and AVCapture video

00:10:01.480 --> 00:10:03.430
preview layer enhancements.

00:10:03.600 --> 00:10:05.800
Stabilization first.

00:10:05.800 --> 00:10:06.880
What is video stabilization?

00:10:06.880 --> 00:10:10.130
As the name implies,
it steadies shaky shots.

00:10:10.130 --> 00:10:13.450
And with help from the
gyro and core motion,

00:10:13.450 --> 00:10:17.460
we're able to take what would
previously be unusable footage and

00:10:17.590 --> 00:10:21.190
make it into something that's a good,
lasting memory that

00:10:21.190 --> 00:10:23.880
someone can actually use.

00:10:23.880 --> 00:10:28.500
Besides steadying for shake in the hand,
it's also compensating for

00:10:28.500 --> 00:10:30.270
rolling shutter artifacts.

00:10:30.270 --> 00:10:32.860
If you're not familiar
with what those are,

00:10:32.910 --> 00:10:36.780
rolling shutter is local motion that
happens in a frame between the time

00:10:36.780 --> 00:10:40.620
it started being captured and to
the end time when it was captured.

00:10:40.620 --> 00:10:43.940
It can manifest itself as
a little ripple or a wave.

00:10:44.160 --> 00:10:46.940
Here we see a picture of me
riding my bike down a hill.

00:10:46.970 --> 00:10:51.370
And as you'll see here,
I witnessed a phenomenon.

00:10:51.380 --> 00:10:55.040
It was a spontaneous change
in the curvature of the

00:10:55.390 --> 00:10:56.330
earth right before my eyes.

00:10:56.450 --> 00:11:00.480
But luckily,
my phone was there to capture it.

00:11:00.990 --> 00:11:04.130
This can be really disconcerting
to users because it sort of,

00:11:04.250 --> 00:11:06.930
you know,
gives a seasick or earthquake effect,

00:11:06.930 --> 00:11:10.170
and it can really ruin
otherwise good video.

00:11:10.490 --> 00:11:13.180
The most dramatic way to show the
effects of video stabilization

00:11:13.240 --> 00:11:15.070
is to show you a before/after.

00:11:15.170 --> 00:11:16.900
And here we have on the left the before.

00:11:16.900 --> 00:11:23.200
This is shot by a member from our team
in a small boat traveling in the Alameda.

00:11:23.270 --> 00:11:25.600
And it's great because we get
to see this bridge closing,

00:11:25.600 --> 00:11:27.360
the big green bridge.

00:11:27.450 --> 00:11:30.840
And what you see on the left is
obviously shaking around a lot.

00:11:30.840 --> 00:11:34.900
He's doing his best to hold
the phone absolutely still.

00:11:34.950 --> 00:11:38.560
But, you know,
the motor is going and there are waves.

00:11:38.700 --> 00:11:42.060
So things up close look okay,
but the farther away we get,

00:11:42.120 --> 00:11:44.760
and particularly when you see
all of these straight lines,

00:11:44.820 --> 00:11:48.160
it's really obvious when there's shake.

00:11:48.260 --> 00:11:51.520
And what we have on the left is
probably not a piece of footage

00:11:51.520 --> 00:11:56.410
that I would choose to keep
because it makes me sick to watch.

00:11:56.490 --> 00:11:59.020
But if you look on the right side,
take a look at this.

00:11:59.020 --> 00:12:00.600
This is really dramatic.

00:12:00.660 --> 00:12:02.960
The left side is just kind
of juddering all around,

00:12:02.960 --> 00:12:05.270
but the right side is
absolutely rock solid.

00:12:05.490 --> 00:12:08.850
It looks almost as if he had a Steadicam
there on the boat and he was just,

00:12:08.850 --> 00:12:12.220
you know,
gently panning from side to side.

00:12:12.400 --> 00:12:14.670
So applaud for that.

00:12:14.720 --> 00:12:17.150
That's a technical marvel.

00:12:20.690 --> 00:12:22.200
So video stabilization, why use it?

00:12:22.200 --> 00:12:26.540
Well, I think the pros are obvious.

00:12:26.740 --> 00:12:29.280
Camera phones in general
are susceptible to shake,

00:12:29.340 --> 00:12:30.730
partly because of how people hold them.

00:12:30.740 --> 00:12:32.230
You know, they're very light.

00:12:32.380 --> 00:12:33.870
They're held with one hand.

00:12:34.010 --> 00:12:36.400
It's hard to hold your
arm exactly straight.

00:12:36.560 --> 00:12:40.630
So there's going to be some
shake when people take videos.

00:12:40.850 --> 00:12:45.380
Also because the bigger we go,
and remember, our images are getting

00:12:45.380 --> 00:12:46.930
bigger and bigger and bigger.

00:12:47.030 --> 00:12:51.000
The original iPhone that did
video only shot at 640 by 480.

00:12:51.000 --> 00:12:52.830
Now we're up to 1080p.

00:12:52.940 --> 00:12:56.140
And the HD resolution recordings are
especially susceptible to rolling

00:12:56.140 --> 00:12:58.250
shutter because they have more pixels.

00:12:58.390 --> 00:13:00.620
There's more time for that
local motion to happen in

00:13:00.620 --> 00:13:03.350
the frame and wreck the shot.

00:13:03.600 --> 00:13:06.590
Stabilization saves
otherwise unusable footage,

00:13:06.590 --> 00:13:10.150
which is great, because it means you've
saved a memory for someone.

00:13:10.680 --> 00:13:12.830
and the kicker is it works in real time.

00:13:12.970 --> 00:13:15.900
You don't have to import
it into iMovie afterwards,

00:13:15.930 --> 00:13:20.150
let it analyze the data,
crop out some area so that you've

00:13:20.150 --> 00:13:21.730
lost some of your field of view.

00:13:21.790 --> 00:13:24.000
It just works in real
time and when it's done,

00:13:24.100 --> 00:13:28.700
you have a 1920 by 1080 movie
that's already stabilized and

00:13:28.700 --> 00:13:30.300
this is a wonderful thing.

00:13:30.340 --> 00:13:31.680
Why not use stabilization?

00:13:31.680 --> 00:13:36.960
Well, I think most people will
want to use stabilization,

00:13:37.130 --> 00:13:40.230
but if you have some pixel
processing algorithms in place that

00:13:40.450 --> 00:13:45.640
might not interact well with it,
you should be aware of what it's doing.

00:13:45.730 --> 00:13:49.020
Stabilization does alter the pixels
because it's correcting for shake.

00:13:49.020 --> 00:13:51.830
It does have to move them around.

00:13:51.830 --> 00:13:57.460
So that means your output that you
see in the movie is no longer matching

00:13:57.460 --> 00:13:58.990
what you see in the video preview.

00:13:58.990 --> 00:14:02.150
If it's important for you to have
a one-to-one correspondence between

00:14:02.150 --> 00:14:06.040
what's coming out the video data output
and what's being shown in the preview,

00:14:06.190 --> 00:14:08.060
stabilization will no
longer make that true.

00:14:08.120 --> 00:14:11.770
And it may not interoperate
well with whatever pixel

00:14:11.870 --> 00:14:14.950
processing algorithms you have,
in which case you would

00:14:15.050 --> 00:14:16.140
want to turn it off.

00:14:16.220 --> 00:14:18.680
Also, you should be aware that
it does add latency,

00:14:18.720 --> 00:14:21.530
one or more frames to
the video data output.

00:14:21.700 --> 00:14:25.500
So expect just a little bit more
lag when stabilization is turned on.

00:14:27.380 --> 00:14:28.860
Where is it supported?

00:14:28.920 --> 00:14:33.120
It's supported on iPhone
4S and the new iPad.

00:14:34.300 --> 00:14:35.270
Compatibility.

00:14:35.340 --> 00:14:38.600
As I stated before,
all the HD resolutions

00:14:38.600 --> 00:14:39.520
are now compatible.

00:14:39.780 --> 00:14:42.290
In iOS 5, we only did 1080p.

00:14:42.370 --> 00:14:46.000
Now we've extended that support
to all the HD resolutions,

00:14:46.000 --> 00:14:48.050
720p and 540p as well.

00:14:52.040 --> 00:14:52.880
Where does it not work?

00:14:52.990 --> 00:14:54.490
Well, it does not work with front camera.

00:14:54.500 --> 00:14:59.040
We do not stabilize video from the
front camera because there it's SD only,

00:14:59.040 --> 00:15:03.000
and typically the subject
in the picture is just,

00:15:03.120 --> 00:15:05.150
you know,
maybe a meter away from the screen,

00:15:05.390 --> 00:15:09.240
so stabilization is not a big win there.

00:15:09.480 --> 00:15:11.820
It does not work with
AVCapture still image output.

00:15:12.100 --> 00:15:14.230
Still images are not stabilized.

00:15:14.310 --> 00:15:17.740
And it also does not work with preview,
as I stated before.

00:15:17.800 --> 00:15:20.760
Usually when you stabilize preview,
people have this weird feedback

00:15:20.830 --> 00:15:23.440
effect of trying to correct for
what's already been corrected,

00:15:23.440 --> 00:15:25.430
and then they can't really
follow themselves well.

00:15:25.580 --> 00:15:30.260
It's better to keep the preview
uncorrected for stabilization.

00:15:30.340 --> 00:15:33.550
Now, in iOS 5,
we did give some limited support to

00:15:33.550 --> 00:15:37.480
developers for using stabilization,
but largely it happened

00:15:37.480 --> 00:15:38.760
behind your backs.

00:15:38.820 --> 00:15:42.840
Movie file output always
stabilizes 1080p video,

00:15:42.840 --> 00:15:44.420
period, on iOS 5.

00:15:44.470 --> 00:15:46.000
Nothing you can do about it.

00:15:46.060 --> 00:15:49.020
It never stabilizes any other resolution.

00:15:49.120 --> 00:15:54.790
And video data output also never
stabilizes any session preset.

00:15:54.860 --> 00:15:56.770
And there was no API to opt in or out.

00:15:56.850 --> 00:16:00.130
So 1080p, you're getting it if
you're recording to movies,

00:16:00.130 --> 00:16:01.060
nowhere else.

00:16:01.100 --> 00:16:05.210
But in iOS 6,
we're keeping the behavior the same

00:16:05.210 --> 00:16:08.120
if your app is linked before iOS 6.

00:16:08.120 --> 00:16:12.520
But then once you
recompile against iOS 6,

00:16:12.520 --> 00:16:16.080
you must opt in for
stabilization where you want it.

00:16:16.290 --> 00:16:19.410
Both movie file output and
video data output are eligible,

00:16:19.460 --> 00:16:22.270
so you can use it with either one.

00:16:22.670 --> 00:16:24.250
And here's how you opt in.

00:16:24.340 --> 00:16:26.900
There's a little bit of boilerplate code.

00:16:26.900 --> 00:16:29.500
You're probably familiar with--you know,
you create a capture session,

00:16:29.500 --> 00:16:32.270
you create a device input
for the back camera.

00:16:32.460 --> 00:16:36.100
You make either a movie file
output or a video data output.

00:16:36.140 --> 00:16:39.080
You get a reference to its
connection by getting the

00:16:39.090 --> 00:16:41.540
connection with media type video.

00:16:42.040 --> 00:16:44.070
And then you opt in for
it when it's available.

00:16:44.160 --> 00:16:46.960
So if video stabilization is
supported on that particular

00:16:47.230 --> 00:16:51.640
platform for that particular camera,
you say, "Set enables video stabilization

00:16:51.640 --> 00:16:54.960
when available to Yes." Now,
as I stated earlier,

00:16:54.960 --> 00:16:58.280
it's not--not all session
presets are stabilized.

00:16:58.540 --> 00:16:59.680
The SD ones are not.

00:16:59.860 --> 00:17:01.060
The photo preset is not.

00:17:01.060 --> 00:17:03.100
How will you know when it kicks in?

00:17:03.230 --> 00:17:06.200
You can observe,
using key-value observation,

00:17:06.200 --> 00:17:09.500
the connection's video
stabilization enabled property.

00:17:09.600 --> 00:17:11.660
It will flip to Yes when
stabilization goes on,

00:17:11.660 --> 00:17:16.130
and it will flip to off
when it's no longer in use.

00:17:16.310 --> 00:17:18.260
There are a couple of gotchas that
you should know about with video

00:17:18.340 --> 00:17:21.440
stabilization and in general with
setting properties on connections

00:17:21.440 --> 00:17:23.160
that I wanted to call out.

00:17:23.270 --> 00:17:26.640
When inputs or outputs are
added to a capture session,

00:17:26.640 --> 00:17:29.010
connections-- these are a
different kind of object,

00:17:29.090 --> 00:17:32.300
an AVCapture connection-- they
are implicitly formed between

00:17:32.300 --> 00:17:34.580
compatible inputs and outputs.

00:17:34.690 --> 00:17:35.500
This is what it looks like.

00:17:35.600 --> 00:17:37.740
Let's say you have a
session and an output.

00:17:37.740 --> 00:17:39.730
Let's say a movie file output.

00:17:39.830 --> 00:17:44.960
And then you add an AVCapture
device input to your session.

00:17:45.050 --> 00:17:48.460
What you don't see is
that behind the scenes,

00:17:48.560 --> 00:17:51.810
the session asks the device input,
"What kind of media can you

00:17:51.810 --> 00:17:55.040
produce?" And it asks the outputs,
"What kind of media can you

00:17:55.170 --> 00:17:59.200
accept?" And it finds the matches,
and it forms a connection between them.

00:17:59.300 --> 00:18:01.920
In this case,
that device input produces video,

00:18:01.990 --> 00:18:06.020
and the movie file output accepts video,
so it implicitly forms an

00:18:06.020 --> 00:18:10.210
AVCapture connection for video,
and it's there.

00:18:10.740 --> 00:18:13.040
Now, the reverse is also true.

00:18:13.090 --> 00:18:17.270
If you switch cameras,
so you need to remove an input

00:18:17.540 --> 00:18:20.800
and then add a new input,
what happens is as soon as you

00:18:20.800 --> 00:18:25.100
remove that AVCapture device
input for the back camera,

00:18:25.130 --> 00:18:29.470
it also implicitly severs the connection
to the existing AVCapture connection

00:18:29.770 --> 00:18:31.220
and both disappear at the same time.

00:18:31.220 --> 00:18:36.570
So what that means is any settings
that you have-- that you've applied

00:18:36.570 --> 00:18:38.460
to that connection are now lost.

00:18:38.460 --> 00:18:43.190
So you need to add the new input
and reapply the same settings that

00:18:43.190 --> 00:18:45.450
you did to the new connection.

00:18:47.490 --> 00:18:51.440
And I highly recommend using
AVCapturEssion's begin configuration

00:18:51.440 --> 00:18:56.450
and commit configuration when
you are doing these high-level,

00:18:56.450 --> 00:19:00.170
big graph changes,
because what it does is it holds off the

00:19:00.820 --> 00:19:05.630
commit of any of these property changes
until you say commit configuration,

00:19:05.630 --> 00:19:10.110
and it prevents multiple stops
and restarts of the graph.

00:19:11.080 --> 00:19:14.380
You can see the updated AVCam sample
code for an example of how to

00:19:14.390 --> 00:19:19.570
opt in for video stabilization,
and that, too, is already up there.

00:19:20.480 --> 00:19:23.840
All right,
on to real-time face detection.

00:19:23.890 --> 00:19:24.640
This is neat.

00:19:24.700 --> 00:19:27.340
Face detection,
this is the same face detector

00:19:27.340 --> 00:19:29.890
that's used in the camera app,
and it's different than the

00:19:29.890 --> 00:19:33.460
one we showed you last year,
which was CI Face Detector.

00:19:33.600 --> 00:19:35.600
Look at those cute kids.

00:19:35.780 --> 00:19:40.320
Scans for faces in real time,
and it can track up to

00:19:40.410 --> 00:19:42.790
10 faces in real time.

00:19:43.350 --> 00:19:44.770
So the contrast isn't great up here.

00:19:44.840 --> 00:19:48.400
I hope you can see all of the
rectangles around their faces.

00:19:48.510 --> 00:19:52.380
It assigns a unique ID to
each face in the frame.

00:19:52.860 --> 00:19:58.660
And the detector sees the camera in
the native orientation of the camera.

00:19:58.660 --> 00:20:00.500
That is,
if you take out your iPhone right

00:20:00.500 --> 00:20:04.740
now and you turn it sideways and
put the home button on the right,

00:20:04.880 --> 00:20:07.940
that's the orientation in
which the camera is mounted.

00:20:08.080 --> 00:20:11.700
So in this picture, for instance,
even though we're holding it portrait,

00:20:11.700 --> 00:20:15.390
it found the girl on the right first
because she's closest to the top left

00:20:15.540 --> 00:20:18.740
if you rotate by 90 degrees to the left.

00:20:19.580 --> 00:20:22.640
It provides a timestamp for each face.

00:20:22.640 --> 00:20:24.060
The timestamp will be the same.

00:20:24.160 --> 00:20:28.040
It correlates to the frame
in which that face was found.

00:20:28.100 --> 00:20:30.490
And that's good when you want
to match it up with video later.

00:20:30.540 --> 00:20:32.140
We'll talk about that later.

00:20:32.190 --> 00:20:35.410
It also finds the rectangle
bounding each face.

00:20:35.510 --> 00:20:39.980
And these rectangles are given in
the coordinate space of the device.

00:20:40.170 --> 00:20:44.520
That is, they're scalar coordinates
from zero to one.

00:20:45.380 --> 00:20:48.890
Again, in the un-rotated space.

00:20:49.110 --> 00:20:50.940
It can also determine the roll angle.

00:20:50.940 --> 00:20:56.940
So you see the tall boy right
there has his face turned slightly.

00:20:58.400 --> 00:21:31.800
[Transcript missing]

00:21:32.160 --> 00:21:36.740
It can determine roll angles in
increments of 30 degrees either way.

00:21:36.820 --> 00:21:41.970
It can detect faces all the way from
straight up to all the way upside down.

00:21:41.970 --> 00:21:44.320
So great for finding kids on monkey bars.

00:21:46.000 --> 00:21:48.550
It also determines the yaw angle.

00:21:48.580 --> 00:21:51.140
Yaw is also known as turn.

00:21:51.140 --> 00:21:53.360
Before we were doing tilt,
now we're doing turn.

00:21:53.510 --> 00:21:57.840
So here we see that the tall
boy has turned his face,

00:21:57.910 --> 00:22:01.660
and so it's going to detect
that he has a yaw angle of 315,

00:22:01.660 --> 00:22:07.710
or minus 45 degrees from
center from the perspective of

00:22:07.710 --> 00:22:10.060
the person turning the face.

00:22:10.060 --> 00:22:13.300
It always uses positive angles,
so you'll always see

00:22:13.300 --> 00:22:15.790
something between 0 and 360.

00:22:15.890 --> 00:22:18.820
Also notice that it no longer
finds the girl on the right.

00:22:18.850 --> 00:22:21.290
That's because she's
turned her head too far.

00:22:21.390 --> 00:22:27.440
It can only find faces that are between
0 and 90 this way and 0 and 90 this way.

00:22:27.570 --> 00:22:34.730
So between 91 degrees and 269 degrees,
it won't find a face anymore.

00:22:36.160 --> 00:22:39.000
And good news,
it works with front and back camera,

00:22:39.140 --> 00:22:40.230
all presets.

00:22:40.280 --> 00:22:44.460
It's resolution independent,
and it happens quickly regardless of

00:22:44.520 --> 00:22:47.740
which capture preset you might be using.

00:22:47.900 --> 00:22:49.140
What does it not do?

00:22:49.330 --> 00:22:54.120
Sorry aliens, sorry pets,
it will not find you.

00:22:54.120 --> 00:22:57.140
Unless you're an alien that
looks a lot like a human.

00:22:59.320 --> 00:23:01.620
It also does not recognize
particular faces.

00:23:01.690 --> 00:23:02.700
That's a distinction to make.

00:23:02.700 --> 00:23:03.840
It's a face detector.

00:23:03.840 --> 00:23:05.840
It's not a face recognizer.

00:23:05.970 --> 00:23:09.280
So while it might find two faces
here and assign an ID to them,

00:23:09.280 --> 00:23:13.590
it's not going to know that that's
Princess Clara and Sad Captain America.

00:23:14.340 --> 00:23:16.300
No.

00:23:16.390 --> 00:23:17.910
It also does not remember faces.

00:23:18.050 --> 00:23:20.940
That means that once it's
assigned an ID to a face,

00:23:21.000 --> 00:23:23.600
if that face turns too
far and it loses it,

00:23:23.740 --> 00:23:26.300
or if the face goes out of
the frame and comes back in,

00:23:26.410 --> 00:23:27.910
it's a new face to the detector.

00:23:27.960 --> 00:23:32.600
It assigns it a new
ID that's incremented higher.

00:23:33.210 --> 00:23:35.110
It also does not
currently determine pitch,

00:23:35.210 --> 00:23:39.040
so the nose up or nose down,
it can't tell the difference.

00:23:39.140 --> 00:23:40.790
It's either a face that's there or not.

00:23:40.930 --> 00:23:43.970
We'll tell you yawn, roll, but not pitch.

00:23:46.400 --> 00:23:49.300
It also does not find, as I said before,
faces with a yaw angle

00:23:49.300 --> 00:23:51.500
between 91 and 269.

00:23:51.570 --> 00:23:54.970
It has to be able to see some of
the defining features of the face

00:23:55.350 --> 00:23:57.750
to determine that it is a face.

00:23:58.210 --> 00:24:02.280
So why might you want to use
AV Foundation's real-time face

00:24:02.370 --> 00:24:06.820
detector as opposed to the CI face
detector that we showed you last year?

00:24:06.860 --> 00:24:09.920
Well, first and foremost,
it's optimized for real-time capture.

00:24:09.920 --> 00:24:12.480
It's the same one that's
used by the camera app.

00:24:12.580 --> 00:24:14.160
It's hardware accelerated.

00:24:14.270 --> 00:24:16.250
So it uses very little CPU.

00:24:17.860 --> 00:24:22.180
It's capture resolution independent,
meaning it doesn't matter if you're in

00:24:22.400 --> 00:24:26.130
a photo preset with very large images,
it will still find

00:24:26.130 --> 00:24:27.980
faces at the same speed.

00:24:28.130 --> 00:24:30.440
It also supports
tracking faces over time.

00:24:30.500 --> 00:24:33.180
The CI Face Detector
interface is a push one,

00:24:33.220 --> 00:24:36.630
meaning every time you feed it a frame,
it searches through the

00:24:36.630 --> 00:24:37.890
frame to find faces.

00:24:38.030 --> 00:24:41.000
Here, once we've found a face,
we can lock onto them and we don't

00:24:41.000 --> 00:24:44.880
need to search the whole frame again
and again to find that same face.

00:24:44.970 --> 00:24:47.640
So once it's found a face,
the tracking is very fast,

00:24:47.720 --> 00:24:49.860
and the latency is very low.

00:24:50.780 --> 00:24:54.480
But that doesn't mean that Core Image's
CI face detector is no longer relevant.

00:24:54.550 --> 00:24:57.480
To the contrary,
it's still very relevant.

00:24:57.860 --> 00:24:58.800
Why?

00:24:58.800 --> 00:25:02.590
Well, it's available on all
supported iOS devices in iOS 6.

00:25:02.750 --> 00:25:06.270
The same is not true for
the real-time face detector.

00:25:06.350 --> 00:25:09.780
And also because of its push interface,
that means you can use

00:25:09.780 --> 00:25:11.060
arbitrary source images.

00:25:11.060 --> 00:25:12.960
They don't have to come from the camera.

00:25:13.010 --> 00:25:16.480
You could pull an image from the assets
library and find faces in that one,

00:25:16.480 --> 00:25:18.410
and it will tell you about them.

00:25:18.620 --> 00:25:21.670
The AV Foundation real-time face
detector only works with content

00:25:21.670 --> 00:25:24.480
coming from the camera in real time.

00:25:25.240 --> 00:25:26.650
All right, so enough talk.

00:25:26.700 --> 00:25:27.580
Let's have a demo.

00:25:27.590 --> 00:25:32.770
I'd like to call up Ethan Tiratompson
to give us a demo of StashCam 2.

00:25:37.310 --> 00:25:38.740
Hi, everyone.

00:25:38.740 --> 00:25:43.880
I've been working with Brad to
update StashCam for these new APIs.

00:25:43.880 --> 00:25:47.310
But I'd like to start with reviewing
the core image face detection,

00:25:47.320 --> 00:25:49.510
which you see running here,
because as Brad pointed out,

00:25:49.640 --> 00:25:52.320
that this is still pertinent
for a variety of situations,

00:25:52.320 --> 00:25:55.170
such as if your images are not
coming from a capture source or if

00:25:55.170 --> 00:25:58.620
you aren't running on a device with
the necessary hardware support.

00:26:00.620 --> 00:26:04.860
But now if I switch to running
AV Foundation's face detection,

00:26:04.860 --> 00:26:06.930
we see that it gets similar results.

00:26:08.280 --> 00:26:09.470
With a much higher frame rate.

00:26:10.240 --> 00:26:10.490
Yes.

00:26:10.550 --> 00:26:13.020
If you look at the top,
the frame rate's running there.

00:26:13.020 --> 00:26:15.570
So you see it's getting the
full 30 frames a second.

00:26:15.580 --> 00:26:20.150
And it hasn't dropped the
core image frame rate at all.

00:26:20.150 --> 00:26:24.910
So it's able to do that without
stealing any CPU from whatever

00:26:24.910 --> 00:26:26.800
else you've got running.

00:26:26.800 --> 00:26:30.180
So now if I put on my Stash,
and I'm going to run it,

00:26:30.190 --> 00:26:34.640
you can see some of the new fun features
that we've been -- that this supports.

00:26:34.790 --> 00:26:45.480
So we've got the rolls and the yaws.

00:26:51.090 --> 00:26:55.430
And to show you the ID tracking,
I can tap on my face and

00:26:55.510 --> 00:26:56.720
I can get to a pirate.

00:26:56.880 --> 00:26:57.840
Arr!

00:26:57.940 --> 00:26:59.990
I think Brad looks particularly
good in a clown outfit.

00:27:00.420 --> 00:27:01.000
Scary face.

00:27:01.000 --> 00:27:02.000
Yeah, very scary.

00:27:02.620 --> 00:27:03.000
Scary clown.

00:27:03.070 --> 00:27:05.000
And we also in this have a still capture.

00:27:05.000 --> 00:27:09.000
So this is operating on
the video preview layer.

00:27:09.000 --> 00:27:13.000
I can tap the button at the bottom and
save my StashCam memories for later.

00:27:13.000 --> 00:27:16.870
And there's all the API in
there for using that.

00:27:16.980 --> 00:27:18.980
So that is StashCam.

00:27:19.490 --> 00:27:19.930
Great.

00:27:19.930 --> 00:27:20.950
Thank you, Ethan.

00:27:25.070 --> 00:27:28.990
StashCam 2 ships with built-in
support for mustaches,

00:27:29.080 --> 00:27:32.890
clowns, and pirates,
and we're going to make aliens and pet

00:27:32.990 --> 00:27:36.070
overlays available as an in-app purchase.

00:27:37.250 --> 00:27:39.200
All right,
so let's talk about how we did that.

00:27:39.240 --> 00:27:43.440
First, he showed you there was still that
legacy CIA face detector path that

00:27:43.520 --> 00:27:48.140
was in the first revision of StashCam,
and this is how you do it.

00:27:48.240 --> 00:27:51.590
It's got a device input
at the top and a session,

00:27:51.740 --> 00:27:53.680
and at the bottom,
it has a video data output,

00:27:53.750 --> 00:27:56.910
a still image output,
and a preview layer.

00:27:57.250 --> 00:28:00.100
As it gets frames in real time
through the video data output,

00:28:00.100 --> 00:28:03.480
it pushes them in one at a
time to the CI face detector,

00:28:03.660 --> 00:28:07.740
which scans the frames, finds the faces,
returns a result,

00:28:07.790 --> 00:28:11.710
and then he uses those results to overlay
the red rectangles on top of the video

00:28:11.710 --> 00:28:14.610
preview layer as core animation layers.

00:28:14.730 --> 00:28:17.750
And he also uses the CI face
detector when taking a still image

00:28:17.900 --> 00:28:24.650
down the CI face detector path,
and then uses the result to composite

00:28:24.650 --> 00:28:29.830
using CG and then using image IO to write
the JPEG and record it to assets library.

00:28:29.840 --> 00:28:31.620
So all that code is still in there.

00:28:31.620 --> 00:28:35.310
This is a great sample because it
shows you both ways to use faces.

00:28:35.450 --> 00:28:38.560
It also has the real time path,
and the programming model

00:28:38.560 --> 00:28:40.200
is a little bit different.

00:28:40.220 --> 00:28:41.150
It's the same at the top.

00:28:41.150 --> 00:28:43.700
We still have a still image
output and a video preview layer,

00:28:43.700 --> 00:28:46.690
but then in the middle,
there's this new kind of output

00:28:46.690 --> 00:28:48.920
called an AVCapture metadata output.

00:28:49.060 --> 00:28:51.270
Notice it's not a face-specific one.

00:28:51.280 --> 00:28:53.390
It's for metadata in general.

00:28:53.970 --> 00:28:58.650
And the metadata output
outputs an array of metadata,

00:28:58.650 --> 00:29:02.130
or in this case, an array of faces,
which he can then use to draw

00:29:02.240 --> 00:29:03.930
the mustache layers as before.

00:29:04.160 --> 00:29:08.640
And he can use that same AV metadata
output to pair with a still image and use

00:29:08.640 --> 00:29:11.400
CG as before to composite the mustaches.

00:29:11.680 --> 00:29:15.140
Let's talk more about this
AVCapture metadata output.

00:29:15.880 --> 00:29:18.950
Previously,
AVCapture device input would only

00:29:19.330 --> 00:29:21.520
expose a single kind of input port.

00:29:21.560 --> 00:29:23.410
It would say, "I have video available.

00:29:23.540 --> 00:29:25.720
That's the only kind of
data I can produce." Now,

00:29:25.720 --> 00:29:29.150
on supported platforms,
that device input will say,

00:29:29.170 --> 00:29:33.860
"I support both video and metadata."
So that input port can be hooked

00:29:33.860 --> 00:29:36.100
up to things that consume metadata.

00:29:36.260 --> 00:29:37.430
And we're in luck.

00:29:37.520 --> 00:29:40.810
We have a new AVCapture output
subclass that does consume metadata

00:29:41.250 --> 00:29:43.140
called the AVCapture Metadata Output.

00:29:43.210 --> 00:29:46.370
Now, today, there's only one kind
of metadata supported,

00:29:46.370 --> 00:29:47.500
which is faces.

00:29:47.930 --> 00:29:51.160
But you can imagine that in the future,
we might add other kinds of metadata

00:29:51.460 --> 00:29:55.020
that can be had through this output.

00:29:55.020 --> 00:29:57.500
If you've used video data output,
you'll be right at home.

00:29:57.500 --> 00:29:59.280
It's patterned after that one.

00:29:59.410 --> 00:30:02.280
So instead of outputting
a sample buffer at a time,

00:30:02.280 --> 00:30:06.860
it outputs an NSArray of AV metadata
objects to your delegate on a

00:30:06.860 --> 00:30:10.620
serial queue that you have defined.

00:30:10.720 --> 00:30:15.190
And it also allows discovery of a bunch
of available metadata object types.

00:30:15.190 --> 00:30:20.030
That means if in the future we add
foo discovery and face discovery,

00:30:20.040 --> 00:30:22.530
you can know what kinds
of metadata objects are

00:30:22.530 --> 00:30:25.600
available through this output,
and you can filter to the

00:30:25.600 --> 00:30:27.380
subset of objects that you want.

00:30:27.470 --> 00:30:30.430
So if you only want faces
and you don't want foos,

00:30:30.430 --> 00:30:34.630
you can specify to the metadata
output that you want to set

00:30:34.700 --> 00:30:36.490
the types to just faces.

00:30:36.600 --> 00:30:43.080
I'd recommend future-proofing your code
now if all you want is faces to do that.

00:30:43.210 --> 00:30:45.600
Now,
what's in this face object that you get?

00:30:45.600 --> 00:30:49.550
In the capture output did output
metadata objects callback,

00:30:49.800 --> 00:30:53.420
you'll get an array of objects,
and they all have a time stamp,

00:30:53.420 --> 00:30:57.470
a rectangle, an ID, a roll angle,
just as I was talking about before.

00:30:57.540 --> 00:31:00.330
That's where you can do
interesting things with your faces.

00:31:02.640 --> 00:31:05.830
So let's talk about the
definition for what a face is.

00:31:06.040 --> 00:31:08.640
First of all,
what's the rectangle of a face?

00:31:08.790 --> 00:31:13.610
The bounds extend from just above
the eyebrows to just below the lips.

00:31:13.790 --> 00:31:17.130
So it's not actually the whole
face with the forehead and all.

00:31:17.280 --> 00:31:19.770
It's about what you see here.

00:31:21.140 --> 00:31:24.990
Also, the CG rect coordinates are
scalar values from zero to one,

00:31:25.010 --> 00:31:26.070
as I said before.

00:31:26.280 --> 00:31:32.640
It's in the--origin is top left,
and it's in the unrotated

00:31:32.770 --> 00:31:35.880
camera devices orientation.

00:31:36.870 --> 00:31:40.560
And CI Face Detector and the
metadata output rectangles are

00:31:40.700 --> 00:31:42.530
comparable in size and origin.

00:31:42.640 --> 00:31:43.580
That's good.

00:31:43.580 --> 00:31:47.360
So if you are going back and forth
between using one detector and the other,

00:31:47.360 --> 00:31:50.360
you can use them with confidence
knowing that the rectangles they

00:31:50.430 --> 00:31:53.670
find and the faces that they find
are going to be roughly the same.

00:31:55.900 --> 00:32:00.640
There's also still image support
when using face detection.

00:32:00.800 --> 00:32:04.440
So if you have a metadata
output and it's enabled,

00:32:04.560 --> 00:32:06.630
and you have an AVCapture
still image output,

00:32:06.750 --> 00:32:09.860
then when you take images,
we will attach the face

00:32:09.860 --> 00:32:13.000
information to those still images.

00:32:13.130 --> 00:32:18.260
And if you then write those still
images using our JPEG still image

00:32:18.420 --> 00:32:22.670
NSData representation-- that's
a mouthful-- to turn it from a

00:32:22.700 --> 00:32:27.290
sample buffer into an NSData,
we'll preserve those face rectangles

00:32:27.300 --> 00:32:32.080
and IDs in the metadata as XMP.

00:32:33.730 --> 00:32:34.590
Where is it supported?

00:32:34.590 --> 00:32:37.030
As I said before,
it's only on the newer devices,

00:32:37.040 --> 00:32:40.740
so iPhone 4S, iPad 2,
and the new iPad all support

00:32:40.740 --> 00:32:43.000
real-time face detection.

00:32:43.500 --> 00:32:44.210
Great.

00:32:44.390 --> 00:32:48.360
On to our third main feature set,
which is AVCapture Video Preview

00:32:48.360 --> 00:32:50.100
Layer Enhancements.

00:32:50.260 --> 00:32:52.960
First, conversion methods.

00:32:53.070 --> 00:32:56.260
Boy, people whine about this a
lot on the developer forums.

00:32:56.260 --> 00:32:59.280
So we're finally giving in,
and we're giving you what you asked for.

00:32:59.490 --> 00:33:03.240
This is a bug that I had,
and I love the title of it,

00:33:03.240 --> 00:33:04.840
so I just included it here.

00:33:05.090 --> 00:33:08.810
Setting focus and exposure points
of interest is ridiculously hard.

00:33:08.920 --> 00:33:12.420
We tend to agree the developer's
point was well taken.

00:33:12.480 --> 00:33:16.980
It seems like a totally arbitrary space,
regardless of device orientation.

00:33:17.120 --> 00:33:19.740
The API is really hard to use.

00:33:20.300 --> 00:33:23.610
So we do have a piece of sample code that
shows how to do it for one orientation,

00:33:23.610 --> 00:33:28.000
but it doesn't apply to all orientations,
and it's a little bit fiddly.

00:33:28.090 --> 00:33:33.250
Let's start with a review of how these
AVCapture device points of interest work.

00:33:33.550 --> 00:33:35.900
A focus point of interest,
this would be if you want to do

00:33:35.900 --> 00:33:38.940
like a tap interface for focusing.

00:33:39.050 --> 00:33:44.190
The point is from zero to one with zero,
zero in the top left and one,

00:33:44.190 --> 00:33:46.040
one in the bottom right.

00:33:46.530 --> 00:33:50.390
and the camera sensor's native or
unrotated orientation is landscape.

00:33:50.610 --> 00:33:52.690
For the back camera,
its native orientation

00:33:52.700 --> 00:33:54.310
is landscape right,
that is,

00:33:54.310 --> 00:33:55.900
with the home button on the right.

00:33:55.900 --> 00:33:59.630
And for the front camera,
the native orientation is landscape left,

00:33:59.750 --> 00:34:02.780
that is,
with the home button on the left.

00:34:03.400 --> 00:34:06.390
So let's say you have a phone
rotated in portrait and you're

00:34:06.430 --> 00:34:09.060
taking a picture of a little girl.

00:34:09.120 --> 00:34:11.680
What the camera sees
is what you see here.

00:34:11.730 --> 00:34:15.360
It's actually a landscape right
orientation with zero in the top

00:34:15.500 --> 00:34:17.840
left and one in the bottom right.

00:34:17.840 --> 00:34:21.720
So what makes translating
these coordinates so hard?

00:34:21.960 --> 00:34:25.000
There's a lot of math involved.

00:34:25.000 --> 00:34:27.290
Preview may be rotated.

00:34:27.900 --> 00:35:56.800
[Transcript missing]

00:35:57.620 --> 00:36:00.750
So you can imagine it's pretty
hard then to figure out if someone

00:36:01.040 --> 00:36:06.620
taps on your video preview layer,
how does that turn into an unrotated,

00:36:06.740 --> 00:36:08.740
unmirrored source.

00:36:09.040 --> 00:36:10.440
Conversion methods to the rescue.

00:36:10.440 --> 00:36:15.700
We've taken all of the trouble out of it
by adding these new conversion methods.

00:36:15.700 --> 00:36:17.060
And here's what it looks like.

00:36:17.120 --> 00:36:20.120
Let's say you have a tap point
from a gesture recognizer.

00:36:20.180 --> 00:36:24.100
You just call preview layer capture
device point of interest for point,

00:36:24.100 --> 00:36:26.930
and it will give you the converted point.

00:36:31.300 --> 00:37:52.800
[Transcript missing]

00:37:53.090 --> 00:37:57.230
on an AVCapture output object
transformed metadata object

00:37:57.680 --> 00:38:00.610
for metadata object connection.

00:38:02.320 --> 00:38:02.570
All right.

00:38:02.890 --> 00:38:05.800
Second video preview layer
enhancement is support for pausing

00:38:05.800 --> 00:38:09.900
and resuming the video preview
layer without stopping the session.

00:38:09.900 --> 00:38:13.360
Many of you want to pause rendering
on the preview layer because

00:38:13.360 --> 00:38:16.960
you've taken a still image,
you want to do some processing,

00:38:16.960 --> 00:38:18.960
you don't want to waste
cycles previewing,

00:38:18.960 --> 00:38:22.460
or you want to just match the
still image that you just took.

00:38:22.570 --> 00:38:26.870
You can do that now because
AVCapture Video Preview Layer exposes

00:38:26.870 --> 00:38:28.880
an AVCapture connection.

00:38:29.020 --> 00:38:29.900
Let you in on a secret.

00:38:29.900 --> 00:38:31.400
It always had a capture connection.

00:38:31.400 --> 00:38:32.360
We just didn't expose it.

00:38:32.480 --> 00:38:36.290
So now that it's public,
you can use all of the properties

00:38:36.290 --> 00:38:39.760
that you would normally use
in an AVCapture connection,

00:38:39.760 --> 00:38:42.740
but with respect to a preview
layer instead of an output.

00:38:42.790 --> 00:38:46.450
So to pause video preview,
all you do is find its

00:38:46.610 --> 00:38:50.800
connection and call setEnabled
"no." And while you do that,

00:38:50.800 --> 00:38:52.490
rendering is paused on the preview.

00:38:52.490 --> 00:38:53.750
It shows you a frozen frame.

00:38:53.950 --> 00:38:58.400
And then when you setEnabled "yes,"
it goes back to its regular rendering,

00:38:58.400 --> 00:39:02.180
and it does not cause any glitch
in any other outputs or lose

00:39:02.180 --> 00:39:04.380
your focus or exposure points.

00:39:05.800 --> 00:39:09.530
Because we now have a
first-class connection on the

00:39:09.540 --> 00:39:12.610
AVCapture video preview layer,
it means we have some redundant

00:39:12.660 --> 00:39:15.840
API in video preview layer,
and we've taken this

00:39:15.840 --> 00:39:17.790
opportunity to deprecate them.

00:39:17.890 --> 00:39:22.400
You can see AVCaptureVideoPreviewLayer.h
for the ones that you

00:39:22.400 --> 00:39:23.790
should convert over to.

00:39:23.950 --> 00:39:25.200
Here's a list.

00:39:25.320 --> 00:39:27.800
The deprecated ones are calls
that you would make on the layer.

00:39:27.990 --> 00:39:29.800
Now you do the same thing,
except you get the

00:39:29.800 --> 00:39:32.830
layer's connection first,
and then set the same properties

00:39:32.830 --> 00:39:34.620
on the layer's connection.

00:39:36.420 --> 00:39:40.630
We have three miscellaneous APIs that
didn't fit into any of the big buckets

00:39:40.780 --> 00:39:42.620
that I was talking about earlier,
so I just thought I'd throw

00:39:42.620 --> 00:39:45.140
them all on one slide and
tell you about them quickly.

00:39:45.260 --> 00:39:48.040
AVCaptureDevice's TorchActive property.

00:39:48.180 --> 00:39:51.660
This lets you know whether
or not the torch can be used.

00:39:51.660 --> 00:39:56.100
As you know,
using the torch can make your device hot,

00:39:56.100 --> 00:39:59.240
and if it gets too hot,
the torch has to shut down

00:39:59.240 --> 00:40:02.560
and can't be used until the
unit cools off sufficiently.

00:40:02.680 --> 00:40:05.210
You can now use the TorchActive
property to know about the

00:40:05.290 --> 00:40:09.060
current availability of the torch,
whether it--and it's

00:40:09.260 --> 00:40:12.120
key value observable,
so it will change when

00:40:12.120 --> 00:40:14.630
the torch is now active.

00:40:15.090 --> 00:40:17.840
You can also set the torch
mode to something other

00:40:17.840 --> 00:40:20.690
than fully on or fully off.

00:40:20.980 --> 00:40:22.740
Some of you out here are
writing flashlight apps.

00:40:22.810 --> 00:40:23.890
I just know you are.

00:40:24.030 --> 00:40:27.280
If you are, you might want to set the
torch mode to something like

00:40:27.480 --> 00:40:29.880
halfway between full and zero.

00:40:30.000 --> 00:40:33.930
Or perhaps you might want to use it to
do interesting effects with your video.

00:40:34.170 --> 00:40:39.610
You can now set the torch level to
things other than top or bottom.

00:40:39.940 --> 00:40:42.390
Also,
AVCapture still image output has a nice

00:40:42.400 --> 00:40:45.430
new feature when recording JPEG images.

00:40:45.540 --> 00:40:48.280
Previously,
you had no control over the quality of

00:40:48.280 --> 00:40:50.800
the JPEG compression that was applied.

00:40:51.140 --> 00:40:53.500
It would always give
you 85 percent quality.

00:40:53.500 --> 00:40:57.740
We determined that that was the
right amount of compression to use.

00:40:57.740 --> 00:41:02.270
But now you can use the AVVideoQuality
key with the still image output and

00:41:02.270 --> 00:41:06.800
specify anything from zero to one,
zero being zero quality and

00:41:06.940 --> 00:41:09.940
one being 100 percent quality.

00:41:10.860 --> 00:41:15.190
All right, on to our next big topic area,
which is solutions for performance

00:41:15.280 --> 00:41:17.580
problems in your Capture app.

00:41:17.780 --> 00:41:21.000
We're going to talk about a couple
common problems that we see over

00:41:21.000 --> 00:41:23.190
and over on the developer forums.

00:41:23.310 --> 00:41:25.620
First of which is,
my app is dropping frames

00:41:25.920 --> 00:41:27.040
during video capture.

00:41:27.120 --> 00:41:30.950
And usually the second question is,
is it my fault?

00:41:31.060 --> 00:41:32.290
And we all know the answer to that.

00:41:32.370 --> 00:41:35.680
The answer is, yes,
of course it's your fault.

00:41:37.230 --> 00:41:39.600
It's never Apple's fault.

00:41:39.720 --> 00:41:41.600
What can I do to recover?

00:41:41.810 --> 00:41:45.100
That's the more important question is,
once I see it happening,

00:41:45.100 --> 00:41:46.940
what can I do about it?

00:41:47.000 --> 00:41:47.820
We're going to talk about that.

00:41:47.940 --> 00:41:50.930
Also,
for those of you who use AssetWriter,

00:41:51.010 --> 00:41:52.700
you may have noticed that
you have dropped frames at

00:41:52.700 --> 00:41:54.790
the beginning of your movie,
and we'll talk about

00:41:54.820 --> 00:41:56.220
how to mitigate that.

00:41:56.330 --> 00:41:59.650
Also, you may have noticed that you
have some garbage-looking frames

00:41:59.820 --> 00:42:04.150
in your AV AssetWriter-recorded
movies if you use GL for rendering.

00:42:04.160 --> 00:42:06.800
We'll talk about how to get around that.

00:42:06.930 --> 00:42:11.680
And many of you have a do-it-yourself
kind of preview where you get video data,

00:42:11.680 --> 00:42:14.020
you manipulate it,
and then you need to draw

00:42:14.020 --> 00:42:16.480
your own preview somehow,
and it's slow.

00:42:16.590 --> 00:42:17.780
What do you do about it?

00:42:17.820 --> 00:42:21.010
Okay, we're going to talk about
all those four things.

00:42:21.010 --> 00:42:23.370
First of all, handling frame drops.

00:42:23.820 --> 00:42:27.740
Always, always,
always set AVCapture video data outputs,

00:42:27.810 --> 00:42:31.040
always discards late video frames to yes.

00:42:31.160 --> 00:42:35.380
There is one tiny exception,
and that is if you are recording

00:42:35.790 --> 00:42:39.860
and you are super likely to
always be faster than real time.

00:42:39.860 --> 00:42:44.760
Because when you set always
discards late video frames to yes,

00:42:44.870 --> 00:42:49.860
what it tells the video data output to
do is to size its buffer queue to one at

00:42:49.860 --> 00:42:52.260
the end of the video processing pipeline.

00:42:52.430 --> 00:42:57.350
So that means if you don't pull a
frame on time because you were late,

00:42:57.460 --> 00:43:00.290
it will never get further
behind than the current frame.

00:43:00.300 --> 00:43:03.560
It will throw out the current frame,
append the new one.

00:43:03.700 --> 00:43:06.750
So in effect,
it's always giving you the latest.

00:43:06.830 --> 00:43:11.800
And it keeps you from building up latency
if you have a process that's taking too

00:43:11.800 --> 00:43:15.000
much time and it's doing so chronically.

00:43:15.100 --> 00:43:17.460
So it saves you from
periodically slow processing.

00:43:17.460 --> 00:43:21.120
If you just have a momentary glitch,
you're still going to get all the frames.

00:43:21.120 --> 00:43:23.700
It's just that when you had the glitch,
you might miss a few.

00:43:23.900 --> 00:43:26.440
What it does not save you from
is chronically slow processing.

00:43:26.440 --> 00:43:29.380
If you're always slower than real time,
you're still going to drop a ton

00:43:29.490 --> 00:43:35.180
of frames and you're still going
to have a bad user experience.

00:43:37.740 --> 00:43:40.620
Also,
we've provided a way in iOS 6 to help

00:43:40.620 --> 00:43:45.260
you debug your dropped capture frames,
and that is we've given you

00:43:45.260 --> 00:43:49.410
a new delegate method in your
video data output that's just

00:43:49.500 --> 00:43:53.430
like the didOutputSampleBuffer,
except it's didDropSampleBuffer.

00:43:53.570 --> 00:43:57.630
So you can know when you're
dropping sample buffers.

00:43:58.920 --> 00:44:00.900
and why might that be helpful?

00:44:00.900 --> 00:44:04.720
Well, the sample buffer that you dropped,
it contains no image data.

00:44:05.030 --> 00:44:05.790
You were late.

00:44:05.890 --> 00:44:07.860
The image data is no longer there.

00:44:07.860 --> 00:44:09.240
It's been recycled.

00:44:09.360 --> 00:44:10.830
You can't see it anymore.

00:44:10.980 --> 00:44:14.330
But the sample buffer does
contain timing information,

00:44:14.540 --> 00:44:17.950
format description, and most importantly,
it contains some attachments

00:44:18.050 --> 00:44:21.260
that you'll want to look at,
like the dropped frame reason.

00:44:21.470 --> 00:44:24.240
So that will tell you why
you're dropping frames.

00:44:24.380 --> 00:44:27.290
If it says the dropped
reason was frame was late,

00:44:27.440 --> 00:44:31.420
it's because you are using the
always discards late video frames,

00:44:31.530 --> 00:44:32.340
but you're still late.

00:44:32.540 --> 00:44:34.830
So that means it had a cue size of one.

00:44:35.130 --> 00:44:37.300
It had to pull one out
to put a new one in.

00:44:37.510 --> 00:44:40.970
That means you're operating
slower than real time.

00:44:41.540 --> 00:44:46.220
The out-of-buffers one,
that happens if you've been so late

00:44:46.530 --> 00:44:51.330
so many times and you're not using
the discards late video frames.

00:44:51.360 --> 00:44:53.830
So let's say you're in
a recording scenario,

00:44:53.840 --> 00:44:56.740
but you've dropped so many frames
now that the capture device

00:44:56.740 --> 00:44:58.460
has no more left to give you.

00:44:58.510 --> 00:45:01.540
And it just can't produce
anymore until you give some back.

00:45:01.600 --> 00:45:04.790
Out-of-buffers means you're way behind.

00:45:05.610 --> 00:45:09.040
And then a discontinuity means
something unexpected happened,

00:45:09.040 --> 00:45:14.170
the capture device had to reset itself,
maybe something crashed,

00:45:14.270 --> 00:45:19.830
but it just means that you're going to
have a--some number of frames were lost,

00:45:19.840 --> 00:45:24.290
and that the next video frame that comes
in will have a much later timestamp

00:45:24.290 --> 00:45:28.600
than the one that you are currently on,
more than one frame.

00:45:30.960 --> 00:45:32.940
So how can you mitigate these?

00:45:32.940 --> 00:45:35.600
Well, having this tool at your disposal,
knowing when you're

00:45:35.600 --> 00:45:36.920
dropping frames is huge.

00:45:36.920 --> 00:45:39.480
You can know when it's
happening and take action.

00:45:39.520 --> 00:45:42.570
The action that you take might be
that while you're developing your app,

00:45:42.730 --> 00:45:45.610
you pay attention to this and figure
out the devices on which you're

00:45:45.610 --> 00:45:49.590
dropping frames so that when you ship,
you've already tuned those devices

00:45:49.590 --> 00:45:52.360
to know what frame rate to use.

00:45:52.380 --> 00:45:55.860
But you can also use it as
a stop gap if in the field,

00:45:55.910 --> 00:45:58.940
like in real time, you find that you're
still dropping frames.

00:45:58.940 --> 00:46:03.830
One way to mitigate them is by
lowering the frame rate dynamically.

00:46:03.830 --> 00:46:08.400
You may not be aware that as of iOS 5,
you can lower the frame rate

00:46:08.420 --> 00:46:12.880
dynamically on the video data
output without stopping the session.

00:46:13.000 --> 00:46:19.020
and it will gradually and
neatly throttle down to the

00:46:19.110 --> 00:46:20.790
new frame rate that you've set.

00:46:21.090 --> 00:46:23.100
So by lowering the min
and max frame rate,

00:46:23.120 --> 00:46:26.720
it means you don't have to drop a
pile of frames to catch back up.

00:46:26.720 --> 00:46:29.490
You can let it do it gradually,
maybe over a second,

00:46:29.610 --> 00:46:34.720
and get up to a reasonable quality
of service where you can keep up.

00:46:34.740 --> 00:46:38.310
The way that you set min and
max frame rate is by setting

00:46:38.390 --> 00:46:41.290
the video min frame duration.

00:46:41.290 --> 00:46:44.120
Duration is one over frame rate.

00:46:44.120 --> 00:46:47.510
So you set the min frame duration
to set a new max frame rate,

00:46:47.670 --> 00:46:52.350
and you set the max frame duration
to set the min frame rate.

00:46:53.620 --> 00:46:55.830
Okay, asset writer frame drops.

00:46:56.130 --> 00:46:57.360
This one's a little bit stickier.

00:46:57.570 --> 00:46:59.760
And I told you that
it's always your fault,

00:46:59.800 --> 00:47:01.610
but in fact,
here it's kind of Apple's fault.

00:47:01.670 --> 00:47:04.520
So we're fixing it.

00:47:04.670 --> 00:47:08.600
AVCaptureMovieFileOutput is
really good at capturing movies.

00:47:08.600 --> 00:47:09.590
Why?

00:47:09.670 --> 00:47:12.600
Because it's optimized for
real-time file writing.

00:47:12.600 --> 00:47:16.590
It knows what format
you're going to produce.

00:47:16.600 --> 00:47:20.010
It pre-allocates buffers for
glitch-free movie writing,

00:47:20.160 --> 00:47:23.520
so it can start right away
without dropping any frames.

00:47:23.600 --> 00:47:28.550
AVAssetWriter, on the other hand,
is limited by its interface.

00:47:28.570 --> 00:47:33.590
It does not know the source format that
you're going to be providing up front.

00:47:33.730 --> 00:47:35.930
So when is it going to
do all of that setup,

00:47:35.930 --> 00:47:39.600
building up its render pipelines
and compressors and such?

00:47:39.690 --> 00:47:41.600
It can't prime that render
pipeline ahead of time.

00:47:41.600 --> 00:47:44.590
It has to do it when you append
your first sample buffer.

00:47:44.600 --> 00:47:46.600
Now it knows what the source format is.

00:47:46.600 --> 00:47:48.600
It knows how it can proceed.

00:47:48.600 --> 00:47:51.100
But that's too late,
because if you're appending

00:47:51.100 --> 00:47:52.570
the first sample buffer,
you meant for that one

00:47:52.570 --> 00:47:53.600
to go into the file.

00:47:53.600 --> 00:47:56.070
So now you're going to wind
up with some frame drops at

00:47:56.080 --> 00:47:57.600
the beginning of the movie.

00:47:58.530 --> 00:48:03.870
So we're getting by this by
giving you a new initializer

00:48:03.990 --> 00:48:06.540
for AV Asset Writer input.

00:48:06.590 --> 00:48:09.180
First of all,
continue to do what you have been doing,

00:48:09.220 --> 00:48:12.410
which is set expects media
data in real time to yes.

00:48:12.410 --> 00:48:16.690
And now new in iOS 6,
you can use a new init method to hint

00:48:16.740 --> 00:48:19.600
it the source format before you start.

00:48:19.710 --> 00:48:22.660
So the existing init
method looks like this,

00:48:22.710 --> 00:48:26.140
Asset Writer input with
media type output settings.

00:48:26.350 --> 00:48:29.170
And the new one just
adds one more parameter,

00:48:29.510 --> 00:48:30.700
source format hint.

00:48:32.810 --> 00:48:33.440
So use that.

00:48:33.700 --> 00:48:37.410
Use that if you're in the real-time
case because it lets you set up--it lets

00:48:37.460 --> 00:48:40.950
AssetWriter set up itself ahead of time.

00:48:41.110 --> 00:48:43.000
Now,
it doesn't mean that it's zero cost now.

00:48:43.000 --> 00:48:45.000
You still have to pay the cost somewhere.

00:48:45.140 --> 00:48:49.820
The costs of setup now move from
the first append sample buffer call

00:48:50.190 --> 00:48:53.000
to when you call start writing.

00:48:53.060 --> 00:48:57.380
So what that means is you should still
set up your asset writer off of the

00:48:57.380 --> 00:49:03.000
video data output serial queue on which
you're getting delegate callbacks.

00:49:03.000 --> 00:49:04.750
You don't want to do it there.

00:49:05.170 --> 00:49:07.000
That start writing might take
hundreds of milliseconds,

00:49:07.000 --> 00:49:11.000
which might cause frames to pile
up behind you or cause frame drops.

00:49:11.000 --> 00:49:15.000
So set up your asset writer
off of that serial queue,

00:49:15.000 --> 00:49:18.000
and then when you're actually
ready to start going,

00:49:18.000 --> 00:49:22.000
you can append sample buffers
in your delegate callback.

00:49:23.320 --> 00:49:25.710
All right,
rendering with OpenGL and writing to

00:49:25.720 --> 00:49:29.210
AVAssetWriter can be a little sticky.

00:49:29.540 --> 00:49:33.240
When you render to a texture
using CVOpenGL ES Texture Cache,

00:49:33.360 --> 00:49:36.930
you have to ensure that GL has
finished rendering the buffer

00:49:37.330 --> 00:49:39.700
before you pass it to AssetWriter.

00:49:39.820 --> 00:49:41.830
If you don't,
you're going to wind up with

00:49:41.830 --> 00:49:44.940
some garbage or lines in your
movie that you don't want.

00:49:45.060 --> 00:49:47.340
So the way to do that,
the safe way to make sure

00:49:47.340 --> 00:49:51.300
that GL has finished with the
frames is to call glFinish.

00:49:51.300 --> 00:49:54.690
It's safe, but unfortunately,
that's going to block the CPU and

00:49:54.690 --> 00:49:58.660
wait until the GPU is done rendering,
and you don't want that.

00:49:58.660 --> 00:50:02.110
You don't want to block your CPU so
that the GPU can do its work.

00:50:02.340 --> 00:50:04.960
You want both to be
busy at the same time.

00:50:05.290 --> 00:50:09.620
There's also glFlush,
which flushes out any pending commands

00:50:09.620 --> 00:50:12.720
to the GPU but doesn't finish them.

00:50:12.720 --> 00:50:16.780
So here's our recommendation to
get more out of the CPU and the

00:50:16.780 --> 00:50:20.890
GPU to get them working in parallel,
is to use what we call a delayed

00:50:20.980 --> 00:50:24.500
glFinish or staggered glRender
to keep both of them busy.

00:50:24.500 --> 00:50:26.100
Here's what it looks like.

00:50:26.100 --> 00:50:30.190
In your video data output,
You'll get a frame.

00:50:30.340 --> 00:50:35.230
Pass that frame to OpenGL to
render it and call glFlush.

00:50:35.380 --> 00:50:38.490
It's now working on frame N.

00:50:38.570 --> 00:50:40.100
Don't call glFinish.

00:50:40.200 --> 00:50:42.290
Just hold on to the frame.

00:50:43.470 --> 00:50:47.200
Now wait until you get frame N plus 1.

00:50:47.300 --> 00:50:49.530
Now call GLFinish for the first frame.

00:50:49.880 --> 00:50:53.690
That means you let the CPU do all
that work to get the second frame

00:50:53.690 --> 00:50:57.880
in before you called GLFinish,
and you gave GL some more time to

00:50:57.880 --> 00:51:00.670
work on rendering that first frame.

00:51:01.690 --> 00:51:05.100
Now you can pass the second frame to GL,
call GL flush,

00:51:05.270 --> 00:51:09.740
hold on to the second frame while you
pass the first frame to AssetWriter,

00:51:09.880 --> 00:51:13.370
and then wash, rinse, repeat.

00:51:16.730 --> 00:51:20.670
GL Flush is only necessary if
you are rendering for Asset

00:51:20.670 --> 00:51:22.280
Writer but not presenting it.

00:51:22.370 --> 00:51:25.360
If you do call present
render buffer for preview,

00:51:25.510 --> 00:51:28.600
it implicitly takes care
of the GL Flush for you.

00:51:28.690 --> 00:51:33.750
Also, that big staggered GL render that
I just took you through is only

00:51:33.750 --> 00:51:36.790
necessary on devices before iOS 6.

00:51:36.910 --> 00:51:39.830
iOS 6 and later,
Asset Writer takes care of this

00:51:40.230 --> 00:51:45.100
for you so that GL is always
guaranteed to be done with the buffer

00:51:45.370 --> 00:51:46.600
before it writes it to the movie.

00:51:46.610 --> 00:51:50.710
You will not get any more
of those garbage frames.

00:51:53.770 --> 00:51:55.680
Okay, how to draw my own preview fast.

00:51:55.780 --> 00:51:59.010
This is the last performance
problem we're going to talk about.

00:51:59.240 --> 00:52:02.530
Well,
the first way to do it and the easiest

00:52:03.040 --> 00:52:08.250
and the most efficient is to use our own
built-in video preview layer if you can.

00:52:08.340 --> 00:52:11.900
If all you're going to do is render
things on top of the video preview,

00:52:11.950 --> 00:52:14.010
such as StashCam did,
where it's drawing the

00:52:14.020 --> 00:52:19.470
mustaches on top of the video,
well, no need to be manipulating pixels.

00:52:19.470 --> 00:52:23.890
Just use our video preview layer
and use Core Animation to composite

00:52:23.890 --> 00:52:25.260
other images on top of it.

00:52:25.520 --> 00:52:27.780
That's as easy as can be
and it's very efficient.

00:52:28.000 --> 00:52:32.810
Plus, you get all of Core Animation's
rotations and animations for free.

00:52:34.200 --> 00:52:36.860
If you are using video data output
because you need to manipulate

00:52:36.920 --> 00:52:41.340
the pixels before previewing,
our recommendation is to use OpenGL.

00:52:41.500 --> 00:52:45.300
We found that it is consistently the
fastest way to draw your own preview.

00:52:45.410 --> 00:52:49.670
It's not necessarily as easy,
but we've given you some good

00:52:49.670 --> 00:52:51.160
sample code to show you how.

00:52:51.290 --> 00:52:53.890
There's the GL Camera Ripple sample code.

00:52:54.010 --> 00:52:56.240
It operates in 4:2:0V.

00:52:56.350 --> 00:53:00.000
If at all possible,
use 4:2:0V for your preview.

00:53:00.000 --> 00:53:02.300
And if you can manipulate
your pixels in 4:2:0V,

00:53:02.300 --> 00:53:03.240
please do.

00:53:03.330 --> 00:53:06.090
Because it's less than half
of the data size of BGRA.

00:53:06.340 --> 00:53:10.580
It's the native format of the camera,
and it just makes things faster

00:53:10.580 --> 00:53:12.770
and more memory efficient.

00:53:12.900 --> 00:53:17.240
If, however, you need to use BGRA for
output and for preview,

00:53:17.350 --> 00:53:19.140
please review the
Rosie Writer sample code.

00:53:19.140 --> 00:53:23.740
It shows how to do the same
things with operations in BGRA.

00:53:23.860 --> 00:53:27.180
On to our last topic,
synchronizing motion data with video.

00:53:27.180 --> 00:53:29.010
This is a fun one.

00:53:30.440 --> 00:53:33.140
Okay, let's start out by having
a demo of VideoSnake.

00:53:33.140 --> 00:53:37.210
I'd like to call up Walker Eagleston
to help me demo this.

00:53:41.500 --> 00:53:45.480
It has a nifty icon, too.

00:53:45.510 --> 00:53:46.540
There we go.

00:53:46.820 --> 00:53:51.460
Okay, it's black,
so you can't see the whole screen frame.

00:53:51.510 --> 00:53:57.610
But what's happening here is it's
using AV Foundation plus Core Motion,

00:53:57.770 --> 00:54:00.380
the accelerometer, in this case,
to draw a video snake.

00:54:00.520 --> 00:54:02.200
The name of this app is Video Snake.

00:54:02.340 --> 00:54:04.330
So what you see, I'm the head.

00:54:04.360 --> 00:54:05.780
I am the head of the snake.

00:54:05.850 --> 00:54:08.830
And behind you -- behind me,
undulating around,

00:54:08.870 --> 00:54:12.000
you see a motion history
of where I've been.

00:54:12.050 --> 00:54:15.780
And we're using Core Motion for
the accelerometer only here.

00:54:15.930 --> 00:54:20.770
So as Walker translates the iPad around,
so moves it left, right, top,

00:54:20.830 --> 00:54:24.970
up and down, you'll see the tail of
the snake undulate around.

00:54:25.120 --> 00:54:27.750
There's also a second
effect built into this app,

00:54:27.790 --> 00:54:30.090
which is more of a collage effect.

00:54:30.190 --> 00:54:32.140
So he's demoing that to you now.

00:54:32.210 --> 00:54:36.230
This is sort of a poor man's pano,
I guess.

00:54:36.460 --> 00:54:38.980
As he rolls around,
this takes into account not

00:54:38.980 --> 00:54:41.380
only translation but rotation.

00:54:41.400 --> 00:54:46.480
So as he kind of moves the iPad
up and down and rotates it around,

00:54:46.480 --> 00:54:50.720
it will keep track of where
I am and draw the history around

00:54:50.720 --> 00:54:55.170
me in more of a 3-D space.

00:54:55.180 --> 00:55:00.550
And so what's neat about this is that
it knows the camera's field of view by

00:55:00.920 --> 00:55:05.740
using the sample buffer attachment's
focal length in 35-millimeter film.

00:55:05.740 --> 00:55:09.730
And that lets him guess how much --
when to stop to snap a new picture.

00:55:09.980 --> 00:55:13.040
So every 10 degrees or so,
he's snapping a new picture

00:55:13.040 --> 00:55:15.180
and using GL to render it.

00:55:15.180 --> 00:55:18.240
And what you wind up with is
this cool kind of collage effect.

00:55:18.240 --> 00:55:21.070
None of it works, though,
if we can't correlate those video

00:55:21.180 --> 00:55:25.100
timestamps to the timestamps that
we're getting from Core Motion.

00:55:25.100 --> 00:55:26.710
Thank you, Walker.

00:55:31.760 --> 00:55:34.690
All right, so how did we do that?

00:55:34.810 --> 00:55:38.490
You couldn't see it because
how would you see audio?

00:55:38.740 --> 00:55:40.800
But there was a big record
button at the bottom,

00:55:40.800 --> 00:55:45.070
which we were tempted to press,
but we didn't because recording is cool,

00:55:45.070 --> 00:55:48.960
but it's better seen in
code than demoed on stage.

00:55:48.960 --> 00:55:51.360
VideoSnake is doing both video and audio.

00:55:51.360 --> 00:55:54.920
So at the top of his AVCapture session,
he has both a video

00:55:54.920 --> 00:55:57.300
device and a microphone.

00:55:57.300 --> 00:56:01.900
He has a video data output
and an audio data output.

00:56:01.900 --> 00:56:01.900
And he's

00:56:03.490 --> 00:56:07.310
And to correlate the
timestamps with core motion,

00:56:07.310 --> 00:56:11.360
he's taking the video frames that
he gets and he's undoing AV Sync.

00:56:11.360 --> 00:56:15.750
We'll talk about that in a minute.

00:56:16.430 --> 00:56:17.290
That's wrong.

00:56:17.380 --> 00:56:18.660
He's not passing to a CI face detector.

00:56:18.740 --> 00:56:21.930
That should say passing to core motion.

00:56:22.540 --> 00:56:27.510
And then passing the core motion
and getting the timestamps,

00:56:27.510 --> 00:56:30.390
it's then rendering with OpenGL.

00:56:30.930 --> 00:56:34.500
And the rendered frames,
that is the composited snake,

00:56:34.500 --> 00:56:37.300
is now previewed using GL.

00:56:37.410 --> 00:56:42.680
And the results are brought back into
main memory where they can be written

00:56:42.680 --> 00:56:47.870
to AssetWriter and correlated with the
audio that came in at the same time.

00:56:48.020 --> 00:56:49.960
The audio and video is in sync.

00:56:50.020 --> 00:56:54.040
This sample was not on the
developer site as of this morning,

00:56:54.040 --> 00:56:57.100
but I think it will be
there this afternoon.

00:56:58.290 --> 00:57:00.710
All right, so how do you synchronize
motion data with video?

00:57:00.990 --> 00:57:03.940
The problem is we've got
three input sources now.

00:57:03.940 --> 00:57:08.180
We've got the gyro core motion,
we've got a video device,

00:57:08.180 --> 00:57:09.970
we've got an audio device.

00:57:09.970 --> 00:57:12.430
That means potentially three clocks.

00:57:12.430 --> 00:57:15.420
And you know the old saying,
"The man with two clocks never

00:57:15.420 --> 00:57:18.170
knows what time it is." Well,
we really don't know what

00:57:18.190 --> 00:57:21.030
time it is because we've got
three clocks to deal with.

00:57:21.030 --> 00:57:25.390
But we're lucky because, in fact,
there are only two clocks to deal with,

00:57:25.390 --> 00:57:29.320
because both the video camera and
core motion use the same clock.

00:57:29.380 --> 00:57:30.780
It's the host time clock.

00:57:30.830 --> 00:57:34.360
So now we only have to worry about the
audio being on a different clock and

00:57:34.520 --> 00:57:36.200
thus a slightly different timeline.

00:57:38.250 --> 00:57:41.850
To synchronize motion data with video,
first you have to know

00:57:41.850 --> 00:57:43.890
about the motion samples.

00:57:44.090 --> 00:57:47.000
The Core Motion samples
contain a timestamp.

00:57:47.130 --> 00:57:51.100
You can get at them by using
CMDeviceMotion timestamp,

00:57:51.100 --> 00:57:53.140
and that'll give you an NSTime interval.

00:57:53.250 --> 00:57:56.300
It's timestamped using the
Mach absolute time of the motion,

00:57:56.320 --> 00:58:00.060
so the exact time that the
accelerometer event happened

00:58:00.060 --> 00:58:03.420
or the gyro event happened,
that's what you're getting when

00:58:03.420 --> 00:58:05.600
you ask it for the timestamp.

00:58:05.710 --> 00:58:10.750
And Core Motion does use the host
time clock to do its timestamping.

00:58:11.440 --> 00:58:15.640
When you do tricks like this where you're
using motion and you're using video,

00:58:15.750 --> 00:58:19.820
you'll typically want more motion samples
than you have video samples so that you

00:58:19.880 --> 00:58:23.700
can compare motion that was happening
around the time of the video frame.

00:58:23.910 --> 00:58:26.520
Because typically people can
move around faster than the

00:58:26.610 --> 00:58:28.060
video can spit out frames.

00:58:28.210 --> 00:58:32.900
So we recommend at least a sampling rate
of 2x your video frame rate if you're

00:58:32.900 --> 00:58:35.490
going to do effects with Core Motion.

00:58:36.010 --> 00:58:37.490
Now, on to the video timestamps.

00:58:37.500 --> 00:58:38.860
How do you get those?

00:58:38.990 --> 00:58:42.700
Each sample buffer of video that you
get has a timestamp that you can get

00:58:42.700 --> 00:58:47.750
using CMSampleBufferGetPresentationTime,
and that is the Mach absolute

00:58:47.750 --> 00:58:48.960
time of the frame.

00:58:49.140 --> 00:58:53.560
Front and back camera on iOS
both use the Mach absolute

00:58:53.560 --> 00:58:56.120
time or the host time clock.

00:58:56.710 --> 00:58:58.840
For the audio, though,
it doesn't just contain

00:58:58.940 --> 00:59:00.040
one frame of audio.

00:59:00.040 --> 00:59:01.830
It contains N frames of audio.

00:59:01.890 --> 00:59:05.920
And the time stamp there is the time
of the first frame in the buffer.

00:59:06.130 --> 00:59:08.040
And, again, it's on a different clock.

00:59:08.110 --> 00:59:10.700
But it's the time that it
came into the microphone.

00:59:10.850 --> 00:59:13.420
The audio capture device
uses the audio clock.

00:59:13.480 --> 00:59:16.770
And to do AV sync, then,
we have to pick one or the other

00:59:16.770 --> 00:59:20.200
because otherwise they would drift
apart and you would have movie

00:59:20.200 --> 00:59:23.090
recordings that were out of sync.

00:59:23.100 --> 00:59:25.020
So because they might drift,
we have to pick one.

00:59:25.020 --> 00:59:27.860
Well, we picked the audio clock
because the audio clock,

00:59:27.920 --> 00:59:30.850
it's easier to sync video
to audio than vice versa.

00:59:30.860 --> 00:59:32.710
So audio becomes the master.

00:59:32.710 --> 00:59:37.060
That means that the video stamps that
you get on the video sample buffers when

00:59:37.160 --> 00:59:39.250
you are also capturing audio are altered.

00:59:39.250 --> 00:59:42.250
They're on the audio timeline,
not on the video timeline.

00:59:42.390 --> 00:59:45.160
That's no good if you want to
correlate them with motion,

00:59:45.160 --> 00:59:47.740
which is on the host time clock.

00:59:47.740 --> 00:59:51.700
So the way you undo AV sync is
by getting a reference to the

00:59:51.700 --> 00:59:55.560
audio clock and the video clock,
as seen here.

00:59:55.560 --> 00:59:58.210
And then taking your presentation
time from the video buffer,

00:59:58.210 --> 01:00:02.560
which, again, is on the audio timeline,
and call CM sync convert time,

01:00:02.780 --> 01:00:07.140
which converts from the audio timeline
to the video clock's timeline.

01:00:07.140 --> 01:00:10.060
So now you're back on
the core motion timeline.

01:00:10.060 --> 01:00:14.280
You can now do your correlating of times.

01:00:14.430 --> 01:00:16.540
So in summary,
what did we talk about today?

01:00:16.640 --> 01:00:19.980
We talked about OS X features,
mostly performance.

01:00:20.140 --> 01:00:25.040
We were really centered on getting
the performance better in OS X.8.

01:00:25.040 --> 01:00:26.390
8.

01:00:27.110 --> 01:00:29.320
We also talked about camera
ecosystem as a whole.

01:00:29.630 --> 01:00:32.840
Be sensitive to our users' private data.

01:00:32.960 --> 01:00:35.190
Don't violate their trust.

01:00:35.580 --> 01:00:40.100
We also spent a lot of time talking
about iOS 6 capture features,

01:00:40.260 --> 01:00:44.470
solving performance problems, and lastly,
we talked about correlating

01:00:44.470 --> 01:00:46.240
motion data with video.

01:00:46.310 --> 01:00:48.160
Hope you've enjoyed our session today.

01:00:48.160 --> 01:00:50.740
For more information,
please talk to Eric Verschen,

01:00:50.740 --> 01:00:51.920
our evangelist.

01:00:52.130 --> 01:00:53.620
Thanks again,
and enjoy the rest of the show.