WEBVTT

00:00:10.690 --> 00:00:12.100
Good morning, everyone.

00:00:12.170 --> 00:00:15.200
Welcome to the Accelerate
Framework session.

00:00:15.310 --> 00:00:16.350
My name is Luke Chang.

00:00:16.350 --> 00:00:18.720
I'm an engineer in Vector
and Numerics group.

00:00:18.960 --> 00:00:21.730
I'm here today to talk
about Accelerate Framework.

00:00:22.000 --> 00:00:27.110
Have you ever thought about making an
image processing app or audio equalizer

00:00:27.540 --> 00:00:30.460
app or any computation-intensive apps?

00:00:30.660 --> 00:00:33.460
If you have, you're in the right place.

00:00:33.690 --> 00:00:37.760
Accelerate Framework can help
you make those apps and more.

00:00:38.010 --> 00:00:40.420
That's why we like to call it

00:00:41.110 --> 00:00:46.370
Your one-stop shopping for fast
and energy efficient libraries.

00:00:46.700 --> 00:00:49.430
We call it one-stop
shopping for a reason.

00:00:49.540 --> 00:00:52.310
In the Accelerate framework,
we support a wide range of

00:00:52.310 --> 00:00:55.040
computational functionalities.

00:00:55.360 --> 00:00:58.850
We have signal processing, VDSP.

00:00:58.850 --> 00:01:01.540
We have image processing, Vimage.

00:01:01.600 --> 00:01:04.750
Linear algebra, we have LAPAC and BLAS.

00:01:04.860 --> 00:01:09.170
Transcendental math functions,
we have VForce and VMathlib.

00:01:10.380 --> 00:01:15.700
So this is really a wide range
of computational functionalities.

00:01:15.700 --> 00:01:19.400
In this session,
I'd like to talk about three things.

00:01:19.400 --> 00:01:23.540
First,
why you want to use Accelerate Framework.

00:01:23.570 --> 00:01:27.760
Second, where in your code you could
use Accelerate Framework

00:01:27.760 --> 00:01:30.020
will help you recognize that.

00:01:30.020 --> 00:01:34.540
And lastly, we're going to show you how
to use Accelerate Framework.

00:01:34.590 --> 00:01:37.300
So let's get started on the first one.

00:01:37.530 --> 00:01:42.400
You want to use Accelerate framework
because it can help you make great apps.

00:01:42.530 --> 00:01:47.190
In my experience,
a great app needs a few things.

00:01:48.330 --> 00:01:50.640
First, it has to be useful.

00:01:50.750 --> 00:01:52.910
It does something I need,
so I will go ahead,

00:01:52.930 --> 00:01:56.360
download it from the App Store,
give it a try.

00:01:56.950 --> 00:02:00.210
After I downloaded it, it has to work.

00:02:00.310 --> 00:02:05.950
I don't want my app to crash every five
minutes or give me the wrong results.

00:02:06.380 --> 00:02:09.800
Those are just the two basic
requirements for a good app.

00:02:10.120 --> 00:02:12.000
It's not a great app yet.

00:02:12.180 --> 00:02:16.300
To make it a great app,
I want my app to be responsive.

00:02:16.420 --> 00:02:21.640
Responsive to my every touch,
every swipe, or every pinch.

00:02:21.760 --> 00:02:24.600
A slow app is not a good app.

00:02:25.510 --> 00:02:29.740
And last, I want my app to have
a long battery life.

00:02:29.880 --> 00:02:34.300
If the app is draining the battery a lot,
I probably would not use it when

00:02:34.330 --> 00:02:36.200
I don't have my charger around.

00:02:36.310 --> 00:02:39.030
That would significantly
cut down my usage time.

00:02:39.150 --> 00:02:40.900
That's not good.

00:02:41.050 --> 00:02:44.250
So, these are the things the users want.

00:02:44.480 --> 00:02:45.690
Now, how about you?

00:02:45.780 --> 00:02:48.860
As a developer, what do you want?

00:02:48.960 --> 00:02:51.730
Well, I am a developer too.

00:02:51.880 --> 00:02:54.900
Here are the things
that I want for my code.

00:02:56.140 --> 00:03:00.480
I want my code easy to write so
I can finish my app sooner and

00:03:00.570 --> 00:03:03.970
put my app on the App Store so
people can start using it.

00:03:04.430 --> 00:03:08.310
After I finish the app,
I want my code to be readable.

00:03:08.310 --> 00:03:11.840
So when I hand it over to the
next developer for maintenance,

00:03:11.970 --> 00:03:13.870
they won't keep coming
back asking questions.

00:03:13.950 --> 00:03:18.500
I could focus my time and
energy on the next great app.

00:03:19.560 --> 00:03:24.010
And if there's ever a need
to port from iOS to Mac OS X,

00:03:24.010 --> 00:03:28.410
like many of the games do now,
they support both iOS and Mac OS X,

00:03:28.580 --> 00:03:32.340
I want the same code to
just work on both platforms.

00:03:32.470 --> 00:03:38.040
All I need to do is just rebuild
the project for iOS or for Mac OS X,

00:03:38.040 --> 00:03:39.590
and that's it.

00:03:39.960 --> 00:03:46.390
So, now, how does Accelerate Framework
help you achieve these goals?

00:03:47.030 --> 00:03:51.150
Well, Accelerate Framework has
more than 2,000 APIs,

00:03:51.210 --> 00:03:55.080
so it's more than likely that something
you're trying to do will already

00:03:55.080 --> 00:03:57.400
support that in Accelerate Framework.

00:03:57.510 --> 00:04:01.280
So you don't have to write your own,
and using Accelerate is very easy,

00:04:01.390 --> 00:04:04.300
so your code is going
to be easy to write.

00:04:05.660 --> 00:04:09.100
Accelerate Framework is
well tested and accurate.

00:04:09.180 --> 00:04:11.560
So if your apps are using
Accelerate Framework,

00:04:11.610 --> 00:04:16.130
your result will be accurate and your
app is going to be robust as well.

00:04:17.390 --> 00:04:22.300
Accelerate Framework, by its name,
is fast and energy efficient.

00:04:22.420 --> 00:04:25.670
So your app is going to be responsive,
it can do more work in

00:04:25.810 --> 00:04:30.670
a short amount of time,
and you won't drain on the battery.

00:04:31.510 --> 00:04:37.560
Lastly, Accelerate Framework is
available on both iOS and OS X.

00:04:37.680 --> 00:04:42.980
So the same code will just work if
you want to port from iOS to OS X.

00:04:43.410 --> 00:04:47.860
So, these are the reasons that
a lot of people are already

00:04:47.870 --> 00:04:49.700
using Accelerate Framework.

00:04:49.700 --> 00:04:52.540
A lot of great apps are
using Accelerate Framework.

00:04:52.680 --> 00:04:55.850
In fact, we did a survey recently
on the Mac App Store.

00:04:56.160 --> 00:04:58.110
Here's what we found.

00:04:59.820 --> 00:05:05.500
Nine of the ten top-grossing apps
in the Mac App Store use Accelerate.

00:05:05.660 --> 00:05:10.310
So, again, a lot of people are using it,
great apps are using it,

00:05:10.310 --> 00:05:11.980
and so should you.

00:05:12.490 --> 00:05:16.620
Well, I'm going to let you in on a little
secret on how we made Accelerate

00:05:16.620 --> 00:05:19.560
framework fast and energy efficient.

00:05:21.500 --> 00:05:24.600
We use SIMD instructions
to optimize our code.

00:05:24.680 --> 00:05:27.500
SIMD stands for single instruction,
multiple data.

00:05:27.660 --> 00:05:32.280
Let's say you have four floating-point
numbers you want to add to another four.

00:05:32.650 --> 00:05:34.310
You can use SIMD instructions.

00:05:34.420 --> 00:05:38.760
In one instruction, you can do all four
arithmetic calculations.

00:05:38.880 --> 00:05:44.060
On Intel, we take advantage of SSE and
the latest technology,

00:05:44.100 --> 00:05:44.800
AVX.

00:05:44.900 --> 00:05:48.910
On ARM, we use the NEON instruction set.

00:05:49.540 --> 00:05:55.400
And we also write hand-tuned assembly
for each processor Apple supports.

00:05:55.540 --> 00:05:59.040
So we carefully schedule
our assembly instruction,

00:05:59.040 --> 00:06:02.640
and we do software
pipelining or loop unrolling,

00:06:02.640 --> 00:06:08.010
sometimes both,
to make sure the maximum parallelism in

00:06:08.010 --> 00:06:13.070
your code is exposed to the processor,
so the processor can execute more

00:06:13.070 --> 00:06:13.070
than one instruction at a time.

00:06:13.660 --> 00:06:17.500
And lastly,
we multithreaded our code using GCD.

00:06:17.640 --> 00:06:23.080
So if your system has multiple cores,
which most of the Apple devices do now,

00:06:23.210 --> 00:06:26.120
you'll be using all of the
core on the system to achieve

00:06:26.120 --> 00:06:28.310
the maximum performance.

00:06:29.980 --> 00:06:33.360
Here's how you link against
Accelerate framework in Xcode.

00:06:33.410 --> 00:06:35.080
It's very simple.

00:06:35.130 --> 00:06:40.200
You click on the
Build Faces tab right here.

00:06:41.200 --> 00:06:45.800
And click on the Add button,
the small Add button,

00:06:45.800 --> 00:06:49.340
right after the library list.

00:06:49.490 --> 00:06:54.240
Esco is going to show you a list of
frameworks that you can link against.

00:06:54.350 --> 00:06:56.860
On the top of the list,
there's Accelerate framework.

00:06:57.140 --> 00:07:00.550
So let's select that and click Add.

00:07:01.260 --> 00:07:05.540
Now, your project is linking
against Accelerate Framework.

00:07:05.610 --> 00:07:09.390
In order to use the API in
Accelerate Framework,

00:07:09.940 --> 00:07:14.640
All you need to do is just include
Accelerate header in your source file.

00:07:14.640 --> 00:07:16.680
Then you can start using it.

00:07:17.740 --> 00:07:21.340
If you're feeling more adventurous,
you want to do it in command line,

00:07:21.340 --> 00:07:22.750
it's just as easy.

00:07:23.850 --> 00:07:28.140
You pass -framework Accelerate
option to your compiler,

00:07:28.140 --> 00:07:29.150
and that's it.

00:07:29.250 --> 00:07:32.570
Your code will be linking
against Accelerate framework

00:07:32.570 --> 00:07:34.670
and get the best performance.

00:07:35.690 --> 00:07:39.110
Now you know how to use
Accelerate framework,

00:07:39.110 --> 00:07:44.150
I'm going to present an FFT case study
that we did recently to show you exactly

00:07:44.160 --> 00:07:47.430
why you want to use Accelerate framework.

00:07:48.930 --> 00:07:52.010
FFT stands for Fast Fourier Transform.

00:07:52.210 --> 00:07:56.600
It is one of the most important signal
processing operations out there.

00:07:56.670 --> 00:08:00.500
It's used in audio compression,
it's used in image processing,

00:08:00.600 --> 00:08:01.870
even used in video.

00:08:02.120 --> 00:08:03.900
Pretty much everywhere.

00:08:04.010 --> 00:08:09.010
So if you don't know how to write an FFT,
you could use the one in Accelerate

00:08:09.010 --> 00:08:13.900
Framework or pick up this popular book,
Numerical Recipes in C.

00:08:13.900 --> 00:08:19.870
It has the algorithm and
implantation on how to implement FFT.

00:08:20.510 --> 00:08:24.900
So we're going to compare
these two on two metrics.

00:08:25.050 --> 00:08:28.230
First one is execution time.

00:08:28.340 --> 00:08:30.370
Execution time is easy to understand.

00:08:30.630 --> 00:08:33.040
You want your code to
run as fast as possible,

00:08:33.200 --> 00:08:38.190
so the execution time,
you want it to be as short as possible.

00:08:38.890 --> 00:08:42.970
And then we want to compare the
energy consumption of using Accelerate

00:08:42.970 --> 00:08:46.350
Framework and numerical recipes in C.

00:08:46.860 --> 00:08:52.240
I'd like to spend more time on energy
consumption to talk about what is

00:08:52.370 --> 00:08:55.050
the energy consumption of a function.

00:08:55.480 --> 00:08:58.460
Because there is a common
misunderstanding out there that

00:08:59.120 --> 00:09:03.780
people think power consumption is
the same as energy consumption.

00:09:03.980 --> 00:09:05.700
But it's not true.

00:09:06.260 --> 00:09:09.800
In terms of battery life,
what you care about is really

00:09:10.060 --> 00:09:13.010
the energy consumption,
not the power.

00:09:13.260 --> 00:09:15.760
A battery can hold a
certain amount of energy,

00:09:15.880 --> 00:09:21.190
so if your app is more energy efficient,
the battery life is going to be longer.

00:09:21.430 --> 00:09:26.990
So let me put the relation between
energy and power into equation.

00:09:27.980 --> 00:09:32.910
Energy is an integral of
instantaneous power over time.

00:09:33.470 --> 00:09:38.140
In reality, we cannot get the continuous
measurement of instantaneous power.

00:09:38.290 --> 00:09:45.110
What we do is use the piecewise
summation to approximate the integral.

00:09:45.780 --> 00:09:49.080
The equation looks complicated,
but it's actually very simple.

00:09:49.270 --> 00:09:51.970
Let me put this into a graph.

00:09:53.550 --> 00:09:58.500
This is a typical energy consumption
profile that we measure on the system.

00:09:58.590 --> 00:10:04.040
So at the beginning, system is idle,
so the system consuming the idle power,

00:10:04.040 --> 00:10:04.760
P0.

00:10:04.830 --> 00:10:08.990
At time T0, the workload comes in,
in this case the FFT.

00:10:09.360 --> 00:10:13.100
The system will start doing something,
the process is processing those data.

00:10:13.170 --> 00:10:16.410
The instantaneous power jump to P1.

00:10:17.120 --> 00:10:24.160
And at time T1, the workload finished,
system come back to the idle power P0.

00:10:24.300 --> 00:10:28.300
There are two things that you need
to pay attention to in this graph.

00:10:28.480 --> 00:10:33.450
First one is the time
difference between T0 and T1.

00:10:33.600 --> 00:10:35.150
That's your execution time.

00:10:35.260 --> 00:10:38.310
You want it to be as short as possible.

00:10:38.820 --> 00:10:44.840
The second thing is the area
under the curve between T0 and T1.

00:10:44.840 --> 00:10:48.140
That is the energy
consumption of this function.

00:10:48.300 --> 00:10:51.490
You want it as less as possible.

00:10:51.870 --> 00:10:54.800
So now we know what we're comparing.

00:10:54.800 --> 00:10:58.990
We use the metrics to compare these two,
the Accelerate framework

00:10:58.990 --> 00:11:01.290
FFT and numerical recipes in C.

00:11:01.650 --> 00:11:02.900
Let's look at the competition.

00:11:03.090 --> 00:11:06.930
Our competition is numerical with b, c,
and c.

00:11:07.190 --> 00:11:11.960
We asked an average programmer to
write an FFT based on the book,

00:11:11.960 --> 00:11:15.550
so he did a straight from
the book implementation.

00:11:15.780 --> 00:11:19.960
It's about 50 lines of code,
and it looked like this.

00:11:21.360 --> 00:11:24.200
So this code is not
terribly difficult to write.

00:11:24.210 --> 00:11:28.640
I know everyone in this room is
capable of writing one of your own.

00:11:28.730 --> 00:11:31.540
But there are a couple of things
you need to pay attention to.

00:11:31.670 --> 00:11:36.590
First, you have to be careful about the
index and where you want to use add,

00:11:36.590 --> 00:11:38.960
where you want to use subtract.

00:11:39.110 --> 00:11:43.170
After you get all the signs
and the details right,

00:11:43.220 --> 00:11:45.170
you're not done yet.

00:11:45.800 --> 00:11:48.250
You still need to test for accuracy.

00:11:48.360 --> 00:11:53.370
You want to make sure the error
tolerance is within your app's range.

00:11:53.630 --> 00:11:56.600
And you want to measure for performance,
because if you're

00:11:56.600 --> 00:11:59.440
writing a real-time app,
you don't want your app to lose

00:11:59.450 --> 00:12:03.610
audio samples or lose video frames.

00:12:03.930 --> 00:12:07.230
And last, you need to document your code.

00:12:07.340 --> 00:12:09.750
You have to document where
you took the algorithm from

00:12:09.750 --> 00:12:12.370
and how you implemented it.

00:12:13.480 --> 00:12:19.280
In our opinion,
all this is just too much work.

00:12:19.570 --> 00:12:26.050
You could use the time to focus on your
next project or next big feature and

00:12:26.140 --> 00:12:30.100
save the time by using Accelerate FFT.

00:12:30.290 --> 00:12:33.630
So here is how you use Accelerate FFT.

00:12:33.750 --> 00:12:37.890
There are three simple steps
for you to use Accelerate FFT:

00:12:38.140 --> 00:12:42.280
Setup, Operate, and Destroy.

00:12:42.760 --> 00:12:46.080
So, of course,
you have to include Accelerate

00:12:46.080 --> 00:12:51.110
header to have access to all
Accelerate framework APIs.

00:12:51.640 --> 00:12:56.230
Then you pass in the
data and the FFT length.

00:12:56.280 --> 00:12:59.580
FFT length is represented
in terms of log2.

00:12:59.640 --> 00:13:01.890
So you can see the log2n equal to 10.

00:13:02.120 --> 00:13:06.530
That means we're trying
to do a 1024-point FFT.

00:13:07.500 --> 00:13:11.490
So once at the start,
before you process any of your data,

00:13:11.610 --> 00:13:14.950
you call FFT Create Setup.

00:13:15.660 --> 00:13:19.550
You'll pass in the FFT length
and the Radix information.

00:13:19.830 --> 00:13:22.410
Here we have Radix 2.

00:13:23.490 --> 00:13:26.420
And then you go on to
operate on your data.

00:13:26.640 --> 00:13:28.400
You call FFT-ZIP.

00:13:28.520 --> 00:13:33.320
It will do an in-place
complex-to-complex FFT.

00:13:33.420 --> 00:13:37.020
You pass in the setup structure,
where the data is,

00:13:37.090 --> 00:13:41.770
and this one on the third
argument is telling FFT that my

00:13:41.770 --> 00:13:44.120
data is in contiguous memory.

00:13:44.230 --> 00:13:47.310
So you access one after another.

00:13:47.420 --> 00:13:52.050
And the length information,
and we want to do a forward FFT.

00:13:52.160 --> 00:13:55.600
You can call FFT-ZIP multiple
times to handle all your data

00:13:55.600 --> 00:13:57.740
with the same setup structure.

00:13:57.740 --> 00:14:01.640
You can reuse the same setup structure,
and only once at the end when

00:14:01.640 --> 00:14:06.500
you're done with all your data,
you call Destroy FFT Setup.

00:14:06.590 --> 00:14:11.140
This is to reclaim the memory that's
allocated to the setup structure

00:14:11.140 --> 00:14:13.250
so you can avoid memory leak.

00:14:14.230 --> 00:14:18.320
So,
using Accelerate FFT is really simple.

00:14:18.710 --> 00:14:20.710
Let's look at the
performance of these two,

00:14:20.880 --> 00:14:24.650
Accelerate FFT and
Numerical Recipes in C.

00:14:26.290 --> 00:14:31.640
Here is the energy consumption
profile for numerical recipes in C.

00:14:31.640 --> 00:14:34.700
And here is the Accelerate FFT.

00:14:34.760 --> 00:14:39.700
As you can see, the execution time for
Accelerate FFT is much,

00:14:39.700 --> 00:14:43.470
much shorter than numerical recipes in C.

00:14:44.290 --> 00:14:49.000
Even though the instantaneous
power of Accelerate FFT is more

00:14:49.000 --> 00:14:53.800
than numerical recipes in C,
but since the time is so much shorter,

00:14:53.970 --> 00:14:58.180
the total area under the curve,
which is the energy consumption,

00:14:58.260 --> 00:15:01.470
is much less than numerical recipes in C.

00:15:02.620 --> 00:15:05.860
Let me put this into perspective for you.

00:15:05.990 --> 00:15:09.530
Let's normalize the performance
and energy consumption of

00:15:09.640 --> 00:15:13.500
numerical recipes in C to 1.

00:15:13.780 --> 00:15:17.090
Here's what Accelerate FFT looks like.

00:15:20.980 --> 00:15:27.670
Accelerate FFT is more than nine times
faster than numerical recipes in C,

00:15:27.670 --> 00:15:32.340
while it only consumes one-eighth of
the energy of numerical recipes in C.

00:15:32.520 --> 00:15:37.240
So you can do more work
while consuming less power.

00:15:37.340 --> 00:15:41.890
This is exactly why you want
to use Accelerate Framework.

00:15:42.610 --> 00:15:45.380
Now, enough with the hard-code data.

00:15:45.690 --> 00:15:50.170
Let me tell you a brief history
of Accelerate Framework.

00:15:51.320 --> 00:15:56.200
Accelerate Framework has been
available on Mac OS X for many years.

00:15:56.280 --> 00:16:01.680
We have VDSP for signal processing,
we have linear algebra libraries,

00:16:01.760 --> 00:16:05.910
we have image processing libraries,
and we have the math functions.

00:16:06.550 --> 00:16:08.990
Over the years,
we're trying to bring every

00:16:08.990 --> 00:16:12.980
component from Mac OS X to iOS.

00:16:13.230 --> 00:16:17.640
We start with VDSP and linear algebra.

00:16:17.680 --> 00:16:22.400
Last year, we added Vimage and VForce.

00:16:22.830 --> 00:16:24.420
It's been a huge success for us.

00:16:24.500 --> 00:16:27.630
So this year,
I'm happy to announce that we added

00:16:27.630 --> 00:16:30.450
the last piece of Accelerate Framework.

00:16:30.660 --> 00:16:32.500
We added the VMathlib.

00:16:32.500 --> 00:16:36.640
So if your code is using
Accelerate Framework on Mac OS X,

00:16:36.640 --> 00:16:38.500
you can safely port it to iOS.

00:16:38.500 --> 00:16:41.340
All the components are there.

00:16:41.420 --> 00:16:46.300
We have a complete picture
for both iOS and Mac OS X.

00:16:48.160 --> 00:16:52.000
Now, since I introduced vMathlib,
let's talk about it.

00:16:52.000 --> 00:16:56.900
vMathlib is a SIMD vector library.

00:16:57.070 --> 00:16:59.900
It operates on the SIMD vector.

00:17:01.320 --> 00:17:05.680
In Vector Numerics Group,
we support math for every data length.

00:17:05.910 --> 00:17:08.970
For your scalar data,
one flowing point input and

00:17:08.970 --> 00:17:12.390
one flowing point output,
we have LibM.

00:17:13.550 --> 00:17:16.290
For your array data, we have a VForce.

00:17:16.480 --> 00:17:20.480
VForce operates on arrays,
so it takes array as input and

00:17:20.480 --> 00:17:23.510
generates another array as output.

00:17:23.920 --> 00:17:26.960
VMathlib is something in between.

00:17:27.100 --> 00:17:30.770
VMathlib operates on SIMD vectors.

00:17:30.990 --> 00:17:35.400
So in Accelerate Framework,
we define the vfloat data type that maps

00:17:35.600 --> 00:17:38.700
to the vector register in your processor.

00:17:38.810 --> 00:17:41.520
In most architecture,
vfloat will have four elements,

00:17:41.730 --> 00:17:45.890
four single precision falling
point numbers in the structure.

00:17:46.000 --> 00:17:50.610
And you'll be using vfloat when
you're writing your own vector code.

00:17:51.060 --> 00:17:55.100
For those who are not familiar with LibM,
here are a few words.

00:17:55.140 --> 00:17:59.050
LibM is your standard math library in C.

00:17:59.490 --> 00:18:01.430
It has a collection of
transcendental functions.

00:18:01.530 --> 00:18:07.070
Here are the familiar names:
exp , log , sine, cosine.

00:18:07.890 --> 00:18:11.630
For VForce, it operates on arrays.

00:18:11.880 --> 00:18:17.500
It has the VV as the function prefix,
so you can see VV exp ,

00:18:17.500 --> 00:18:20.590
VV log , VV sin , etc.

00:18:21.320 --> 00:18:25.270
The mathlib has only
one "v" as the prefix.

00:18:25.270 --> 00:18:30.090
So there is v_exp_f, v_log_f, v_sine_f,
etc.

00:18:30.980 --> 00:18:36.400
You want to use the math lib when
you're writing your own vector code.

00:18:36.790 --> 00:18:40.920
While you're writing your vector code,
you probably sometimes want to

00:18:40.920 --> 00:18:43.480
take sine or cosine of a value.

00:18:43.660 --> 00:18:45.300
What do you do in this case?

00:18:45.380 --> 00:18:49.550
Well, you could use libm to
achieve what you want,

00:18:49.660 --> 00:18:51.840
something like this.

00:18:51.990 --> 00:18:58.180
So you include math.h to use libm,
and you write a for loop to take each

00:18:58.230 --> 00:19:03.950
element in the input vector and then
store the result into the output vector.

00:19:04.440 --> 00:19:07.660
So there's an obvious
problem in this code.

00:19:07.780 --> 00:19:11.310
You want to write a vector algorithm
because you want to take advantage

00:19:11.720 --> 00:19:14.630
of the performance of vector units.

00:19:14.750 --> 00:19:18.720
But LibM is not using vector units,
so you're not getting the

00:19:18.720 --> 00:19:20.780
performance improvement.

00:19:21.150 --> 00:19:25.000
So how about we use V-force?

00:19:25.030 --> 00:19:30.590
V-force does use the vector unit,
and the code will look like this.

00:19:31.090 --> 00:19:36.350
So include Accelerate header
and B4s operate on arrays.

00:19:36.380 --> 00:19:39.950
So you have to take the address
of your input vector and output

00:19:40.050 --> 00:19:44.110
vector and then tell B4s the length.

00:19:44.400 --> 00:19:47.180
Well, it works, but it's awkward,
obviously.

00:19:47.320 --> 00:19:51.940
And another thing is,
because v4 is designed to work on arrays,

00:19:52.040 --> 00:19:54.420
it takes the pointer to that array.

00:19:54.540 --> 00:19:57.040
It involves the memory access.

00:19:57.170 --> 00:20:01.640
So if your input and output
vectors are already in register,

00:20:01.640 --> 00:20:04.900
there's no need to do the memory access.

00:20:05.000 --> 00:20:07.790
We can use vMathlib to achieve that.

00:20:07.910 --> 00:20:11.430
Here's how you write
the code using vMathlib.

00:20:11.760 --> 00:20:13.480
So it's very simple.

00:20:13.660 --> 00:20:18.140
We call v_sine_f and pass
v_x as the input argument,

00:20:18.260 --> 00:20:21.290
and you'll have the
result in another vector,

00:20:21.290 --> 00:20:21.840
v_y.

00:20:21.980 --> 00:20:25.100
The code is much cleaner,
and there is no memory access.

00:20:25.420 --> 00:20:28.170
You will get the optimum performance.

00:20:28.830 --> 00:20:31.260
So that's the mathlet.

00:20:31.350 --> 00:20:34.340
I briefly mentioned VForce
in the previous slide,

00:20:34.400 --> 00:20:36.830
but without going into too much detail.

00:20:36.970 --> 00:20:39.660
Now is the time to do so.

00:20:39.780 --> 00:20:43.860
VForce is a vectorized math library.

00:20:44.730 --> 00:20:47.050
He operates on arrays,
and in addition to the

00:20:47.050 --> 00:20:50.650
transcendental functions,
we have rounding functions.

00:20:51.060 --> 00:20:53.100
All four rounding modes are supported.

00:20:53.270 --> 00:20:59.110
And we also have lots of other stuff,
like square root, remainder, next after,

00:20:59.110 --> 00:20:59.700
etc.

00:21:01.020 --> 00:21:04.160
Let's say you want to write
a signal generator app,

00:21:04.160 --> 00:21:08.050
and you want to generate a
frequency-modulated sine wave.

00:21:08.400 --> 00:21:11.420
Again, you can use LibM to do that.

00:21:11.650 --> 00:21:16.340
You write a for loop to call sine f,
which is the LibM function.

00:21:16.430 --> 00:21:19.460
You go through each element in
the input array and then sort

00:21:19.460 --> 00:21:21.340
the result to the output array.

00:21:21.420 --> 00:21:24.000
That works, but it's not optimal.

00:21:24.050 --> 00:21:26.340
Let's use V-force for that.

00:21:27.060 --> 00:21:28.640
It's very simple to use vforce.

00:21:28.880 --> 00:21:34.300
You simply replace a for loop with
one function call to vv sin f.

00:21:34.370 --> 00:21:39.250
Passing the address to the input
buffer and output buffer and

00:21:39.250 --> 00:21:43.390
also the pointer to the length,
the frequency modulated sine wave

00:21:43.390 --> 00:21:45.540
will be generated and output buffered.

00:21:45.680 --> 00:21:47.660
It's just that simple.

00:21:48.380 --> 00:21:52.140
Now, let's look at the performance
comparison of the two.

00:21:52.260 --> 00:21:56.870
One is using V-force,
the other one is just using the for loop.

00:21:58.370 --> 00:22:02.480
As you can see,
V-force is more than twice faster

00:22:03.070 --> 00:22:05.540
than using a simple for loop.

00:22:05.670 --> 00:22:10.390
At the same time,
it consumes less energy.

00:22:10.770 --> 00:22:15.900
V-force is more than twice more energy
efficient than using a simple for loop.

00:22:16.060 --> 00:22:20.200
So again, faster and energy efficient.

00:22:20.320 --> 00:22:23.360
That's the motto of Accelerate Framework.

00:22:24.080 --> 00:22:28.300
We didn't just cherry pick
sine to be our example.

00:22:28.390 --> 00:22:31.000
There are a bunch of other
functions in vForce as well.

00:22:31.370 --> 00:22:35.000
We have truncation, log, x, power.

00:22:35.250 --> 00:22:39.000
Across the board,
you can see a typical 2x speedup.

00:22:39.140 --> 00:22:43.510
So you can safely use vForce
and expect great results.

00:22:44.910 --> 00:22:47.220
Here's some detail about vForce.

00:22:47.270 --> 00:22:52.080
vForce supports both single and double
precision falling point numbers.

00:22:52.200 --> 00:22:57.310
It handles edge cases correctly,
so if your input has infinity or nans,

00:22:57.630 --> 00:23:01.240
positive zero, negative zero,
you don't have to worry about it.

00:23:01.390 --> 00:23:04.690
You just pass them to vForce,
and then vForce will

00:23:04.690 --> 00:23:06.810
handle those cases for you.

00:23:07.630 --> 00:23:10.700
V4 requires minimum data alignment.

00:23:10.710 --> 00:23:13.790
We only require the
native data alignment.

00:23:13.910 --> 00:23:15.690
For a single precision
flowing point number,

00:23:15.770 --> 00:23:17.090
that's going to be four bytes.

00:23:17.170 --> 00:23:19.860
For a double precision
flowing point number,

00:23:19.860 --> 00:23:21.100
it's eight bytes.

00:23:21.230 --> 00:23:24.000
And we also support in-place calculation.

00:23:24.130 --> 00:23:26.630
You don't have to allocate a
temp buffer to hold the results.

00:23:26.920 --> 00:23:30.250
We can just operate
on your data in place.

00:23:30.950 --> 00:23:35.950
And a lot of people ask us this question,
"I only have 20 elements.

00:23:36.110 --> 00:23:38.140
I only have 20 numbers.

00:23:38.190 --> 00:23:44.080
Is it beneficial to use V-force?" Well,
as a rule of thumb,

00:23:44.240 --> 00:23:48.880
You will see performance improvement
when you have more than 16 elements.

00:23:49.080 --> 00:23:50.870
While some function
has higher thresholds,

00:23:50.950 --> 00:23:53.920
some function has lower thresholds,
if you're interested in

00:23:53.920 --> 00:23:56.380
finding out the exact number,
you can just write a

00:23:56.490 --> 00:23:58.100
simple app to do a test.

00:23:58.180 --> 00:24:01.010
But as a rule of thumb,
if you have more than 16

00:24:01.070 --> 00:24:03.820
elements in your array,
you're good to go.

00:24:03.840 --> 00:24:08.260
You can use vForce and
expect great results.

00:24:08.820 --> 00:24:10.690
So that's V-force.

00:24:10.790 --> 00:24:15.590
We talked about the math library,
VMathlib and V-force.

00:24:15.590 --> 00:24:18.860
I'm going to talk about
another big block,

00:24:18.860 --> 00:24:19.700
VDSP.

00:24:20.100 --> 00:24:23.600
VDSP is our signal processing library.

00:24:23.730 --> 00:24:27.920
It has pretty much everything
you need for signal processing.

00:24:28.040 --> 00:24:33.010
We have basic operations like add,
subtract, conversion,

00:24:33.140 --> 00:24:36.120
and we also have discrete
full-wave transform.

00:24:36.250 --> 00:24:39.770
The FFT that we saw earlier in
the case study is actually part of

00:24:39.770 --> 00:24:41.910
the discrete full-wave transform.

00:24:42.200 --> 00:24:47.510
Discrete full-wave transform,
we support Radix 2, Radix 3, and Radix 5.

00:24:48.210 --> 00:24:51.540
And we also have convolution,
if you want to do your own

00:24:51.540 --> 00:24:52.950
filtering on the signal.

00:24:53.070 --> 00:24:57.600
And we also have correlation,
if you want to do signal analysis.

00:24:57.700 --> 00:24:59.610
They're there for you.

00:25:01.060 --> 00:25:04.240
In iOS 6, we added two new features.

00:25:04.250 --> 00:25:06.890
The first one is discrete
cosine transform,

00:25:06.890 --> 00:25:10.700
and another one is the
Bi-Qual I/O filter.

00:25:10.770 --> 00:25:14.440
These two features are
requested by many developers,

00:25:14.510 --> 00:25:17.240
and we do look at those feature requests.

00:25:17.290 --> 00:25:19.730
When the timing is right,
we'll add them in.

00:25:19.850 --> 00:25:22.660
So if you find something
that you really need,

00:25:22.740 --> 00:25:27.170
it's really great, and it's not available
in Accelerate Framework,

00:25:27.330 --> 00:25:29.360
Go file feature requests.

00:25:29.410 --> 00:25:30.380
Don't be afraid.

00:25:30.400 --> 00:25:33.520
We do look at them,
and we're going to work on

00:25:33.530 --> 00:25:35.870
them when the timing is right.

00:25:36.420 --> 00:25:40.500
Descript Cosine Transform is very
similar to the FFT that we saw earlier,

00:25:40.500 --> 00:25:44.930
so I'm going to take the
Biquad IL filter as an example.

00:25:46.050 --> 00:25:51.230
Here is a series of end-stage
cascaded biquad ILR filter.

00:25:51.420 --> 00:25:55.780
So you pass input into end-stage
of second-order biquad filter,

00:25:55.780 --> 00:25:57.840
and you get the output at the end.

00:25:57.900 --> 00:26:02.040
This is very common in
the audio processing.

00:26:02.170 --> 00:26:05.420
So we added this in iOS 6.

00:26:05.840 --> 00:26:11.560
What it does basically is we optimize
the biquad filter because there is an

00:26:11.560 --> 00:26:16.000
inherent feedback loop in the biquad
filter that's very hard to optimize.

00:26:16.000 --> 00:26:19.780
We did the work for you,
so you can just use the biquad

00:26:19.780 --> 00:26:24.270
ILR filter in Accelerate
Framework and get great results.

00:26:24.470 --> 00:26:27.100
Here's an example on how to use it.

00:26:27.100 --> 00:26:33.350
It is the same three steps-- set up,
operate, and destroy.

00:26:34.660 --> 00:26:38.700
First,
include Accelerate header as usual.

00:26:38.700 --> 00:26:42.840
And we specify we want to
do a 10-stage IR filter.

00:26:42.990 --> 00:26:45.280
And for each stage,

00:26:45.500 --> 00:26:49.700
AI filter requires five
coefficients and two delay stages.

00:26:49.830 --> 00:26:53.320
You can think of delay stages as
the current state of the filter.

00:26:53.440 --> 00:26:54.640
There are two of them.

00:26:54.770 --> 00:26:57.870
And there's input, output,

00:26:58.790 --> 00:27:03.940
Once at the start of your program,
you call "Biqua Create Setup"

00:27:03.960 --> 00:27:08.030
to create the setup structure
that's needed by the operation.

00:27:08.170 --> 00:27:11.310
You pass in the field
coefficient and number of stages.

00:27:11.580 --> 00:27:14.570
A setup structure will
be created for you.

00:27:14.920 --> 00:27:18.740
And again,
you can call the VDSP biquad to operate

00:27:18.740 --> 00:27:25.290
on the data multiple times to work on all
your data with the same setup structure.

00:27:26.160 --> 00:27:32.140
And at the end, you destroy the setup to
reclaim the memory allocated.

00:27:32.800 --> 00:27:36.730
So that's the same three
simple steps that you use

00:27:36.870 --> 00:27:40.530
for Biqua I/O filter and FFT.

00:27:40.650 --> 00:27:45.040
Now here are a few things more
about the data type in VDSP.

00:27:45.170 --> 00:27:49.440
We support single and double precision,
falling point numbers,

00:27:49.530 --> 00:27:52.100
and we also support real
and complex numbers.

00:27:52.210 --> 00:27:56.040
So you can do a real-to-complex
Fourier transform FFT or

00:27:56.040 --> 00:27:58.180
complex-to-real FFT.

00:27:58.640 --> 00:28:00.790
Those are all supported.

00:28:01.120 --> 00:28:04.400
will also support strided data access.

00:28:04.510 --> 00:28:08.300
In the previous examples,
I always used a stride of one,

00:28:08.360 --> 00:28:12.000
meaning the data in the
memory is contiguous.

00:28:13.020 --> 00:28:15.900
We support, you know,
when you're getting data

00:28:15.980 --> 00:28:18.690
from somewhere else,
you want to access every other

00:28:18.690 --> 00:28:20.340
two or every other three.

00:28:20.370 --> 00:28:22.000
We do support that.

00:28:22.000 --> 00:28:26.440
However, it's our recommendation that if
you have control over your data,

00:28:26.460 --> 00:28:29.800
you want to arrange your data in
contiguous memory so we can fully

00:28:29.800 --> 00:28:33.590
take advantage of the vector unit
and give you the best results.

00:28:33.600 --> 00:28:37.160
If you can't,
don't worry about copying the data

00:28:37.160 --> 00:28:39.680
in memory to make it contiguous.

00:28:39.680 --> 00:28:43.770
We can just operate on the
data with strided access.

00:28:44.570 --> 00:28:50.030
So that is VDSP,
the Signal Processing Library.

00:28:51.680 --> 00:28:55.280
Now, another big block in
Accelerate Framework is vImage,

00:28:55.440 --> 00:28:58.250
the image processing library.

00:29:00.040 --> 00:29:02.650
We're in the era of digital photography.

00:29:02.780 --> 00:29:07.200
People love to share photos,
and before they share a photo,

00:29:07.330 --> 00:29:10.200
they actually like to do some
post-processing on the photo,

00:29:10.200 --> 00:29:14.830
like removing the red eye
or increase the saturation,

00:29:14.900 --> 00:29:16.890
all things you can do.

00:29:17.140 --> 00:29:20.250
So, Vimage is a great help.

00:29:21.300 --> 00:29:24.000
Vimage can do a lot of things.

00:29:24.120 --> 00:29:26.040
It can do convolution.

00:29:26.150 --> 00:29:30.360
Convolution can achieve various effects,
like blur.

00:29:30.710 --> 00:29:33.860
I'm going to show you how to use
the convolution to blur right now.

00:29:33.860 --> 00:29:35.980
Here's the effect.

00:29:39.910 --> 00:29:42.160
And you can also do geometry.

00:29:42.270 --> 00:29:46.700
You can rotate and scale
your picture like this.

00:29:51.310 --> 00:29:52.430
You can do morphology.

00:29:52.610 --> 00:29:54.140
This one is very interesting.

00:29:54.230 --> 00:29:59.390
I'm going to turn each snowflake
in this picture into a bigger one.

00:30:03.130 --> 00:30:06.800
There is also Alpha that you
can blend two pictures together.

00:30:06.800 --> 00:30:10.290
I'm going to fade one
out and fade another in.

00:30:15.520 --> 00:30:19.670
We have transform functions that
can change the hue in this picture.

00:30:19.710 --> 00:30:23.010
I'm going to send this
little guy to outer space.

00:30:27.350 --> 00:30:28.320
And we have Histogram.

00:30:28.580 --> 00:30:32.000
If you take a lot of photos,
you're familiar with this.

00:30:32.110 --> 00:30:36.700
It is the intensity distribution
of your RGB channels.

00:30:36.760 --> 00:30:38.520
Like this.

00:30:39.500 --> 00:30:42.370
And lastly,
we have Conversion function to convert

00:30:42.370 --> 00:30:45.470
your image format from one to another.

00:30:45.940 --> 00:30:48.520
We add a few things to vImage.

00:30:48.550 --> 00:30:54.560
In Mountain Lion and iOS 6, we have BGRA,
RGBA support,

00:30:54.560 --> 00:30:57.860
and now we also support 16-bit integer.

00:30:59.800 --> 00:31:04.300
Because convolution is
such a versatile operation,

00:31:04.360 --> 00:31:06.900
I'd like to mention a
few words about it too.

00:31:07.020 --> 00:31:11.220
So, I show you blur,
but it's not the only

00:31:11.300 --> 00:31:12.700
thing convolution can do.

00:31:12.760 --> 00:31:14.980
It can also do edge detection.

00:31:15.140 --> 00:31:20.080
You can use different kernels on each
color channel to achieve a pretty fancy

00:31:20.160 --> 00:31:22.590
result I'm going to show you right now.

00:31:22.720 --> 00:31:24.590
Here's blur.

00:31:24.700 --> 00:31:29.910
and edge detection and different
color channel operation.

00:31:30.760 --> 00:31:35.130
So it looks very fancy,
but it's actually very simple.

00:31:35.150 --> 00:31:37.180
It's a very simple idea.

00:31:37.450 --> 00:31:42.980
Convolution is basically a
weighted average of nearby pixels.

00:31:43.410 --> 00:31:46.540
Let's say you have a kernel like this.

00:31:46.540 --> 00:31:51.670
The center pixel is more heavily
weighted than the side or corner pixel.

00:31:52.020 --> 00:31:54.000
And you have an image like this.

00:31:54.110 --> 00:31:58.490
You can see the purple pixels
form a sharp edge in this image.

00:31:58.860 --> 00:32:03.300
Let's apply the weighted
average on the center pixel.

00:32:03.510 --> 00:32:07.200
What you get is a lighter purple pixel.

00:32:07.440 --> 00:32:12.010
Same idea, you apply this kernel to all
the pixels in this image.

00:32:13.030 --> 00:32:16.280
Basically,
the sharp edge formed by the purple

00:32:16.280 --> 00:32:18.690
pixel is replaced by a softer edge.

00:32:18.850 --> 00:32:21.980
Essentially, this is a blurring effect.

00:32:22.600 --> 00:32:26.060
Convolution is this simple,
so you might be thinking,

00:32:26.060 --> 00:32:29.490
"I can write one myself." That's true.

00:32:29.630 --> 00:32:34.060
It's just nested for loop,
four of them nested together.

00:32:34.220 --> 00:32:39.520
However, there is problem in this code.

00:32:39.520 --> 00:32:39.520
First,

00:32:39.740 --> 00:32:43.050
It does not handle the
edge pixel correctly.

00:32:43.160 --> 00:32:45.810
When you're at the edge,
you don't have all your

00:32:45.810 --> 00:32:47.840
nearby pixels for the kernel.

00:32:48.020 --> 00:32:50.000
You need to handle that case.

00:32:50.130 --> 00:32:55.420
And it doesn't handle the overflow case,
so you might see an artifact in

00:32:55.420 --> 00:32:58.000
your picture due to overflow.

00:32:58.150 --> 00:33:01.550
And the most important
thing here is it's really,

00:33:01.550 --> 00:33:02.690
really slow.

00:33:02.850 --> 00:33:05.920
In our experience,
a good convolution code takes

00:33:05.920 --> 00:33:09.610
more than 100 slides of code,
if it's not more.

00:33:10.320 --> 00:33:13.700
So I'm going to show you
the performance comparison.

00:33:13.920 --> 00:33:15.950
I'm going to show you
how to use VMG-First,

00:33:15.960 --> 00:33:20.840
a simple function called
to VMG-Conv ARGB 8888.

00:33:20.940 --> 00:33:25.510
Passing the source and destination,
and we'll pass a little bit more

00:33:25.510 --> 00:33:27.400
information about the kernel.

00:33:27.520 --> 00:33:30.860
Your convolution result will
be ready in the destination.

00:33:30.990 --> 00:33:35.350
So now I'm going to show
you the performance result.

00:33:35.460 --> 00:33:41.040
This is a 7x7 convolution
on a 1024x768 image.

00:33:41.070 --> 00:33:48.000
The image is more than 14 times
faster than using a good scalar code.

00:33:49.090 --> 00:33:51.710
And energy consumption, same story.

00:33:51.810 --> 00:33:53.380
It's very energy efficient.

00:33:53.680 --> 00:33:56.990
It consumes less than
one-eighth of the energy.

00:33:57.100 --> 00:34:01.150
So fast and energy efficient.

00:34:02.200 --> 00:34:06.590
There are a lot of image formats,
and we cannot support all of them.

00:34:06.800 --> 00:34:10.100
So we classify them into two categories.

00:34:10.240 --> 00:34:15.460
First is the core formats,
and the rest is the non-core formats.

00:34:15.750 --> 00:34:18.330
There are four types in core formats.

00:34:18.420 --> 00:34:23.320
If you have a single channel image,
each pixel can be an 8-bit integer

00:34:23.320 --> 00:34:26.280
or a 32-bit folding point number.

00:34:26.400 --> 00:34:34.600
We also support four channel interleaves.

00:34:34.600 --> 00:34:34.610
So we have ARGB 8888 and ARGB FFFF.

00:34:35.560 --> 00:34:41.570
Any format that's not in this floor
is considered non-core formats.

00:34:41.670 --> 00:34:47.360
We have BGRA, RGB, GBR,
16-bit unsigned integer,

00:34:47.360 --> 00:34:49.830
or 16-bit falling point number.

00:34:50.450 --> 00:34:54.400
And the main difference between
core format and non-core format

00:34:54.470 --> 00:35:00.360
is that Vimage operation supports
core formats extensively,

00:35:00.480 --> 00:35:02.810
but not non-core formats.

00:35:03.150 --> 00:35:07.450
So you might be wondering, OK,
now if I have an image

00:35:07.560 --> 00:35:10.860
that's in non-core format,
I want to do some operation

00:35:10.860 --> 00:35:12.400
on it using the image.

00:35:12.530 --> 00:35:13.740
Can I do that?

00:35:13.790 --> 00:35:16.110
Yes, of course you can.

00:35:16.480 --> 00:35:18.790
We have all the conversion
functions to help you.

00:35:18.930 --> 00:35:20.320
Here is an example.

00:35:20.400 --> 00:35:26.310
I want to do a scaling operation on
the pre-multiplied PlanarF image.

00:35:27.020 --> 00:35:30.890
The core format expects the image
to be a non-premultiplied image.

00:35:31.250 --> 00:35:33.400
So if you have a
premultiplied alpha image,

00:35:33.680 --> 00:35:36.890
you want to convert it
to the core format first.

00:35:37.110 --> 00:35:44.460
So you call this conversion function,
vimage on premultiplied data, planar f.

00:35:44.590 --> 00:35:46.800
It works in place.

00:35:47.410 --> 00:35:51.140
And then you can operate
on the core format,

00:35:51.200 --> 00:35:53.820
which is the Planner F.

00:35:54.430 --> 00:36:01.100
And at the end, you convert it back to
your desired image format.

00:36:01.200 --> 00:36:04.150
Now, you might be thinking, "Wow.

00:36:04.270 --> 00:36:06.180
It looks like three times the workload.

00:36:06.200 --> 00:36:10.730
I have to convert it and I have
to operate on it and then

00:36:10.770 --> 00:36:13.920
convert it back at the end.

00:36:13.920 --> 00:36:13.920
Well,

00:36:14.250 --> 00:36:16.880
I'm going to show you the
performance result to tell you

00:36:16.880 --> 00:36:19.000
that it is okay to do that.

00:36:19.180 --> 00:36:22.600
As you can see,
the conversion at the beginning and at

00:36:22.600 --> 00:36:28.540
the end only takes less than 2% of the
execution time on that piece of code.

00:36:28.700 --> 00:36:32.940
So still, the majority time spent is
on the operation itself,

00:36:32.940 --> 00:36:34.800
not on the conversion.

00:36:35.170 --> 00:36:37.950
So if you have a file,
if you have an image

00:36:37.950 --> 00:36:41.200
that's in non-core format,
you can go ahead and

00:36:41.210 --> 00:36:44.120
convert it to core format,
do all sorts of operations

00:36:44.120 --> 00:36:47.830
you want to do on the image,
and at the end, you convert it to the

00:36:47.830 --> 00:36:49.500
one that you desired.

00:36:49.660 --> 00:36:53.230
So you're going to get the
performance improvement that

00:36:53.230 --> 00:36:55.410
you want while saving energy.

00:36:57.590 --> 00:37:01.260
So some few more data
requirements in vImage.

00:37:01.260 --> 00:37:04.440
Again, we require minimum data alignment.

00:37:04.480 --> 00:37:08.220
For a single precision,
point-in-point number is 4 bytes.

00:37:08.220 --> 00:37:10.720
And the data is not containerized.

00:37:10.760 --> 00:37:16.160
So you're not copying the entire image
from one memory location to another.

00:37:16.160 --> 00:37:18.900
We're just passing the pointers around.

00:37:18.930 --> 00:37:23.380
So there is no copy
involved in vImage buffer.

00:37:23.380 --> 00:37:25.400
It's very efficient.

00:37:27.100 --> 00:37:29.150
So that's the image.

00:37:29.160 --> 00:37:32.240
And the last piece is the
linear algebra library.

00:37:32.260 --> 00:37:35.250
I'm going to invite my colleague,
Geoff Beltor.

00:37:35.570 --> 00:37:38.570
He will tell us more
about linear algebra.

00:37:39.450 --> 00:37:40.290
Thanks, Luke.

00:37:40.400 --> 00:37:42.490
So for linear algebra in
the Accelerate framework,

00:37:42.490 --> 00:37:44.000
we've got two great packages.

00:37:44.020 --> 00:37:46.660
We've got a LAPAC,
the linear algebra package,

00:37:46.870 --> 00:37:50.240
and we've got BLAS,
the basic linear algebra subprograms.

00:37:50.390 --> 00:37:53.310
Let's start by taking a look at a LAPAC.

00:37:54.030 --> 00:37:57.030
Well, APAC is the high-level
linear algebra functionality.

00:37:57.150 --> 00:37:59.540
So if you want to solve a
system of linear equations,

00:37:59.550 --> 00:38:02.240
there's going to be
APIs to do that in here.

00:38:02.240 --> 00:38:04.540
Maybe you need to perform
a matrix factorization,

00:38:04.540 --> 00:38:06.540
a QR or an LU factorization.

00:38:06.540 --> 00:38:09.120
There's APIs for that as well.

00:38:09.230 --> 00:38:13.990
There's also a functionality to
compute eigenvalues and eigenvectors.

00:38:14.160 --> 00:38:16.580
There's several hundred
APIs in LAPAC alone,

00:38:16.680 --> 00:38:18.610
so there's a huge amount
of functionality here.

00:38:18.780 --> 00:38:21.260
It's probably going
to have what you need.

00:38:22.590 --> 00:38:26.170
Let's take a look at an example of one
of the really common uses of a lay pack,

00:38:26.210 --> 00:38:29.000
and that's solving a
system of linear equations.

00:38:29.120 --> 00:38:32.480
One of the great things about a lay pack
is it's got something for everybody.

00:38:32.640 --> 00:38:35.000
So if you want to do
this with a single API,

00:38:35.160 --> 00:38:36.930
there's a routine that's
going to do that for you.

00:38:37.060 --> 00:38:39.810
It's going to do the factor and
the solve behind the scenes,

00:38:39.810 --> 00:38:42.000
and it's going to give
you your result back.

00:38:42.000 --> 00:38:44.620
If you want to get your
hands dirty a little bit,

00:38:44.620 --> 00:38:47.000
you can do it the way
I have shown up here.

00:38:47.000 --> 00:38:50.260
So we've prepared our matrix in A,
and we're going to do the

00:38:50.370 --> 00:38:52.000
factor first with dget rf.

00:38:52.130 --> 00:38:56.080
That factor is going to be done in place,
and then we're going to send

00:38:56.080 --> 00:38:59.000
that factored matrix to dget
rs to perform the solve.

00:38:59.000 --> 00:39:00.240
We get our result.

00:39:00.260 --> 00:39:01.680
It's pretty simple.

00:39:01.680 --> 00:39:03.000
It's pretty easy.

00:39:03.350 --> 00:39:06.700
There's a lot that goes on
behind the scenes in a lay pack,

00:39:06.860 --> 00:39:09.480
and a lot of the lay pack is
built on the other package,

00:39:09.480 --> 00:39:10.000
BLAS.

00:39:10.140 --> 00:39:14.700
Let's take an example,
a look at an example of how we use BLAS.

00:39:14.820 --> 00:39:18.150
So the matrix factor that we just
saw there is going to spend a lot of

00:39:18.150 --> 00:39:20.400
time in the matrix-matrix multiply.

00:39:20.540 --> 00:39:22.690
That's what I'm showing here.

00:39:22.700 --> 00:39:24.700
The use case here is
going to be the same.

00:39:24.700 --> 00:39:28.590
We're going to prepare our matrices A, B,
and C, and we're going to call

00:39:28.710 --> 00:39:30.190
into C BLAS DGEM here.

00:39:30.250 --> 00:39:33.980
DGEM stands for
General Matrix Matrix Multiply,

00:39:33.980 --> 00:39:36.990
and the D prefix is Double Precision.

00:39:37.790 --> 00:39:41.240
BLAS supports both row and
column-major storage formats,

00:39:41.280 --> 00:39:43.780
so rows or columns are
contiguous in memory,

00:39:43.880 --> 00:39:47.260
and we need to specify that
when we make the function call.

00:39:47.360 --> 00:39:51.500
Also, it's going to support transposes
and not transposing the data,

00:39:51.500 --> 00:39:56.120
so you don't have to manipulate your
matrices before you use BLAS or LAPAC.

00:39:56.220 --> 00:39:59.000
It's all going to be
done behind the scenes.

00:39:59.090 --> 00:40:01.240
There's a lot of
functionality in BLAS as well.

00:40:01.240 --> 00:40:06.190
So, as I mentioned before,
LAPAC is built pretty heavily on BLAS,

00:40:06.310 --> 00:40:10.000
so BLAS tends to be the low-level
linear algebra operations.

00:40:10.090 --> 00:40:13.530
It tends to be categorized
into three levels.

00:40:13.650 --> 00:40:16.730
So there's vector operations,
dot product, scalar products,

00:40:16.800 --> 00:40:23.300
vector sums, matrix vector operations,
matrix vector product, outer product,

00:40:23.300 --> 00:40:25.340
and then matrix matrix operations.

00:40:25.420 --> 00:40:27.300
So back solves, rank updates,
and the matrix model.

00:40:27.300 --> 00:40:27.300
So, there's a lot of
functionality in BLAS as well.

00:40:27.300 --> 00:40:27.300
So, as I mentioned before,
LAPAC is built pretty heavily on BLAS,

00:40:27.300 --> 00:40:27.300
so BLAS tends to be the low-level
linear algebra operations.

00:40:27.300 --> 00:40:27.480
It tends to be categorized
into three levels.

00:40:27.480 --> 00:40:27.740
So there's vector operations,
dot product, scalar products,

00:40:27.740 --> 00:40:28.080
vector sums, matrix vector operations,
matrix vector product, outer product,

00:40:28.080 --> 00:40:28.280
and then matrix matrix operations.

00:40:28.320 --> 00:40:28.850
So, back solves, rank updates,
and the matrix model.

00:40:28.960 --> 00:40:31.390
multiply that we just saw.

00:40:31.920 --> 00:40:34.120
Let's take a look at
some performance numbers.

00:40:34.380 --> 00:40:38.090
So for the double precision matrix
multiply we saw the example of,

00:40:38.140 --> 00:40:41.120
what I've got here is a graph
on the x-axis I'm showing

00:40:41.120 --> 00:40:42.460
a range of matrix sizes.

00:40:42.540 --> 00:40:48.640
So from 64 by 64 matrices
up through 1,024 by 1,024,

00:40:48.790 --> 00:40:51.660
and on the y-axis
performance in gigaflops.

00:40:51.800 --> 00:40:54.330
So we're looking for higher, better here.

00:40:54.430 --> 00:40:56.630
Let's look at that data.

00:40:56.960 --> 00:41:00.540
It quickly climbs to some
really great performance.

00:41:00.540 --> 00:41:02.220
Even in the small sizes,
we see we're getting

00:41:02.220 --> 00:41:04.460
some good performance.

00:41:04.590 --> 00:41:08.840
By about 500, we've reached a plateau,
and we're going to give you

00:41:08.840 --> 00:41:13.000
that great performance for
as big as you want to get.

00:41:13.060 --> 00:41:17.550
I don't show small sizes here,
but even down into 4x4 and 8x8 matrices,

00:41:17.550 --> 00:41:19.490
we spent a lot of time there, too.

00:41:19.500 --> 00:41:23.580
You're going to get great performance
for a range of matrix sizes.

00:41:24.070 --> 00:41:27.000
For those of you familiar
with performance numbers,

00:41:27.000 --> 00:41:31.800
what we're showing here is almost
90 double precision gigaflops.

00:41:31.840 --> 00:41:33.790
This is really impressive.

00:41:33.980 --> 00:41:37.490
This is an iMac,
so a computer sitting on your desk

00:41:37.490 --> 00:41:40.380
giving you some really great performance.

00:41:40.920 --> 00:41:42.110
Don't take my word for it.

00:41:42.280 --> 00:41:44.040
Let's put this into perspective.

00:41:44.040 --> 00:41:46.880
So what I've got here is a
comparison similar to what

00:41:46.880 --> 00:41:48.890
we did in the FFT case study.

00:41:49.030 --> 00:41:52.420
We've got a straightforward
C implementation of the double

00:41:52.420 --> 00:41:54.290
precision matrix multiply.

00:41:54.310 --> 00:41:58.860
So a programmer finds the algorithm for
matrix multiply in a book and codes it.

00:41:58.970 --> 00:42:02.570
We've normalized the performance
and the energy to one here,

00:42:02.570 --> 00:42:06.650
and let's compare that to the
DGAM in the Accelerate framework.

00:42:08.490 --> 00:42:11.010
We see huge performance
improvements here,

00:42:11.260 --> 00:42:14.060
158 times faster.

00:42:14.220 --> 00:42:17.800
The really best part is we do this
with a fraction of the energy,

00:42:17.800 --> 00:42:22.090
so 1/73rd the amount of energy to
give you this really incredible

00:42:22.090 --> 00:42:24.460
performance improvement.

00:42:26.310 --> 00:42:30.630
Let's look at some of the details of the
data types that BLAS and LAPAC support.

00:42:30.810 --> 00:42:33.800
So they both support single
and double precision,

00:42:33.900 --> 00:42:37.010
both real and complex numbers.

00:42:37.260 --> 00:42:39.790
and a range of data layouts.

00:42:40.060 --> 00:42:43.440
So BLAS, as I mentioned,
is going to support row and column major.

00:42:43.500 --> 00:42:45.790
A LAPAC only supports column major.

00:42:45.910 --> 00:42:49.870
But both BLAS and LAPAC are
going to support dense matrices,

00:42:49.960 --> 00:42:53.440
general matrices, banded matrices,
triangular matrices,

00:42:53.530 --> 00:42:55.960
even a few packed structures.

00:42:56.030 --> 00:42:58.120
And they're going to support
transpose and conjugate

00:42:58.120 --> 00:42:59.530
transposes when appropriate.

00:42:59.540 --> 00:43:01.440
So again,
you're not going to have to modify your

00:43:01.500 --> 00:43:04.780
data before you call into BLAS or LAPAC.

00:43:05.290 --> 00:43:07.500
So that's a quick look at
what's available in a LayPack,

00:43:07.500 --> 00:43:11.200
and with that I'm going to turn it
back over to Luke to wrap up the talk.

00:43:11.240 --> 00:43:12.490
Well, those numbers are amazing.

00:43:12.490 --> 00:43:15.760
158 times faster.

00:43:15.950 --> 00:43:19.720
So right now I'm going to
give you a quick summary.

00:43:20.030 --> 00:43:22.800
Accelerate Framework is easy to use.

00:43:22.920 --> 00:43:24.700
Most of the time,
it's just one function call to

00:43:24.830 --> 00:43:30.340
replace a for loop and to replace
even 50 or 60 lines of code.

00:43:30.460 --> 00:43:32.910
And it's accurate,
so if your app is using

00:43:32.910 --> 00:43:36.980
Accelerate Framework,
your result is going to be accurate, too.

00:43:37.190 --> 00:43:41.300
Accelerate Framework is
fast with low energy usage,

00:43:41.300 --> 00:43:46.500
so your app will be responsive
and have a long battery life.

00:43:47.220 --> 00:43:51.740
It's available on both OS X and iOS,
so the same code will just

00:43:51.740 --> 00:43:56.180
work if you want to port
from one platform to another.

00:43:57.040 --> 00:44:01.400
Here are a few tips for
using Accelerate Framework.

00:44:01.490 --> 00:44:07.160
So if you could, you want to arrange your
data in contiguous memory

00:44:07.160 --> 00:44:09.990
and make it 16-by-aligned.

00:44:10.700 --> 00:44:17.490
We do support the data that has strided
axes or just native data alignment,

00:44:17.490 --> 00:44:21.610
but in order to fully take
advantage of the vector engine,

00:44:21.610 --> 00:44:26.720
we would prefer the data is in
16-by alignment and contiguous.

00:44:26.720 --> 00:44:29.700
I know that sometimes you get
the data from somewhere else,

00:44:29.700 --> 00:44:32.400
you have no control over it, that's fine.

00:44:32.430 --> 00:44:37.690
But if you do, try to make it contiguous
and aligned to 16 bytes.

00:44:37.890 --> 00:44:42.500
You will have the maximum
performance improvement in that case.

00:44:42.920 --> 00:44:46.140
And you also want your
data to be large enough.

00:44:46.210 --> 00:44:52.290
You don't want to do just one
simple vforce function call.

00:44:52.410 --> 00:44:54.890
That's not going to be effective.

00:44:55.780 --> 00:44:58.740
Whenever you have to
make a setup structure,

00:44:58.740 --> 00:45:01.580
you only need to do the setup once.

00:45:02.140 --> 00:45:06.450
Make all the operations that you
need to operate on all your data,

00:45:06.450 --> 00:45:11.790
and then destroy at the end to
reclaim the memories allocated.

00:45:12.830 --> 00:45:16.890
So just a quick refresher,
we have digital signal

00:45:16.890 --> 00:45:19.000
processing in VDSP.

00:45:19.060 --> 00:45:23.020
We have image processing in Vimage.

00:45:23.110 --> 00:45:26.170
There's linear algebra
in BLOFS and LAPack.

00:45:26.250 --> 00:45:31.340
And we have transcendental
function in VMathlib and Vforce.

00:45:31.400 --> 00:45:38.100
So this is really a comprehensive list
of computational functionalities there.

00:45:38.220 --> 00:45:43.100
It is more than likely that you will
find something you can use in your app.

00:45:43.250 --> 00:45:50.130
So on that note, I would like to say,
ladies and gentlemen, let's Accelerate!

00:45:55.360 --> 00:45:58.220
I encourage everyone here
that after the session,

00:45:58.220 --> 00:46:00.990
go look at your code,
find somewhere in your code you

00:46:01.030 --> 00:46:03.920
could use Accelerate Framework,
and give it a try.

00:46:04.210 --> 00:46:08.640
You will see the performance
improvement with your own eyes.

00:46:08.720 --> 00:46:12.440
If you need more information,
here are the contacts.

00:46:12.550 --> 00:46:14.400
So that's the end of the presentation.

00:46:14.400 --> 00:46:15.260
Thank you very much.