WEBVTT

00:00:09.500 --> 00:00:11.400
Good afternoon and welcome.

00:00:11.450 --> 00:00:12.690
My name is Anna Tikhonova.

00:00:12.750 --> 00:00:15.000
I'm an engineer on the OpenCL team.

00:00:15.110 --> 00:00:18.600
Let's talk about adopting
OpenCL in your application.

00:00:18.690 --> 00:00:21.350
So some of you in the audience
might be new to OpenCL.

00:00:21.480 --> 00:00:25.930
So let's begin with what is
OpenCL and why should you use it?

00:00:26.030 --> 00:00:31.700
So today's Macs have a multicore CPU with
a vector unit and one or more GPUs.

00:00:31.780 --> 00:00:35.110
If you want to take advantage of all
this different hardware in your system,

00:00:35.140 --> 00:00:36.940
it can be quite a challenge.

00:00:37.000 --> 00:00:41.030
Because the programming approaches
for the multicore CPUs and for

00:00:41.260 --> 00:00:43.080
the GPUs are quite different.

00:00:43.120 --> 00:00:46.570
You will actually have to write
different versions of your

00:00:46.760 --> 00:00:48.490
code for the CPU and the GPU.

00:00:49.360 --> 00:00:52.000
So specifically,
to take advantage of a CPU,

00:00:52.000 --> 00:00:55.920
you would need to split up the
work across multiple CPU cores

00:00:55.920 --> 00:00:57.800
using Grand Central Dispatch.

00:00:57.890 --> 00:01:01.680
And to utilize the vector unit,
you would need to write SAC or AVX code,

00:01:01.750 --> 00:01:03.890
depending on the instruction set.

00:01:04.210 --> 00:01:08.610
and then this multi-threaded, vectorized,
optimized CPU code will

00:01:08.610 --> 00:01:10.040
not run on the GPU.

00:01:10.040 --> 00:01:14.200
You would need to write a separate,
scalar version of the code for the GPU.

00:01:14.220 --> 00:01:18.200
And I don't know about you,
but this sounds like a lot of work to me.

00:01:18.440 --> 00:01:20.600
But OpenCL is here to help you.

00:01:20.710 --> 00:01:25.640
OpenCL is a technology which enables you
to write parallel code which will run

00:01:25.840 --> 00:01:29.090
efficiently on both the CPU and the GPU.

00:01:29.630 --> 00:01:32.440
The language for writing
OpenCL programs is C-based,

00:01:32.440 --> 00:01:35.650
which is good because most
of you are familiar with C,

00:01:35.650 --> 00:01:38.800
and if you're new to OpenCL,
you will find it very

00:01:38.860 --> 00:01:40.370
easy to get started.

00:01:40.810 --> 00:01:45.380
The common code you will write for the
CPU and the GPU will be scalar in nature,

00:01:45.500 --> 00:01:50.960
which is good because you have
to write just scalar code.

00:01:50.960 --> 00:01:54.540
But you may be thinking,
how could this possibly give me

00:01:54.540 --> 00:01:56.460
the best performance on the CPU?

00:01:56.590 --> 00:02:00.030
Well, OpenCL does the work under the hood
for you with the Intel Auto-Vectorizer.

00:02:00.370 --> 00:02:03.860
Your code will be compiled to
fully utilize the vector unit,

00:02:03.990 --> 00:02:08.190
and you will get a performance
boost without doing any extra work.

00:02:08.310 --> 00:02:11.930
OpenCL will also take care
of scheduling the work across

00:02:11.930 --> 00:02:13.720
multiple CPU cores for you.

00:02:14.180 --> 00:02:17.810
So this is of course a very brief
discussion of what is OpenCL and

00:02:17.810 --> 00:02:19.520
what makes it so powerful.

00:02:19.610 --> 00:02:22.760
If you're new to OpenCL,
if you need more introductory

00:02:22.920 --> 00:02:25.710
level information,
at the end of this talk we will refer

00:02:25.770 --> 00:02:27.680
you to some additional resources.

00:02:27.690 --> 00:02:33.200
But for now, let's just take a look at
what's on the agenda today.

00:02:33.200 --> 00:02:36.160
We have some new and very exciting
things for you in Mountain Lion.

00:02:36.500 --> 00:02:39.010
First,
I will talk about what's new in 1.2,

00:02:39.070 --> 00:02:42.530
OpenCL 1.2,
and then Sion Berkoviets from Intel will

00:02:42.610 --> 00:02:47.340
talk about the improvements to the
Intel Auto-Vectorizer in Mountain Lion.

00:02:47.340 --> 00:02:50.390
And then my colleague Eric will
talk about taking C code and turning

00:02:50.390 --> 00:02:53.200
it into optimized OpenCL code.

00:02:53.440 --> 00:02:57.040
and then his talk will be followed by
a presentation by Russell Williams and

00:02:57.040 --> 00:02:58.630
David McGavin from Adobe.

00:02:58.750 --> 00:03:01.270
They will talk about how they
use OpenCL in the recently

00:03:01.270 --> 00:03:03.400
released Adobe Creative Suite 6.

00:03:03.530 --> 00:03:06.220
So, let's get started.

00:03:06.530 --> 00:03:09.280
The first 1.2 feature I will
talk about has to do with the

00:03:09.360 --> 00:03:11.470
way you compile OpenCL programs.

00:03:11.500 --> 00:03:15.000
But before I tell you exactly what it is,
let's just quickly review how you

00:03:15.000 --> 00:03:17.510
usually compile OpenCL programs.

00:03:17.670 --> 00:03:19.840
You have two options right now.

00:03:19.900 --> 00:03:21.600
First one is online compilation.

00:03:21.650 --> 00:03:24.860
So imagine you wrote some
OpenCL program and you need to compile

00:03:24.980 --> 00:03:26.910
it for both the CPU and the GPU.

00:03:27.020 --> 00:03:29.930
To do this, at runtime,
pass this OpenCL program

00:03:30.190 --> 00:03:33.840
to the OpenCL compiler as a
plain text character string.

00:03:33.880 --> 00:03:38.170
and then the compiler will build it for
you into device-specific executables.

00:03:38.960 --> 00:03:41.340
Your second option is
offline compilation.

00:03:41.370 --> 00:03:45.650
This option is specifically for those
of you who prefer to not ship source.

00:03:45.790 --> 00:03:48.640
In this scenario,
you first use the offline OpenCL compiler

00:03:49.320 --> 00:03:51.840
to compile the OpenCL code into bitcode.

00:03:51.880 --> 00:03:55.260
And then at runtime, instead of loading
plain text source code,

00:03:55.340 --> 00:03:58.040
just pass the bitcode to the
compiler and the compiler will build

00:03:58.100 --> 00:04:00.800
device-specific executables for you.

00:04:00.890 --> 00:04:03.140
And let me show you
exactly how to do this.

00:04:03.190 --> 00:04:07.360
You can find the standalone
OpenCL compiler in the OpenCL framework.

00:04:07.360 --> 00:04:08.520
And this is the path.

00:04:08.750 --> 00:04:10.910
And this is the command
line we'd like you to use.

00:04:11.040 --> 00:04:15.750
Please use the arch flag to specify
which architectures you're building for.

00:04:16.530 --> 00:04:18.880
And, you know,
we've shown you this before,

00:04:18.880 --> 00:04:22.180
but what's new in Mountain Lion is
that we now guarantee that if you build

00:04:22.200 --> 00:04:26.070
your bitcode files in Mountain Lion,
you'll be able to run your programs

00:04:26.070 --> 00:04:28.440
on any OS starting with Mountain Lion.

00:04:28.590 --> 00:04:31.790
And in Mountain Lion,
we're also bringing the traditional

00:04:31.790 --> 00:04:34.360
compilation and linking model to OpenCL.

00:04:34.500 --> 00:04:38.660
And one of the cool things this allows
you to do is to create OpenCL libraries.

00:04:38.790 --> 00:04:42.510
And you can use these libraries
in other OpenCL programs.

00:04:43.020 --> 00:04:43.790
So how do you do that?

00:04:43.900 --> 00:04:45.470
How do you create an OpenCL library?

00:04:45.640 --> 00:04:47.500
Well, just as you would expect.

00:04:47.610 --> 00:04:50.550
Imagine you have some OpenCL source code,
for example,

00:04:50.650 --> 00:04:54.050
a collection of image processing filters,
several collections of

00:04:54.050 --> 00:04:56.610
image processing filters,
and you'd like to create a

00:04:56.610 --> 00:04:58.240
library of these filters.

00:04:58.380 --> 00:05:01.810
So you just compile your code
into intermediate object files

00:05:01.840 --> 00:05:03.640
and then link them together.

00:05:03.800 --> 00:05:06.790
And then during the linking stage,
you have two options.

00:05:06.970 --> 00:05:10.220
The first one is to generate
device-specific executables,

00:05:10.310 --> 00:05:13.710
as before,
or you can create an OpenCL library.

00:05:14.470 --> 00:05:17.400
So how do you use this library
in other OpenCL programs?

00:05:17.500 --> 00:05:20.280
Well, once again,
just as you would expect,

00:05:20.320 --> 00:05:23.010
just include a header file
for this OpenCL library in

00:05:23.010 --> 00:05:30.070
your new OpenCL program,
compile the code for this program,

00:05:30.070 --> 00:05:30.070
and then during the linking stage,
link in your library.

00:05:30.410 --> 00:05:34.590
and then tell the compiler to generate
device-specific executables for you.

00:05:34.710 --> 00:05:36.800
And that's it, you're done.

00:05:36.870 --> 00:05:38.240
So that's it for this feature.

00:05:38.360 --> 00:05:40.850
What else is new in Mountain Lion?

00:05:41.010 --> 00:05:44.500
You're now also able to call
printf within your kernels.

00:05:44.860 --> 00:05:48.310
When the kernel finishes executing,
the output of all the printf calls

00:05:48.310 --> 00:05:50.020
will be flashed to standard out.

00:05:50.110 --> 00:05:53.580
But there's one thing you should be
aware of when using printf in OpenCL.

00:05:53.650 --> 00:05:56.640
If you have many work items
executing in parallel and all

00:05:56.640 --> 00:06:00.440
of them are calling printf,
the order of the printf output for the

00:06:00.630 --> 00:06:03.100
different work items is not guaranteed.

00:06:03.150 --> 00:06:05.580
So, for example,
you can't expect that the work

00:06:05.580 --> 00:06:08.890
item with the global IT zero
will print to standard out first.

00:06:08.930 --> 00:06:10.430
Let's look at an example.

00:06:10.700 --> 00:06:13.180
Imagine you have a whole bunch of
work items executing in parallel

00:06:13.660 --> 00:06:17.000
and all of them are computing an
index into some global data buffer.

00:06:17.040 --> 00:06:21.250
And you're trying to use printf to
print out these indices to standard out.

00:06:21.460 --> 00:06:23.700
And this is the kind
of output you will see.

00:06:23.780 --> 00:06:25.960
So if you look at this,
this is not very helpful.

00:06:26.150 --> 00:06:27.250
What is going on here?

00:06:27.390 --> 00:06:29.370
Which work item is
outputting which index?

00:06:29.520 --> 00:06:30.700
You don't know.

00:06:30.770 --> 00:06:34.480
So we recommend that you always print
out the global work item ID along with

00:06:34.480 --> 00:06:36.490
the data that you're printing out.

00:06:36.560 --> 00:06:38.470
And this way you can really
figure out what's going on.

00:06:40.450 --> 00:06:45.340
Another cool thing about Printf in
OpenCL is that it supports vector types.

00:06:45.410 --> 00:06:50.140
So here are a few examples of how you
can use Printf to output vector data.

00:06:50.200 --> 00:06:53.990
Please note the vector specifier V,
which is followed by number 4.

00:06:54.070 --> 00:06:57.740
So this number stands for the size
of the vector you're printing out.

00:06:57.790 --> 00:07:00.800
And here is the output for
these different Printf calls.

00:07:00.910 --> 00:07:03.700
Please note that the data for
the different vector components

00:07:03.700 --> 00:07:05.610
is separated by commas.

00:07:05.730 --> 00:07:07.700
So this is it for Printf.

00:07:07.740 --> 00:07:09.960
What else is new in Mountain Lion?

00:07:10.990 --> 00:07:15.520
In Lion, you were only able to overload
built-in OpenCL functions.

00:07:15.520 --> 00:07:18.100
And now we enable you to overload
functions that you write.

00:07:18.260 --> 00:07:20.100
Just use the overload attribute.

00:07:20.180 --> 00:07:22.570
So imagine you're working on
a function that graphs data.

00:07:22.710 --> 00:07:26.320
And you can do something like this
to have one version that handles

00:07:26.320 --> 00:07:30.000
integer input and another version
that handles floating point data.

00:07:30.120 --> 00:07:32.230
Or you can use the overload attribute.

00:07:32.380 --> 00:07:35.290
And this will make your code
more clear and more readable.

00:07:35.400 --> 00:07:39.350
Also, when you're using OpenCL,
it's always better if you give us

00:07:39.350 --> 00:07:43.070
more information about what you're
doing so that we can possibly give

00:07:43.070 --> 00:07:45.400
you better performance under the hood.

00:07:45.490 --> 00:07:47.450
So for example,
there's some commonly used

00:07:47.450 --> 00:07:49.300
operations such as filling memory.

00:07:49.360 --> 00:07:51.800
You're probably using
kernels to do this right now.

00:07:51.970 --> 00:07:55.560
In Mountain Lion, we also have some new
convenient APIs for you.

00:07:55.800 --> 00:07:59.410
So one to fill a buffer with
a pattern and another one to

00:07:59.410 --> 00:08:01.420
fill an image with a color.

00:08:01.550 --> 00:08:03.600
If you use these APIs,
we know what you're doing.

00:08:03.600 --> 00:08:05.200
We can possibly give you
really good performance.

00:08:05.200 --> 00:08:07.020
performance.

00:08:07.830 --> 00:08:12.140
also give us more information about
how you're using your memory objects.

00:08:12.140 --> 00:08:15.090
So for example,
if you know that you're only going to

00:08:15.090 --> 00:08:20.430
be writing to a memory object from host,
tell us with this flag and we will know

00:08:20.430 --> 00:08:25.650
that the data for this memory object
does not need to be cached because

00:08:25.740 --> 00:08:29.570
you're not going to be reading it,
we won't let you,

00:08:29.590 --> 00:08:30.710
and the cache can be used more
efficiently for other memory objects

00:08:30.710 --> 00:08:30.710
that you will be reading or writing.

00:08:31.530 --> 00:08:36.870
Also, if you are working on modifying
a memory object on the GPU and

00:08:36.870 --> 00:08:40.740
you're also enqueuing a map
command to modify it on the CPU,

00:08:40.790 --> 00:08:44.120
and that you know that right now
you're using this map command to

00:08:44.210 --> 00:08:45.880
overwrite the data in the mapped region.

00:08:45.920 --> 00:08:47.380
So tell us with this flag.

00:08:47.380 --> 00:08:50.380
And we will know that if the most
up-to-date data is on the GPU,

00:08:50.450 --> 00:08:53.620
we don't need to copy it to the
CPU as we would normally do because

00:08:53.620 --> 00:08:55.450
you're just going to overwrite it.

00:08:55.560 --> 00:08:57.420
It's just going to be
a waste of resources.

00:08:57.420 --> 00:09:00.640
And finally,
we've deprecated some APIs on you.

00:09:00.920 --> 00:09:01.940
We replaced them with new ones.

00:09:01.940 --> 00:09:05.490
So check the new headers,
use the new APIs, and if you forget,

00:09:05.490 --> 00:09:07.510
the compiler will remind you.

00:09:07.520 --> 00:09:10.380
So this is all for
what's new in OpenCL 1.2.

00:09:10.380 --> 00:09:14.010
And now I will invite Sion Berković
to talk about the improvements to the

00:09:14.080 --> 00:09:16.280
Intel Auto-Vectorizer in Mountain Lion.

00:09:18.640 --> 00:09:19.960
So hi, everybody.

00:09:19.960 --> 00:09:21.550
I'm Sion.

00:09:21.720 --> 00:09:25.160
I work at Intel on developing
the auto-vectorizer,

00:09:25.160 --> 00:09:28.240
which I'm going to tell you about now.

00:09:28.330 --> 00:09:33.000
So I'm going to start with a short
recap of what the auto-vectorizer is.

00:09:33.000 --> 00:09:35.710
Then I'll tell you about
what's new in Mountain Lion.

00:09:35.800 --> 00:09:39.860
I'll explain a little bit about how
the auto-vectorizer actually works.

00:09:40.030 --> 00:09:44.160
And I'll give a few tips about
how to write code efficiently in

00:09:44.160 --> 00:09:46.010
the presence of the vectorizer.

00:09:46.010 --> 00:09:47.840
Then I'll show a short demo.

00:09:48.530 --> 00:09:52.800
So if you developed already
some OpenCL code for the CPU,

00:09:52.800 --> 00:09:57.400
you must have seen that getting
performance out of it is not so trivial.

00:09:57.440 --> 00:10:02.080
It requires adding some optimizations
which are target specific,

00:10:02.140 --> 00:10:07.860
most notably dealing
with vectors efficiently.

00:10:18.400 --> 00:10:23.170
It also loses its
performance portability,

00:10:23.170 --> 00:10:28.630
meaning that different targets
or different architectures

00:10:28.640 --> 00:10:34.730
may require somewhat different
optimizations to be really optimal.

00:10:35.030 --> 00:10:37.950
So that's why we introduced
the auto-vectorizer.

00:10:37.950 --> 00:10:42.260
It's basically a CPU compiler
optimization for OpenCL,

00:10:42.290 --> 00:10:45.580
which was introduced in OpenCL for Lion.

00:10:45.650 --> 00:10:52.800
And it basically adds for you the
optimization to use the vector registers

00:10:52.800 --> 00:10:56.970
or SIMD registers in the architecture.

00:10:57.410 --> 00:11:02.070
The way this works is the auto-vectorizer
basically packs together several

00:11:02.170 --> 00:11:04.890
work items into the vector registers.

00:11:05.120 --> 00:11:13.100
Each vector lane represents a
different work item of your work.

00:11:13.720 --> 00:11:18.480
Some of the features of the
auto-vectorizer as existed in Lion is,

00:11:18.820 --> 00:11:21.540
again, it runs by default when
you compile for the CPU.

00:11:21.540 --> 00:11:25.310
It does not require any user
intervention to actually run.

00:11:25.600 --> 00:11:28.900
It works well in the presence of
both scalar and vector operations

00:11:28.920 --> 00:11:30.840
that the user added to the kernel.

00:11:31.060 --> 00:11:34.910
However, it only worked in the
absence of control flow.

00:11:35.250 --> 00:11:40.340
So, introducing the new
auto-vectorizer in Mountain Lion.

00:11:40.340 --> 00:11:46.240
The major addition that we did was the
introduction of support for control flow.

00:11:46.480 --> 00:11:51.790
Now when I say control flow,
I mean more or less all the known styles,

00:11:51.990 --> 00:11:58.710
if-then-else blocks, loops, for loops,
while loops, every nested form of those.

00:11:58.860 --> 00:12:02.650
Also, the vectorizer automatically
optimizes the code to the

00:12:02.650 --> 00:12:07.020
underlying CPU architecture,
and by that providing significant

00:12:07.130 --> 00:12:10.400
speedup compared to non-vectorized code.

00:12:10.490 --> 00:12:13.380
However,
the speedup itself may vary depending

00:12:13.380 --> 00:12:15.810
on the use of control flow in the code.

00:12:15.940 --> 00:12:18.970
Now, to explain why that is,
I'll go now into some more details

00:12:19.380 --> 00:12:23.370
of how the auto-vectorizer actually
works dealing with control flow.

00:12:23.720 --> 00:12:29.500
I start with a simple example
of an if-then-else block.

00:12:29.500 --> 00:12:34.060
So the problem with standard if-then-else
code is that different work items

00:12:34.060 --> 00:12:38.380
of your open-series application may
choose a different path in the code.

00:12:38.380 --> 00:12:42.140
And since the vectorizer
tries to pack everything,

00:12:42.250 --> 00:12:45.430
several work items into
a single instruction,

00:12:45.430 --> 00:12:47.330
then it has a problem there.

00:12:47.330 --> 00:12:51.720
How can it pack it when they are
going in different directions?

00:12:51.720 --> 00:12:55.220
And the way this should work is
that the auto vectorizer should

00:12:55.220 --> 00:13:01.140
basically execute both the then
and the else side of the statement.

00:13:01.230 --> 00:13:04.170
And to do this,
the vectorizer serializes the

00:13:04.320 --> 00:13:06.480
code and executes both sides.

00:13:06.480 --> 00:13:11.110
However, unneeded calculations along the
way are disposed and in the case of

00:13:11.290 --> 00:13:15.730
instructions with side effects such
as every kind of memory access,

00:13:15.730 --> 00:13:19.680
those instructions are avoided.

00:13:19.680 --> 00:13:19.680
So, in this case,
the vectorizer is not able

00:13:19.680 --> 00:13:19.680
to execute the instructions.

00:13:19.680 --> 00:13:19.680
So, in this case,
the vectorizer is not able

00:13:19.680 --> 00:13:19.680
to execute the instructions.

00:13:19.680 --> 00:13:19.680
So, in this case,
the vectorizer is not able

00:13:19.680 --> 00:13:19.680
to execute the instructions.

00:13:19.680 --> 00:13:19.680
So, in this case,
the vectorizer is not able

00:13:19.680 --> 00:13:19.680
to execute the instructions.

00:13:19.680 --> 00:13:19.680
So, in this case,
the vectorizer is not able

00:13:19.680 --> 00:13:19.680
to execute the instructions.

00:13:19.680 --> 00:13:19.680
So, in this case,
the vectorizer is not able

00:13:19.680 --> 00:13:19.680
to execute the instructions.

00:13:19.680 --> 00:13:21.190
So, in this case, the vectorizer avoided.

00:13:21.870 --> 00:13:25.140
However,
there is one case of uniform control

00:13:25.140 --> 00:13:29.730
flow in which case all the work items
go in the exact same direction and

00:13:29.910 --> 00:13:34.670
this is detectable during compilation
and then the code itself is not

00:13:34.670 --> 00:13:39.070
modified as it works just as is.

00:13:39.360 --> 00:13:47.780
Now, in the case of loops,
we have a slightly different problem.

00:13:47.780 --> 00:13:50.950
Different work items may actually
iterate over a loop for a

00:13:50.950 --> 00:13:53.120
different amount of iterations.

00:13:53.120 --> 00:13:56.640
In the example here,
you can see that you have a loop

00:13:56.680 --> 00:14:02.090
where every work item basically
iterates the amount equal to its ID.

00:14:02.100 --> 00:14:05.660
So how can you pack several
work items into instructions

00:14:05.980 --> 00:14:07.800
when you have this thing?

00:14:07.800 --> 00:14:13.900
So, basically what the auto-vectorizer
does in the case of loops,

00:14:13.900 --> 00:14:18.430
it iterates over the loop for
an amount which is enough for

00:14:18.430 --> 00:14:21.860
all the work items it packed,
and it continues iterating until all

00:14:21.860 --> 00:14:28.190
the work items finish their work,
and it will only continue executing past

00:14:28.330 --> 00:14:35.100
the loop after all the work items have
finished doing their loop iterations.

00:14:40.150 --> 00:14:44.370
So now I'll give a couple of tips
for programming efficiently in the

00:14:44.370 --> 00:14:47.170
presence of the auto-vectorizer.

00:14:47.200 --> 00:14:50.440
So one thing is about memory
access in control flow.

00:14:50.450 --> 00:14:54.690
So as I said earlier,
memory accesses in control flow

00:14:54.690 --> 00:14:59.100
need to be masked because some
work items may have to avoid them.

00:14:59.110 --> 00:15:07.000
So adding this masking adds overhead
and basically penalizes performance.

00:15:07.730 --> 00:15:14.120
So the recommendation is avoid as much
as possible having the explicit memory

00:15:14.120 --> 00:15:15.910
accesses inside the control flow.

00:15:16.120 --> 00:15:16.800
Try to move them out.

00:15:16.800 --> 00:15:21.440
As you can see in the small example here,
there is a memory write inside both

00:15:21.440 --> 00:15:27.730
the then and the else side of the if,
and that can be moved outside the

00:15:27.730 --> 00:15:33.070
control flow statement and replaced
with a temporary write to register.

00:15:34.400 --> 00:15:38.060
The second tip is about
row-wise data access.

00:15:38.130 --> 00:15:42.550
The way the auto-vectorizer works
is it packs together several work

00:15:42.630 --> 00:15:49.560
items which have consecutive indices,
global IDs, in global dimension zero.

00:15:49.620 --> 00:15:53.850
That means that when the
kernel accesses array elements,

00:15:53.850 --> 00:15:57.250
for example,
it is preferred to access consecutive

00:15:57.250 --> 00:15:59.990
array elements in consecutive work items.

00:16:00.230 --> 00:16:06.570
As you can see in the small example here,
access to array A is consecutive,

00:16:06.570 --> 00:16:10.050
while in the case of array B,
the access is trided.

00:16:10.060 --> 00:16:12.890
That means that when
the code is vectorized,

00:16:13.000 --> 00:16:16.430
the vectorizer can basically
emit a single vector load

00:16:16.430 --> 00:16:17.930
in the case of array A.

00:16:18.110 --> 00:16:22.460
However, in the case of array B,
it has to do several scalar loads and

00:16:22.490 --> 00:16:25.370
gather the results to a vector register.

00:16:25.980 --> 00:16:30.420
So now I'll show a short example,
the new vectorizer.

00:16:30.420 --> 00:16:34.960
So I start with more or less
the same example that we showed

00:16:34.960 --> 00:16:37.420
last year for the vectorizer.

00:16:37.420 --> 00:16:41.840
This is nine high-definition
movies running in parallel,

00:16:41.840 --> 00:16:45.360
and we apply some image filters on them.

00:16:45.360 --> 00:16:49.260
Now, as you can see on the frames
per second bar on the top,

00:16:49.260 --> 00:16:54.440
the frame per second rate
doesn't decrease too much in

00:16:54.570 --> 00:16:57.210
the presence of these filters.

00:16:57.430 --> 00:17:01.480
However, this time around,
we added a new filter,

00:17:01.480 --> 00:17:05.150
which is a wave simulation,
which is a kind of physics simulator.

00:17:05.340 --> 00:17:09.030
Now, this contains a lot of control flow,
and the old vectorizer

00:17:09.030 --> 00:17:10.350
does not work on it.

00:17:10.350 --> 00:17:13.680
And as you can see,
performance goes down tremendously.

00:17:13.680 --> 00:17:17.590
If I try to use this,
you can see that the wave is

00:17:17.730 --> 00:17:20.470
slowly moving on the screen.

00:17:20.570 --> 00:17:23.860
If I do some more,
I can slow it even more.

00:17:23.860 --> 00:17:25.630
Now, with the new vectorizer,
we have a new filter.

00:17:25.970 --> 00:17:33.930
It's easy to -- you just get
speed up by vectorizing also the

00:17:33.930 --> 00:17:38.270
control flow in the new filter.

00:17:38.310 --> 00:17:44.760
And also, with this,
I can even turn on the rain feature,

00:17:44.760 --> 00:17:49.720
which basically does a whole
lot of raindrops altogether.

00:17:49.720 --> 00:17:54.920
And as you can see, even with that,
performance is hardly impacted at all.

00:18:00.950 --> 00:18:05.130
So to summarize,
the auto-vectorizer is an

00:18:05.400 --> 00:18:10.240
optimization that exists in
OpenCL running behind the scenes.

00:18:10.240 --> 00:18:12.340
You don't need to do
anything to turn it on.

00:18:12.400 --> 00:18:14.880
And it basically optimizes your code.

00:18:14.960 --> 00:18:18.470
And now in Mountain Lion,
it optimizes it even if it includes

00:18:18.470 --> 00:18:21.980
a complex control flow inside,
giving you performance.

00:18:22.130 --> 00:18:23.680
So thank you.

00:18:23.710 --> 00:18:29.180
And I'd like to invite Eric to
talk about OpenCL optimizations.

00:18:31.310 --> 00:18:32.960
Good afternoon.

00:18:32.960 --> 00:18:36.640
My name is Eric and today I will
tell you about OpenCL code tuning.

00:18:36.750 --> 00:18:38.800
Let me show you some numbers.

00:18:38.960 --> 00:18:42.230
So this has a running speed
of a Gaussian blur algorithm

00:18:42.230 --> 00:18:44.910
on a 16 million pixel image.

00:18:44.980 --> 00:18:48.010
I will tell you details
about this algorithm later.

00:18:48.140 --> 00:18:50.780
This first one is what
we get when we run it.

00:18:50.910 --> 00:18:54.660
So just a straight forward
implementation of it on the CPU.

00:18:54.790 --> 00:18:59.610
And then you take the same code and
port it to OpenCL and you get this.

00:18:59.810 --> 00:19:03.990
And after a few steps of tuning,
you can reach that.

00:19:04.140 --> 00:19:06.760
Yeah.

00:19:06.760 --> 00:19:10.700
And this is what I'm going
to tell you about now.

00:19:11.960 --> 00:19:13.090
Okay, let's go.

00:19:13.090 --> 00:19:16.610
So to do this we'll follow
this very simple program.

00:19:16.740 --> 00:19:20.180
Yeah, it reminds me when -- okay, anyway.

00:19:20.280 --> 00:19:23.770
So first we'll choose
a suitable algorithm.

00:19:23.770 --> 00:19:27.530
Then code it to OpenCL,
benchmark it and decide if

00:19:27.530 --> 00:19:29.120
it's fast enough or not.

00:19:29.120 --> 00:19:33.960
And if it's not fast enough,
just identify why and find a way to

00:19:34.030 --> 00:19:37.360
fix it and start all over to line 10.

00:19:37.360 --> 00:19:42.150
So first we need to know what
fast enough means and how to

00:19:42.150 --> 00:19:44.690
choose a suitable algorithm.

00:19:45.680 --> 00:19:48.070
So to know what is fast enough,
the simplest way is just to

00:19:48.070 --> 00:19:51.030
benchmark code on the GPU.

00:19:52.540 --> 00:19:54.610
And we will use very simple kernels.

00:19:54.710 --> 00:19:56.680
So this is what I call the copy kernel.

00:19:56.810 --> 00:20:00.910
It takes an image stored in a buffer.

00:20:01.210 --> 00:20:03.950
and puts it,
read it from the input buffer,

00:20:03.950 --> 00:20:08.000
the in pointer here and write
it to the output buffer.

00:20:08.000 --> 00:20:08.500
Okay.

00:20:08.500 --> 00:20:11.760
So, this function is run for
all pixels of the image.

00:20:11.760 --> 00:20:14.660
So, we will schedule 16 million of it.

00:20:14.660 --> 00:20:17.910
And the first thing we do
inside the function is to get

00:20:17.910 --> 00:20:20.000
which pixel we should process.

00:20:20.000 --> 00:20:20.980
This is X and Y.

00:20:20.980 --> 00:20:25.480
And then we just read one value,
one float value from the input buffer

00:20:25.480 --> 00:20:28.200
and write it to the output buffer.

00:20:28.200 --> 00:20:30.970
Benchmarking this,
we will get the best speed

00:20:30.970 --> 00:20:33.430
we can expect for the GPU.

00:20:33.610 --> 00:20:34.700
All right?

00:20:34.700 --> 00:20:37.040
And then there's a MAT kernel.

00:20:37.050 --> 00:20:39.470
MAT is for multiply plus add.

00:20:39.470 --> 00:20:44.960
So, we read the value like we did before
and do a few floating point operations

00:20:44.960 --> 00:20:49.010
on it like three here before storing
it back to the output buffer.

00:20:49.150 --> 00:20:51.970
And we'll change the number of
floating point operations we'll do.

00:20:52.090 --> 00:20:53.050
Okay.

00:20:53.050 --> 00:20:54.420
Let's benchmark

00:20:55.260 --> 00:20:59.960
Now, so this is on a Mac Pro AMD GPU.

00:20:59.990 --> 00:21:02.700
And this is what we
get with a copy kernel,

00:21:02.720 --> 00:21:05.110
like 13.5

00:21:05.100 --> 00:21:06.940
Giga pixels per second.

00:21:06.980 --> 00:21:10.660
OK, now let's add the
floating point operations.

00:21:11.040 --> 00:21:13.200
Okay, this is with three
floating-point operations.

00:21:13.200 --> 00:21:15.700
As you can see,
we get almost the same speed,

00:21:15.750 --> 00:21:19.740
which means that actually the
MAT kernel is memory-bound and we

00:21:19.740 --> 00:21:24.210
are still waiting for memory and the
floating-point operations are free.

00:21:24.350 --> 00:21:25.300
Okay, let's add more.

00:21:25.450 --> 00:21:27.450
Six.

00:21:27.460 --> 00:21:28.530
Same.

00:21:28.620 --> 00:21:30.180
Twelve.

00:21:31.420 --> 00:21:32.740
18.

00:21:32.780 --> 00:21:33.000
Okay.

00:21:33.000 --> 00:21:35.780
And then we start finally
to see something with 24

00:21:35.780 --> 00:21:38.340
floating point operations.

00:21:38.340 --> 00:21:43.650
So usually it means that what we want
for a suitable algorithm is to have

00:21:43.930 --> 00:21:46.400
first to be embarrassingly parallel.

00:21:46.710 --> 00:21:51.020
It means you process the same
thing for all pixels of the image.

00:21:51.020 --> 00:21:52.760
Then as we have seen,
usually memory will be

00:21:52.760 --> 00:21:53.740
the first bottleneck.

00:21:53.740 --> 00:21:56.120
So you want to minimize memory accesses.

00:21:56.280 --> 00:22:00.220
And then you want to increase
the compute to memory ratio.

00:22:00.460 --> 00:22:03.120
And then from this benchmark
we can get an estimate of the

00:22:03.120 --> 00:22:04.800
running speed of our code.

00:22:05.210 --> 00:22:09.580
Okay,
so now we are ready to choose algorithm.

00:22:09.580 --> 00:22:13.660
And I will present you some
Gaussian blur algorithm.

00:22:13.660 --> 00:22:17.110
This is actually the definition
of the Gaussian blur.

00:22:17.200 --> 00:22:21.990
So the cute green curve is a
Gaussian with a sigma equal five.

00:22:22.020 --> 00:22:29.680
And so to compute the convolution we
need in this case 31 by 31 filter.

00:22:29.740 --> 00:22:33.240
So that will at the end,
if you use this definition to compute it,

00:22:33.240 --> 00:22:38.120
we'll do almost 1,000 image
accesses and 2,000 floating

00:22:38.190 --> 00:22:39.710
point operations per pixel.

00:22:39.760 --> 00:22:40.880
This is huge.

00:22:40.930 --> 00:22:43.660
Fortunately, we have a lot of data

00:22:44.070 --> 00:22:46.100
The Gaussian filter is separable.

00:22:46.100 --> 00:22:49.390
It means that it's a product
of two one-dimensional filters.

00:22:49.710 --> 00:22:54.430
And in this case, we can compute the same
thing by processing two

00:22:54.620 --> 00:22:58.270
one-dimensional convolutions,
one horizontal on each row and

00:22:58.270 --> 00:23:01.400
one vertical on each column,
and we get the same thing.

00:23:01.460 --> 00:23:05.670
And the cost in memory
and flops is much lower.

00:23:06.300 --> 00:23:10.600
And then there is a third algorithm which
is called Recursive Gaussian Filter.

00:23:10.610 --> 00:23:14.260
This one does not compute the
exact Gaussian blur actually,

00:23:14.390 --> 00:23:16.500
but a good approximation of it.

00:23:16.650 --> 00:23:20.000
So it works sequentially on each
row and we have this kind of green

00:23:20.000 --> 00:23:22.280
spider moving from left to right.

00:23:22.360 --> 00:23:26.930
And the first pass will take the input,
the red line,

00:23:27.160 --> 00:23:30.560
and produce a temporary output,
the blue thing.

00:23:30.650 --> 00:23:34.350
And then it will come back
from the right to the left,

00:23:34.350 --> 00:23:37.520
read the input again and the blue
thing again and produce the output,

00:23:37.630 --> 00:23:40.500
the yellow curve.

00:23:41.070 --> 00:23:46.220
Okay, and this... Okay,
to apply this to a two-dimensional image,

00:23:46.220 --> 00:23:50.530
you will then need four passes,
that's spiders,

00:23:50.690 --> 00:23:55.450
so one spider for each row from left
to right and then right to left,

00:23:55.580 --> 00:23:56.740
and the same thing vertically.

00:23:56.760 --> 00:24:03.070
And so the final cost for this one is 10
memory accesses and 64 flops per pixel.

00:24:03.700 --> 00:24:05.600
Okay, this thing finishes?

00:24:05.600 --> 00:24:07.600
Okay, done.

00:24:07.750 --> 00:24:11.200
So I've showed you three
algorithms for Gaussian blur.

00:24:11.200 --> 00:24:16.070
I put them back all together on this
table with the copy kernel first.

00:24:16.240 --> 00:24:20.160
So the first column is the number
of memory accesses per pixel.

00:24:20.360 --> 00:24:23.840
Then you have the number of flaps
and the compute memory ratio.

00:24:23.900 --> 00:24:25.600
And the last column is very interesting.

00:24:25.600 --> 00:24:31.840
It's the estimated running speed
obtained from the copy kernel.

00:24:32.000 --> 00:24:35.120
So you see the last one
does five times more memory

00:24:35.120 --> 00:24:37.120
accesses than the copy kernel.

00:24:37.120 --> 00:24:39.280
So it's one-fifth of the speed.

00:24:39.410 --> 00:24:41.450
Actually,
this is the one we'll be implementing,

00:24:41.450 --> 00:24:42.480
obviously.

00:24:42.480 --> 00:24:44.070
It's supposed to be faster.

00:24:44.200 --> 00:24:46.380
Okay, let's do it.

00:24:46.670 --> 00:24:48.590
That's line 10.

00:24:49.200 --> 00:24:51.340
This is the code.

00:24:51.410 --> 00:24:55.780
I try to simplify it so the gray
lines... We will learn one of these

00:24:56.150 --> 00:24:58.800
functions for each row of the image.

00:24:58.910 --> 00:25:01.400
This is a horizontal path.

00:25:01.500 --> 00:25:04.040
And the first thing we do in
this kernel is to figure out

00:25:04.140 --> 00:25:06.260
which row we must update.

00:25:06.260 --> 00:25:07.500
This is a Y.

00:25:07.570 --> 00:25:11.890
And then we'll do a loop on X from
left to right and from right to left.

00:25:12.010 --> 00:25:15.860
At each step of the loop,
we load one input value

00:25:15.860 --> 00:25:18.490
and store one output value.

00:25:18.570 --> 00:25:21.500
And there's a backward loop below it.

00:25:21.880 --> 00:25:23.500
And this is a vertical kernel.

00:25:23.530 --> 00:25:29.000
It does exactly the same thing
but with X and Y exchange.

00:25:29.070 --> 00:25:33.290
So we run one of these for
each column of the image.

00:25:33.420 --> 00:25:34.390
Right, now we have the code.

00:25:34.400 --> 00:25:36.450
Let's benchmark it.

00:25:37.740 --> 00:25:41.700
That's 27 million pixels per second.

00:25:41.700 --> 00:25:43.630
Didn't I say 2800?

00:25:43.940 --> 00:25:48.300
Well, so clearly we have a --
something is slow here.

00:25:48.390 --> 00:25:51.350
So let's see in detail the
respective speeds of the two

00:25:51.350 --> 00:25:53.420
horizontal and vertical passes.

00:25:53.460 --> 00:25:58.400
This is vertical pass is quite fast,
which is almost 2 gigapixels per second.

00:25:58.540 --> 00:26:01.780
But we have an issue
with horizontal pass.

00:26:01.780 --> 00:26:04.050
It's this incredibly slow.

00:26:04.070 --> 00:26:05.260
So why?

00:26:06.000 --> 00:26:10.530
Usually the The usual suspect in
this case is a memory access pattern.

00:26:11.860 --> 00:26:17.600
So imagine inside the GPU you have a
schedule of 16 million functions to call,

00:26:17.630 --> 00:26:21.540
and they will be called by
groups of 300 at the same time.

00:26:21.700 --> 00:26:24.890
And you will have these 300
work items requesting memory

00:26:24.940 --> 00:26:27.040
access with a different address.

00:26:27.170 --> 00:26:30.190
This is what I call the
memory access pattern.

00:26:30.510 --> 00:26:33.830
And so the hardware is
optimized for certain accesses

00:26:33.930 --> 00:26:36.130
and it does it very fast.

00:26:36.310 --> 00:26:40.090
And in the other cases,
accesses will be conflicting and

00:26:40.090 --> 00:26:44.900
then they will be serialized and
running time will be very slow.

00:26:45.040 --> 00:26:48.310
So how to identify these patterns?

00:26:48.920 --> 00:26:52.080
We can show some simple rules.

00:26:52.140 --> 00:26:54.460
So this is a fast case.

00:26:55.260 --> 00:26:59.730
This is when each consecutive
work items will access consecutive

00:26:59.760 --> 00:27:01.960
addresses in the buffers.

00:27:02.030 --> 00:27:05.860
This is the case the
hardware goes very fast.

00:27:05.970 --> 00:27:06.530
We want this.

00:27:06.610 --> 00:27:09.720
And for image processing,
this is when consecutive

00:27:09.780 --> 00:27:13.900
work items access consecutive
pixels in the same row.

00:27:13.940 --> 00:27:15.130
And this is a slow case.

00:27:15.410 --> 00:27:18.790
It's almost the same,
but we access the buffer with a stride.

00:27:18.890 --> 00:27:21.410
Here it's 1024.

00:27:21.590 --> 00:27:24.490
So in this case,
all hardware requests will end

00:27:24.490 --> 00:27:28.740
up in the same memory channel or
bank and we'll have conflicts and

00:27:28.740 --> 00:27:30.250
the accesses will be serialized.

00:27:30.450 --> 00:27:31.940
It will be extremely slow.

00:27:32.080 --> 00:27:35.120
So in image processing,
this is when consecutive

00:27:35.160 --> 00:27:38.970
work items access consecutive
pixels in the same column.

00:27:39.480 --> 00:27:40.400
Right.

00:27:40.400 --> 00:27:43.350
So let's see how we can... Oh yeah.

00:27:43.760 --> 00:27:46.420
And then in the other cases,
you just need to benchmark the

00:27:46.420 --> 00:27:48.990
code to see in which case you are.

00:27:50.440 --> 00:27:52.880
So let's see if I can
identify the patterns in the

00:27:52.880 --> 00:27:54.890
kernels we have just seen.

00:27:54.890 --> 00:27:57.110
This is a copy kernel.

00:27:57.590 --> 00:27:59.960
So as you can see,
consecutive work items will

00:28:00.040 --> 00:28:01.740
have consecutive values of x.

00:28:01.790 --> 00:28:04.340
This is a global ID 0.

00:28:04.380 --> 00:28:08.430
And then in this case,
access to the input buffer will be fast

00:28:08.510 --> 00:28:13.250
because we access consecutive values,
consecutive addresses in the buffer.

00:28:13.540 --> 00:28:17.800
And the access to the
output buffer will be fast.

00:28:17.910 --> 00:28:21.450
So now let's look at the vertical path.

00:28:21.740 --> 00:28:25.130
In this case again,
consecutive worker times will

00:28:25.130 --> 00:28:29.490
have consecutive values of X,
and we will do fast accesses to

00:28:29.620 --> 00:28:31.870
the input and output buffers.

00:28:33.830 --> 00:28:36.930
Now,
let's have a look at the horizontal one.

00:28:37.110 --> 00:28:39.140
So in this case,
consecutive work items will

00:28:39.140 --> 00:28:42.220
have consecutive values of y.

00:28:42.430 --> 00:28:48.210
And as you can see, the stride for the
access is w in this case.

00:28:48.440 --> 00:28:52.800
So when w will be larger,
it will be in the slow case.

00:28:52.850 --> 00:28:56.410
This is what we have seen actually,
and this is why the

00:28:56.410 --> 00:28:58.560
horizontal kernel is slow.

00:28:58.590 --> 00:29:01.200
Okay,
so we have identified the bottleneck.

00:29:01.320 --> 00:29:02.790
Now, how do we solve it?

00:29:05.490 --> 00:29:08.000
Actually, there is a very simple
solution in this case,

00:29:08.000 --> 00:29:09.500
just to drop it.

00:29:09.570 --> 00:29:13.400
We can use the vertical one
and do transpose after it,

00:29:13.480 --> 00:29:17.100
and then transpose again
after another vertical pass,

00:29:17.250 --> 00:29:18.040
like this.

00:29:18.160 --> 00:29:23.440
So you do vertical pass, transpose,
do it again, and transpose.

00:29:23.560 --> 00:29:25.400
So if we assume we can
do a fast transpose,

00:29:25.490 --> 00:29:29.690
this is supposed to be faster
than the horizontal plus vertical,

00:29:29.690 --> 00:29:30.360
okay?

00:29:30.470 --> 00:29:33.110
So now we need a transpose kernel.

00:29:34.300 --> 00:29:38.400
Yeah, so we can update our
estimate of the running time.

00:29:38.400 --> 00:29:40.230
So we will do two more passes.

00:29:40.310 --> 00:29:42.810
Transpose will be run
with two memory accesses.

00:29:43.090 --> 00:29:45.880
We do that two times,
so that's 14 memory accesses.

00:29:46.230 --> 00:29:51.030
And the estimate drops to 2,000
million pixels per second.

00:29:51.730 --> 00:29:53.090
So how do we transpose?

00:29:53.190 --> 00:29:55.720
Actually,
transposing is just like copying.

00:29:55.890 --> 00:30:00.040
So we read x, y and start with y,
x instead of x, y again.

00:30:00.120 --> 00:30:03.500
So it's almost the same
kernel as a copy kernel.

00:30:04.040 --> 00:30:04.840
All right.

00:30:04.860 --> 00:30:07.730
And within this case,
the access to the input

00:30:07.730 --> 00:30:11.130
buffer will be fast,
but the access to the output

00:30:11.200 --> 00:30:15.190
buffer will be slow because it
will be done with a stride of H.

00:30:15.320 --> 00:30:17.540
So let's benchmark it.

00:30:17.540 --> 00:30:22.290
Okay, and we see exactly what we expect
that it's after edge becomes larger,

00:30:23.400 --> 00:30:25.850
will be slower.

00:30:26.470 --> 00:30:28.800
So how do we solve this?

00:30:29.000 --> 00:30:33.060
Another usual solution to this
issue is to move the problem

00:30:33.060 --> 00:30:35.080
to a faster memory level.

00:30:35.210 --> 00:30:41.540
Inside the GPU, we have processing cores,
that's the yellow boxes here,

00:30:41.540 --> 00:30:45.560
and each core has arithmetic units,
registers, local memory,

00:30:45.560 --> 00:30:48.550
and everything is connected
to a global memory.

00:30:48.770 --> 00:30:52.790
And then it's connected to the host,
the Mac.

00:30:53.640 --> 00:30:57.640
Okay, the speed of this connection
is 10 gigabytes per second.

00:30:57.650 --> 00:30:59.240
It's pretty slow actually.

00:30:59.240 --> 00:31:03.750
And then each layer of memory is 10
times faster than the previous one,

00:31:03.750 --> 00:31:04.780
more or less.

00:31:04.780 --> 00:31:08.560
So that would be 100 gigabytes
per second for the global memory,

00:31:08.560 --> 00:31:12.570
1,000 for the local one,
and 10,000 for registers.

00:31:12.580 --> 00:31:15.550
So morality of this,
when your data is moved

00:31:15.650 --> 00:31:20.200
to the upper levels,
you want to reuse it as much as you can.

00:31:20.200 --> 00:31:23.860
And also you want to avoid
host-to-device transfers.

00:31:23.860 --> 00:31:26.520
So what we will do is move our
problem to the local memory.

00:31:28.490 --> 00:31:29.650
Just like this.

00:31:29.780 --> 00:31:34.360
So we'll have a workgroup,
a block of work items,

00:31:34.410 --> 00:31:39.790
loading a small block of the image,
storing it in local memory,

00:31:40.260 --> 00:31:44.020
And then when all the work items in
the group are finished doing that,

00:31:44.040 --> 00:31:47.170
moving it back again
to the output buffer.

00:31:48.060 --> 00:31:50.540
Okay, let's write the code for this.

00:31:50.640 --> 00:31:53.340
It becomes a little longer,
but it's not very complicated.

00:31:53.340 --> 00:31:58.240
BX and BY will be the workgroup
coordinates and they will be mapped

00:31:58.360 --> 00:32:00.700
to BX/BY blocks in the image.

00:32:00.810 --> 00:32:07.800
And IX and IY are the pixel
coordinates inside the block.

00:32:07.900 --> 00:32:12.440
And then we'll just... Each work item
will load one value from the input buffer

00:32:12.440 --> 00:32:16.180
to the temporary local memory buffer,
this hoax.

00:32:16.470 --> 00:32:19.250
Then we'll wait for all
work items to do that.

00:32:19.380 --> 00:32:21.050
That's a barrier call.

00:32:21.320 --> 00:32:25.860
And then we'll store the temporary
buffer back to global memory output.

00:32:25.950 --> 00:32:30.710
And in this case,
both accesses to global memory are fast.

00:32:32.020 --> 00:32:35.680
Because consecutive work at times
will have consecutive value of ix.

00:32:35.700 --> 00:32:37.560
Okay, let's benchmark it.

00:32:37.560 --> 00:32:38.360
This might be faster.

00:32:40.100 --> 00:32:41.300
Oh, sabotage.

00:32:41.620 --> 00:32:44.440
It's not fast at all.

00:32:44.580 --> 00:32:46.610
So yeah, why?

00:32:46.690 --> 00:32:51.220
We must now consider globally
what happens inside the GPU.

00:32:51.350 --> 00:32:55.580
So we have all these workgroups,
and each workgroup will be

00:32:55.580 --> 00:32:57.340
mapped to a core inside the CPU.

00:32:57.340 --> 00:33:01.060
Let's say if you have 10 cores,
the first 10 workgroups will be

00:33:01.060 --> 00:33:04.300
executed by the 10 cores together.

00:33:04.420 --> 00:33:06.690
So they will all do the
input accesses together.

00:33:06.900 --> 00:33:09.700
And it will be fine,
because it will be on the same rows.

00:33:09.780 --> 00:33:13.570
But then, when they want to store the
result in the output buffer,

00:33:13.950 --> 00:33:17.200
they want to access the same columns.

00:33:17.230 --> 00:33:20.100
And they will be
conflicting in this case.

00:33:20.100 --> 00:33:22.940
So how do we solve this?

00:33:23.350 --> 00:33:27.760
We can just change how the workgroups
are mapped to the image blocks.

00:33:27.960 --> 00:33:33.240
Instead of mapping them by rows,
we'll just map them diagonally like this.

00:33:33.430 --> 00:33:35.820
In this case,
the accesses to the input buffer

00:33:35.820 --> 00:33:39.860
will be fast and so will the
accesses to the output buffer.

00:33:40.010 --> 00:33:43.840
Actually, the change in the code to
do this is pretty small.

00:33:43.840 --> 00:33:46.540
It's just adding one line.

00:33:46.930 --> 00:33:50.160
You just change how the
world groups coordinates are

00:33:50.210 --> 00:33:51.940
mapped to the image blocks.

00:33:52.040 --> 00:33:55.000
Okay, now let's benchmark it.

00:33:55.110 --> 00:33:56.280
Ah, now it's faster.

00:33:56.490 --> 00:33:57.920
Good.

00:33:58.610 --> 00:34:00.400
Right,
so let's take this transpose kernel

00:34:00.400 --> 00:34:05.110
and with the vertical recursive
Gaussian kernel and put that,

00:34:05.260 --> 00:34:07.100
everything together.

00:34:07.150 --> 00:34:09.310
And this is what we get.

00:34:09.800 --> 00:34:10.700
All right.

00:34:10.720 --> 00:34:16.400
We can reach something like
650 million pixels per second.

00:34:16.450 --> 00:34:18.960
You tell me, didn't he say 2,000?

00:34:19.120 --> 00:34:20.550
Yes.

00:34:20.730 --> 00:34:23.800
So, something is still slow here.

00:34:23.910 --> 00:34:27.670
The issue we have now is we
are running only not enough

00:34:27.860 --> 00:34:30.040
work items to saturate the GPU.

00:34:30.100 --> 00:34:33.600
We are running one work item
for each column of the image.

00:34:33.680 --> 00:34:37.260
And that will be something
like thousands of work items.

00:34:37.430 --> 00:34:42.210
But actually, it's not enough to hide all
latencies inside the GPU.

00:34:42.310 --> 00:34:46.290
So, we need to find another
algorithm using more work items.

00:34:46.380 --> 00:34:49.390
But this will be a
subject for another talk.

00:34:49.510 --> 00:34:51.610
So, let me summarize.

00:34:51.940 --> 00:34:56.360
So actually this simple procedure and
we have followed it and in a few steps

00:34:56.500 --> 00:34:59.640
we have obtained a significant speedup.

00:34:59.700 --> 00:35:01.730
I just want to add a few things to this.

00:35:02.030 --> 00:35:06.360
After you write the code,
just check the output you get with

00:35:06.370 --> 00:35:11.490
reference implementations because
otherwise it's quite easy to get much

00:35:11.490 --> 00:35:14.390
faster but with the wrong output.

00:35:14.700 --> 00:35:18.120
So yeah, you need a reference
implementation somewhere.

00:35:18.170 --> 00:35:20.850
And second thing,
don't give up at the first try.

00:35:21.200 --> 00:35:25.590
Usually with very simple changes,
you can get much faster code.

00:35:25.690 --> 00:35:28.600
Well, thank you.

00:35:28.600 --> 00:35:32.580
I will invite on the scene Russell.

00:35:35.020 --> 00:35:37.020
Hi, my name is Russell Williams.

00:35:37.020 --> 00:35:38.500
I'm one of the architects on Photoshop.

00:35:38.500 --> 00:35:42.590
And I'm going to show you how we used
OpenCL to speed up one of the cool

00:35:42.780 --> 00:35:45.700
features in the new Photoshop CS6.

00:35:45.700 --> 00:35:47.910
I'll just start off with a demo here.

00:35:48.140 --> 00:35:50.490
And what this feature is,
is it's a blur gallery.

00:35:50.490 --> 00:35:55.110
It's a collection of creative
blur effects like tilt shift,

00:35:55.320 --> 00:35:56.600
which is a popular effect now.

00:35:56.960 --> 00:35:59.940
What I'm going to show you right
now is called a field blur,

00:35:59.940 --> 00:36:04.220
where you put pins on the image
and adjust how blurry you want that

00:36:04.220 --> 00:36:06.700
general area of the image to be.

00:36:06.700 --> 00:36:12.010
And the blur filter interpolates
between those things.

00:36:12.200 --> 00:36:14.650
And first I'm going to run it on the CPU.

00:36:14.730 --> 00:36:21.200
And this is a hand-optimized
SSE hand-threaded CPU implementation.

00:36:21.200 --> 00:36:25.100
And you can see we used all 24 threads,
all 12 cores on that.

00:36:25.200 --> 00:36:29.830
And it took 5.2 seconds
down there in the corner.

00:36:30.200 --> 00:36:42.090
And if I just go in here and
turn on OpenCL and rerun that,

00:36:43.220 --> 00:36:48.600
didn't even get a progress bar,
1.8 seconds.

00:36:49.400 --> 00:36:55.030
So that was a pretty good speedup
and that's against the very fastest

00:36:55.330 --> 00:36:57.930
12-core Mac there is on the CPU side.

00:36:58.070 --> 00:36:59.300
So that's really good.

00:36:59.300 --> 00:37:00.150
What was it?

00:37:00.150 --> 00:37:05.390
That was about a 200 line,
just a very small OpenCL kernel,

00:37:05.460 --> 00:37:09.300
not doing a Gaussian blur but
simulating lens optics of a blur.

00:37:09.300 --> 00:37:15.300
It gives a much more aesthetically
pleasing and realistic result.

00:37:15.490 --> 00:37:20.640
And we broke the image into 2K by 2K,
4 megapixel blocks in order to fit an

00:37:20.640 --> 00:37:26.090
arbitrarily big image on the GPU and
I'll talk about that again in a second.

00:37:26.610 --> 00:37:28.100
So why did we use OpenCL for this?

00:37:28.100 --> 00:37:30.900
We've done a lot of blur filters
and other things before and

00:37:30.900 --> 00:37:33.320
never used OpenCL in the past
and now we're starting to use it.

00:37:33.500 --> 00:37:36.780
Well, first of all,
we wanted to use the GPU and

00:37:36.780 --> 00:37:41.500
OpenCL is the only cross-platform
GPGPU solution available.

00:37:41.720 --> 00:37:46.700
There are other GPGPU languages
like NVIDIA's CUDA,

00:37:46.850 --> 00:37:54.010
but they aren't cross-platform
and for us that's a deal killer.

00:37:54.820 --> 00:37:56.690
Why didn't we use OpenGL?

00:37:56.810 --> 00:38:00.480
Well, we've used OpenGL in the past
and we will use OpenGL for more

00:38:00.480 --> 00:38:02.500
Photoshop features in the future.

00:38:02.660 --> 00:38:06.700
But OpenGL is fundamentally
a 3D rendering package,

00:38:06.700 --> 00:38:09.000
a geometry rendering package.

00:38:09.090 --> 00:38:13.980
And in order to do something in OpenGL,
you have to recast your

00:38:14.020 --> 00:38:15.800
problem into those terms.

00:38:15.920 --> 00:38:17.500
And it's got a steep learning curve.

00:38:17.500 --> 00:38:20.970
You've got to get your head around
all of that 3D rendering stuff

00:38:21.080 --> 00:38:23.890
before you can write your algorithm.

00:38:23.900 --> 00:38:27.140
And if all you've got is a matrix
of numbers and you want to run

00:38:27.140 --> 00:38:30.100
an algorithm on them and you
get a matrix of numbers out,

00:38:30.300 --> 00:38:36.800
OpenCL is just far simpler,
both to write and to learn.

00:38:37.020 --> 00:38:41.050
Also, in the last year,
I said portability or cross-platform

00:38:41.050 --> 00:38:42.700
is very important to us.

00:38:42.880 --> 00:38:47.780
And OpenCL really hit a new level of
maturity and ubiquity in the last year.

00:38:48.120 --> 00:38:52.720
It's now shipped as part of the
standard user mode install drivers

00:38:53.160 --> 00:38:58.160
on the Windows side for NVIDIA,
AMD, and Intel integrated graphics.

00:38:58.480 --> 00:39:02.870
Now, I originally titled
this OpenCL challenges,

00:39:02.870 --> 00:39:05.150
but really they're
challenges for anything,

00:39:05.160 --> 00:39:06.700
doing anything on the GPU.

00:39:06.790 --> 00:39:10.200
And we were just using
OpenCL on the GPU this time.

00:39:10.200 --> 00:39:14.600
We did our traditional
hand-coded version on the CPU.

00:39:14.720 --> 00:39:17.600
So the challenges on the GPU are,
first of all,

00:39:17.600 --> 00:39:22.300
you need an algorithm that takes
advantage of one of the two things that

00:39:22.300 --> 00:39:22.300
the GPU can do better than the GPU.

00:39:22.300 --> 00:39:28.100
One is a bunch of compute,
and the other is memory bandwidth.

00:39:28.130 --> 00:39:32.400
That however many gigabytes per
second the GPU has to its memory

00:39:32.400 --> 00:39:36.590
is much greater than the bandwidth
the CPU has to its memory.

00:39:36.600 --> 00:39:42.700
And our blur kernel,
like the Gaussian example you just saw,

00:39:42.700 --> 00:39:46.340
will in fact scale more
with GPU bandwidth.

00:39:46.500 --> 00:39:49.810
If you have a GPU with
greater memory bandwidth,

00:39:49.810 --> 00:39:51.440
our blur gets faster.

00:39:51.510 --> 00:39:55.300
If you have a GPU that has the same
memory bandwidth but more compute,

00:39:55.300 --> 00:39:59.300
it doesn't get any faster because
we're still using up those free flops.

00:39:59.300 --> 00:40:02.000
He told you about the
first 20 flops are free.

00:40:02.000 --> 00:40:05.700
And of course, the algorithm needs to be
embarrassingly parallel.

00:40:05.700 --> 00:40:10.700
You need to be able to compute
multiple output results independently

00:40:10.700 --> 00:40:13.770
without having one depend on the last.

00:40:14.860 --> 00:40:18.770
It's really helpful to have,
as he mentioned,

00:40:18.850 --> 00:40:22.420
have a debug reference implementation.

00:40:22.420 --> 00:40:25.510
We were developing this lens blur
algorithm at the same time we

00:40:25.510 --> 00:40:30.800
were developing the OpenCL kernel,
and that made things take a lot longer.

00:40:30.800 --> 00:40:36.510
It's much easier if you know what
answer you're expecting so that

00:40:36.540 --> 00:40:39.490
you can more easily debug OpenCL.

00:40:39.490 --> 00:40:44.010
For debugging your algorithm,
Xcode's debugger is much

00:40:44.010 --> 00:40:45.800
easier to deal with.

00:40:46.100 --> 00:40:49.230
There's always a desire,
at least in the marketing department,

00:40:49.230 --> 00:40:53.050
to say, "Oh, we did it on the GPU and now
it's ten times faster." Well,

00:40:53.200 --> 00:40:55.670
it's not going to be a single number,
right, because the GPU and the

00:40:55.670 --> 00:40:57.790
CPU are independent variables.

00:40:57.940 --> 00:40:59.800
That same GPU will be a lot faster
compared to the GPU on the PC.

00:40:59.800 --> 00:41:00.410
So, it's not going to be a single number,
right, because the GPU and the

00:41:00.410 --> 00:41:00.790
CPU are independent variables.

00:41:00.850 --> 00:41:03.640
to the CPU if you put it
in a six-core Mac Pro.

00:41:03.640 --> 00:41:06.120
And I say unpredictable.

00:41:06.120 --> 00:41:09.190
What I mean there is you
have to be empirical.

00:41:09.200 --> 00:41:12.060
Just like this whole last
talk about optimizing,

00:41:12.180 --> 00:41:15.680
just looking at it up front and saying,
well, this only needs to do so many

00:41:15.680 --> 00:41:19.880
flops and so many memory accesses,
and so it ought to be 10 times faster.

00:41:20.020 --> 00:41:24.260
You have to actually implement it and
optimize it and go through that little

00:41:24.260 --> 00:41:28.100
basic algorithm in order to find out
how much faster it's going to be.

00:41:29.060 --> 00:41:35.820
We are constantly prototyping algorithms
to see which ones will be suitable.

00:41:35.820 --> 00:41:37.840
We have to deal with resource limits.

00:41:37.840 --> 00:41:40.050
If you're dealing with just a
screen image and you know the

00:41:40.050 --> 00:41:42.670
number of pixels you're computing
or how many fit on the screen,

00:41:42.670 --> 00:41:44.160
that's probably not a problem.

00:41:44.160 --> 00:41:48.570
But Photoshop tries to deal
with arbitrarily large images,

00:41:48.580 --> 00:41:50.110
multi-gigabyte images.

00:41:50.210 --> 00:41:54.230
And so the memory on the
GPU is not virtualized for you.

00:41:54.230 --> 00:41:58.570
And so we break up the image
into four megapixel chunks,

00:41:59.050 --> 00:42:01.210
and stitch them back together.

00:42:01.430 --> 00:42:05.420
Similarly, there's no preemptive
multitasking out there.

00:42:05.490 --> 00:42:09.850
And typically, if you're just running
through a bunch of pixels,

00:42:09.930 --> 00:42:10.780
it's not a big problem.

00:42:10.780 --> 00:42:13.430
But if you have some iterative
convergence algorithm,

00:42:13.470 --> 00:42:16.750
you can't just have something
out there running for 30 seconds.

00:42:16.870 --> 00:42:21.010
It'll lock up the screen interaction,
and the operating system will kill your

00:42:21.010 --> 00:42:23.840
process because it doesn't like that.

00:42:24.120 --> 00:42:28.580
So you have to break up your problem
into manageable-sized chunks.

00:42:28.830 --> 00:42:31.250
And then finally,
if you're just coding for the Mac,

00:42:31.250 --> 00:42:32.480
you're pretty golden.

00:42:32.480 --> 00:42:36.540
That's a much easier issue.

00:42:36.740 --> 00:42:39.440
If you're determined
to be cross-platform,

00:42:39.440 --> 00:42:42.500
then you have this problem of
not only do you have a couple of

00:42:42.500 --> 00:42:45.280
operating system versions on the Mac,
but you've got the Windows side

00:42:45.280 --> 00:42:47.440
with NVIDIA drivers,
AMD drivers,

00:42:47.510 --> 00:42:50.290
and Intel integrated graphics drivers,
and they're all updated at

00:42:50.290 --> 00:42:51.950
different times and independently.

00:42:52.050 --> 00:42:54.470
And so it really blows
up your test matrix.

00:42:54.600 --> 00:42:57.720
But even with all those challenges,
we would definitely do it again.

00:42:57.720 --> 00:42:58.580
And as I said, we're actually going
to be doing it again.

00:42:58.580 --> 00:42:58.580
And so we're going to be doing it again.

00:42:58.580 --> 00:43:00.010
And as I said,
we're actually in the process of

00:43:00.010 --> 00:43:04.140
prototyping more algorithms to see
what we'll put in the next version.

00:43:04.220 --> 00:43:07.900
And the other thing is that just
in some very preliminary work,

00:43:07.990 --> 00:43:13.410
we've seen really great results
from the AutoVectorizer for running

00:43:13.410 --> 00:43:16.030
OpenCL on the CPU in Mountain Lion.

00:43:16.210 --> 00:43:18.650
And so we'll be looking
at doing more of that.

00:43:18.750 --> 00:43:21.690
And with that,
I'll turn it over to my coworker,

00:43:21.770 --> 00:43:24.700
Dave McGavran, and they've done some
really amazing stuff,

00:43:24.740 --> 00:43:28.360
and a lot of it, in Premiere Pro.

00:43:28.360 --> 00:43:31.590
Thank you.

00:43:34.070 --> 00:43:34.500
Hi there.

00:43:34.540 --> 00:43:35.600
My name is David McGavern.

00:43:35.600 --> 00:43:38.500
I'm the engineering manager
for Adobe Premiere Pro.

00:43:38.560 --> 00:43:41.570
And we've been doing GPU optimizations
for quite a while now.

00:43:41.700 --> 00:43:43.630
But about a year ago,
we wanted to really see what

00:43:43.630 --> 00:43:45.440
we were going to do with CS6.

00:43:45.440 --> 00:43:47.800
And we had sort of two goals with CS6.

00:43:47.920 --> 00:43:50.590
We didn't just want, you know,
a certain amount of effects to go

00:43:50.680 --> 00:43:52.840
faster or some things to go faster.

00:43:52.890 --> 00:43:55.860
We liked that idea,
but we really wanted to be able to change

00:43:55.860 --> 00:43:57.730
the way that we do video workflows.

00:43:57.800 --> 00:44:01.820
We wanted editors to actually have
a different way that they can now

00:44:01.820 --> 00:44:05.200
use the application to do more
creative things with their work.

00:44:05.400 --> 00:44:07.860
And so when we looked at how
we were going to do that,

00:44:07.860 --> 00:44:08.830
we had two goals.

00:44:08.910 --> 00:44:11.450
One, we wanted to be able to
do it on both platforms.

00:44:11.480 --> 00:44:14.260
And two, we wanted to be able to
do it on the Mac laptop.

00:44:14.360 --> 00:44:18.940
The 15-inch laptop is extremely
popular for our customers,

00:44:18.940 --> 00:44:20.780
and we really wanted to
be able to support that.

00:44:20.830 --> 00:44:23.750
And when you look at really how
you can go about doing that,

00:44:23.750 --> 00:44:26.170
the only answer that
you can go at is OpenCL.

00:44:26.240 --> 00:44:27.700
So we looked at jumping into OpenCL.

00:44:27.700 --> 00:44:32.660
We looked at the OpenCL for Premiere CS6,
and we went really all the way in.

00:44:32.770 --> 00:44:36.300
You'll see we ported our entire
video pipeline to OpenCL,

00:44:36.300 --> 00:44:38.260
not just a little bit, all of it.

00:44:38.260 --> 00:44:40.030
And working with Apple and
our other partners,

00:44:40.070 --> 00:44:43.500
we will have a pretty compelling
reason why that our users will

00:44:43.500 --> 00:44:46.500
now have a different workflow
when they're editing video.

00:44:46.500 --> 00:44:48.340
So I'm going to show you a quick demo.

00:44:48.340 --> 00:44:51.440
So here you can see
Adobe Premiere Pro CS6,

00:44:51.460 --> 00:44:53.120
and we're going to show you
a few interesting ways that

00:44:53.120 --> 00:44:54.160
our workflows have changed.

00:44:54.160 --> 00:44:57.590
So down here in the bottom right,
this is our timeline window.

00:44:57.670 --> 00:45:00.660
What you're seeing there is
four individual HD frames,

00:45:00.660 --> 00:45:04.660
and we put a scale and a position on
them so they're picture-in-picture.

00:45:04.660 --> 00:45:08.660
And one of the new features we have
is the adjustment layer feature,

00:45:08.660 --> 00:45:11.390
and you might be familiar with this
from Photoshop or After Effects.

00:45:11.460 --> 00:45:13.360
Having this in a video editor
that does real-time video

00:45:13.450 --> 00:45:16.710
editing is pretty compelling,
and it's a new feature in CS6.

00:45:16.720 --> 00:45:19.610
So what that allows you to do is
instead of adding effects just

00:45:19.610 --> 00:45:22.420
to the individual video clips,
you can now add an effect

00:45:22.420 --> 00:45:25.610
to the adjustment layer,
and that will change everything

00:45:25.790 --> 00:45:27.620
that's under the adjustment
layer as a total component.

00:45:27.620 --> 00:45:30.620
So the first thing I'm going to do here
is I'm going to go over to the effects,

00:45:30.620 --> 00:45:34.590
and I'm going to add a three-way color
corrector to the adjustment layer.

00:45:34.640 --> 00:45:35.610
So I'm going to go over to the effects,
and I'm going to add a three-way color

00:45:35.610 --> 00:45:36.300
corrector to the adjustment layer.

00:45:37.770 --> 00:45:38.240
So here we go.

00:45:38.280 --> 00:45:41.140
So what we're going to do is we're
going to grade that composite

00:45:41.140 --> 00:45:43.770
of those four HD streams.

00:45:44.170 --> 00:45:44.600
So here we go.

00:45:44.600 --> 00:45:46.600
We're going to make it very colorful.

00:45:46.740 --> 00:45:50.000
And it took a little while because it's
actually a pretty complex optimization.

00:45:50.110 --> 00:45:51.540
And that's a really good
feature for our users.

00:45:51.540 --> 00:45:55.260
They get to now grade an entire
composite of video edits.

00:45:55.370 --> 00:45:56.740
But it's not really
what we were aiming for.

00:45:56.740 --> 00:45:58.400
We really wanted to change workflows.

00:45:58.400 --> 00:46:02.100
We wanted people to be able
to stay creative in real time.

00:46:02.220 --> 00:46:05.280
So I'm going to go in here and
I'm going to turn on OpenCL.

00:46:05.790 --> 00:46:10.880
Again,
this is on the 2011 MacBook Pro 15-inch.

00:46:12.580 --> 00:46:14.070
So here we are now,
and so now I'm going to actually

00:46:14.190 --> 00:46:17.190
use another feature which
we call Don't Stop Playback.

00:46:17.300 --> 00:46:19.510
So now we're actually playing
that back in real time,

00:46:19.510 --> 00:46:22.410
which is 4 HD streams with some
scales and some positions and

00:46:22.410 --> 00:46:23.940
a three-way color corrector.

00:46:24.020 --> 00:46:26.560
And when I say that we can now
do things that we were never

00:46:26.560 --> 00:46:29.940
able to do before with OpenCL,
I'm now actually grading that whole thing

00:46:29.940 --> 00:46:32.600
in real time without dropping any frames.

00:46:32.650 --> 00:46:36.430
That's a very different workflow
for our customers and very powerful,

00:46:36.510 --> 00:46:39.660
and that's a lot of computational
power going on there.

00:46:39.730 --> 00:46:40.850
But that's not it.

00:46:40.880 --> 00:46:43.850
We can now go and add more effects.

00:46:44.510 --> 00:46:46.840
So while I'm editing,
I can go and grab more effects

00:46:46.950 --> 00:46:47.920
and add them to the video stream.

00:46:47.920 --> 00:46:49.260
So we'll just add a
brightness and contrast here.

00:46:49.260 --> 00:46:51.300
And again, editing is not stopping.

00:46:51.300 --> 00:46:54.960
We're not dropping frames on a 15-inch
laptop with a lot of HD streams.

00:46:54.960 --> 00:46:58.420
And I can just continue to
edit this way without stopping,

00:46:58.650 --> 00:47:03.070
being very creative and staying in
the workflow and not being distracted.

00:47:03.140 --> 00:47:05.280
Now, I actually didn't even
tell you all the truth.

00:47:05.360 --> 00:47:08.290
In fact, each of those video streams
also has effects on them.

00:47:08.290 --> 00:47:10.930
So I can go to one of those other
video streams and you'll see

00:47:10.930 --> 00:47:12.830
another three-way color corrector.

00:47:13.900 --> 00:47:16.630
And I can do that and you'll see
that one of the screens -- I don't

00:47:16.670 --> 00:47:19.640
remember which one -- will be
changing as I update that one.

00:47:19.810 --> 00:47:21.400
There you go, on the bottom left there.

00:47:21.400 --> 00:47:23.900
And each of those video streams has that.

00:47:24.040 --> 00:47:29.670
So now we have a very large collection
of video algorithms working in parallel

00:47:30.050 --> 00:47:32.520
and not stopping on a 15-inch laptop.

00:47:32.660 --> 00:47:35.640
And this is how we can use
OpenCL to really change the

00:47:35.640 --> 00:47:37.560
way video editors edit video.

00:47:37.820 --> 00:47:39.630
So that was that.

00:47:45.550 --> 00:47:47.360
So how did we do this?

00:47:47.360 --> 00:47:49.520
So we've been -- like I said,
we've been working with

00:47:49.520 --> 00:47:51.990
GPU stuff for a while,
and we've also been working with

00:47:52.040 --> 00:47:53.560
multiprocessing for a while.

00:47:53.560 --> 00:47:57.270
And so we've done something that
helped us is that we broke up our

00:47:57.270 --> 00:47:59.280
pipeline into multiple stages.

00:47:59.410 --> 00:48:01.830
And the first thing we'll do is
we'll start on those HD frames and

00:48:01.830 --> 00:48:03.650
we'll start reading them from disk.

00:48:03.860 --> 00:48:05.840
And we'll be doing that in parallel.

00:48:05.890 --> 00:48:08.470
And we'll be doing as much as we can
at once ahead of time of the play

00:48:08.500 --> 00:48:10.060
head where you're seeing it play.

00:48:10.210 --> 00:48:12.140
And then we'll start
doing some CPU processing.

00:48:12.140 --> 00:48:15.280
Now, what's very important is for us,
we didn't want to throw every

00:48:15.280 --> 00:48:17.910
single thing up onto the GPU because
then you'd have a whole bunch

00:48:17.940 --> 00:48:20.800
of CPUs not doing anything,
which would be the inverse problem

00:48:20.800 --> 00:48:23.180
that we had a couple years back.

00:48:23.270 --> 00:48:25.040
So we do some CPU processing.

00:48:25.040 --> 00:48:28.150
And in that,
maybe we'll do an H2640 code or

00:48:28.150 --> 00:48:30.250
we'll do some other codec work.

00:48:30.370 --> 00:48:32.880
And there's a few effects that we
didn't actually get to in OpenCL,

00:48:32.880 --> 00:48:33.870
so we'll do those.

00:48:34.040 --> 00:48:35.950
And we'll do that as a subtree render.

00:48:35.960 --> 00:48:39.240
So we'll package everything
under the non-GPU code together.

00:48:39.350 --> 00:48:40.140
And at that point, we're ready to go.

00:48:40.140 --> 00:48:41.140
And we'll start to upload it.

00:48:41.140 --> 00:48:42.880
And all of this is going
in parallel together.

00:48:42.880 --> 00:48:44.980
And at this point,
we get it up onto the GPU and

00:48:45.060 --> 00:48:46.740
the GPU becomes a coprocessor.

00:48:46.770 --> 00:48:50.140
And so while the disk I/O is going
on and the CPU processing going on,

00:48:50.180 --> 00:48:54.040
the GPU starts cranking
away and doing all its work.

00:48:54.070 --> 00:48:58.730
Then we either go out to display using an
OpenGL interop or we bring it back down

00:48:58.730 --> 00:49:01.340
to the system memory to do an export.

00:49:01.410 --> 00:49:05.120
So here, you get an idea of how
much we actually did.

00:49:05.180 --> 00:49:07.220
Again, we started this a year ago.

00:49:07.390 --> 00:49:10.140
And there was a lot of people
who were pretty skeptical

00:49:10.140 --> 00:49:10.140
when I came in this room.

00:49:10.140 --> 00:49:13.140
And they said,
we're going to port to OpenCL.

00:49:13.140 --> 00:49:15.140
That's a lot of GPU work that we do.

00:49:15.140 --> 00:49:18.050
And so what you can see is in the
intrinsics there in that first

00:49:18.050 --> 00:49:20.740
top left column is just the things
that a user may not always see.

00:49:20.740 --> 00:49:24.940
We're doing deinterlacing, compositing,
blending modes, nested sequences.

00:49:24.940 --> 00:49:27.740
All that stuff is what
the user doesn't see.

00:49:27.740 --> 00:49:30.340
And then you have a series
of transitions and effects.

00:49:30.340 --> 00:49:32.340
And those are the things that
the user actually applies.

00:49:32.340 --> 00:49:36.040
All of these are optimized
for the GPU using OpenCL.

00:49:36.040 --> 00:49:40.140
And that's pretty impressive in the
time that we had to get this done.

00:49:40.140 --> 00:49:44.140
So not only that, on top of all that,
we get to do bigger frame sizes.

00:49:44.140 --> 00:49:45.630
We get to do deeper color.

00:49:45.710 --> 00:49:48.130
And we have this all in
a floating point space.

00:49:48.290 --> 00:49:49.950
So it's not even just faster.

00:49:50.240 --> 00:49:52.140
It's also better quality.

00:49:53.490 --> 00:49:54.960
So what did we learn from doing this?

00:49:55.010 --> 00:49:57.610
Well, it was a lot of work
to do this in a year,

00:49:57.730 --> 00:49:59.430
and there was a lot of
things we had to learn.

00:49:59.430 --> 00:50:01.850
So one of the first things you
saw is when Ana was talking,

00:50:01.920 --> 00:50:04.900
she talked about how you had to
load up these kernels at run time.

00:50:04.930 --> 00:50:05.960
There's a lot of kernels.

00:50:05.990 --> 00:50:08.520
And some of those kernels are
broken into multiple kernels.

00:50:08.520 --> 00:50:11.200
So we had a lot of stuff getting
loaded up and compiling at run time.

00:50:11.300 --> 00:50:13.960
We needed to load those
asynchronously or we didn't get

00:50:13.960 --> 00:50:15.540
a very good user experience.

00:50:15.580 --> 00:50:19.470
So by loading them asynchronously,
we can get to the situation where a

00:50:19.760 --> 00:50:21.420
kernel was needed before it was ready.

00:50:21.570 --> 00:50:24.320
So we then actually preempt those
asynchronous loadings so that

00:50:24.480 --> 00:50:27.720
we pick the kernels that are
most important at the right time.

00:50:27.720 --> 00:50:30.100
So that changed the user experience,
and now you don't even know

00:50:30.100 --> 00:50:33.200
that compiling is really going
on when you're launching.

00:50:33.520 --> 00:50:37.100
We also had a lot of work to get
OpenCL Interop high enough performant.

00:50:37.220 --> 00:50:40.040
There's a number of ways to do it,
and we tried them all,

00:50:40.050 --> 00:50:41.540
and we did a lot of benchmarking.

00:50:41.660 --> 00:50:44.780
And what we came to is that
CL Create from GL Buffer and

00:50:44.910 --> 00:50:48.860
CL In-Q Acquire GL Objects were the way
to go for both the upload and download.

00:50:49.020 --> 00:50:52.020
These were the things that
gave us the best performance.

00:50:52.560 --> 00:50:55.640
Then comes the question of
OpenCL images versus buffers.

00:50:55.640 --> 00:50:59.160
They're both sort of independent
ways to deal with memory on the GPU.

00:50:59.270 --> 00:51:02.140
And the OpenCL images actually
fits our model better.

00:51:02.220 --> 00:51:05.290
It's a 2D, you know,
optimized version to get to video frames,

00:51:05.290 --> 00:51:07.320
which fits what we're doing.

00:51:07.400 --> 00:51:08.860
But for the most part,
we actually settled in on

00:51:08.870 --> 00:51:10.620
buffers for a couple reasons.

00:51:10.820 --> 00:51:13.060
Buffers allowed us to do better caching.

00:51:13.060 --> 00:51:14.600
We can actually manage
that memory better,

00:51:14.600 --> 00:51:16.450
and we had a lot of frames
in parallel as you saw.

00:51:16.450 --> 00:51:20.950
It also allows us to read and write to
the same -- to the same buffer at once,

00:51:20.950 --> 00:51:24.140
which is very important
for a lot of our kernels.

00:51:24.760 --> 00:51:27.000
In the year that we had,
you heard Russell say that

00:51:27.010 --> 00:51:29.040
things have really matured.

00:51:29.040 --> 00:51:33.570
We started on when we were doing a big
thing and our code wasn't working right,

00:51:33.780 --> 00:51:36.230
and there was still
growth areas in OpenCL,

00:51:36.230 --> 00:51:39.090
so we actually had trouble
with pinned memory,

00:51:39.210 --> 00:51:41.960
which is a shame because that
actually brings a lot of performance.

00:51:41.990 --> 00:51:44.240
At the time, we had to avoid it because
we couldn't get it to work,

00:51:44.240 --> 00:51:46.000
which is probably our fault.

00:51:46.000 --> 00:51:47.410
So we're going to go back
and look at that again,

00:51:47.420 --> 00:51:49.460
but that's something to keep in
mind that using pinned memory

00:51:49.460 --> 00:51:51.980
will give you better performance,
but it was tricky for

00:51:51.980 --> 00:51:53.080
us to get it to work.

00:51:53.080 --> 00:51:56.300
We also had trouble with structures
passing them up to the kernels.

00:51:56.300 --> 00:51:57.860
We couldn't always get
the alignment right,

00:51:57.860 --> 00:52:00.370
so we found a good way to get
around that is to just flatten

00:52:00.370 --> 00:52:04.320
the structures and move on and
not spend too much time there.

00:52:04.320 --> 00:52:07.040
Also very importantly,
and you've probably heard this

00:52:07.040 --> 00:52:09.480
a lot since you've been at WWDC,
filing radars.

00:52:09.530 --> 00:52:13.210
The OpenCL team has been extremely
responsive when we've found performance

00:52:13.210 --> 00:52:17.000
issues or concerns either helping us fix
our code or fixing bugs in a few cases.

00:52:17.060 --> 00:52:19.580
So file radars,
and that's a really great way

00:52:19.600 --> 00:52:21.460
to get things taken care of.

00:52:21.460 --> 00:52:24.700
So when we look at all that
we've achieved with OpenCL,

00:52:24.700 --> 00:52:27.280
there's still some things that
we can brainstorm about that are

00:52:27.500 --> 00:52:31.360
still possible in a video editing
application to do with OpenCL.

00:52:31.360 --> 00:52:32.880
We can continue to increase
the set of effects.

00:52:33.020 --> 00:52:37.120
There's a few effects that we have that
we don't have OpenCL implementations for.

00:52:37.120 --> 00:52:39.350
It is a little bit of a problem
there because not all of

00:52:39.350 --> 00:52:41.450
those are in use all the time,
and there's maybe more work

00:52:41.550 --> 00:52:43.510
than needed for the amount of
people who use those effects,

00:52:43.670 --> 00:52:45.820
but there's always some there.

00:52:45.820 --> 00:52:48.070
We can start looking at making sure
that our third-party developers

00:52:48.160 --> 00:52:50.580
who write plug-ins for Premiere has
the opportunity to write.

00:52:50.580 --> 00:52:50.580
So we can start looking at making
sure that our third-party developers

00:52:50.580 --> 00:52:50.580
who write plug-ins for Premiere has
the opportunity to write.

00:52:50.580 --> 00:52:50.580
So we can start looking at making
sure that our third-party developers

00:52:50.580 --> 00:52:50.580
who write plug-ins for Premiere has
the opportunity to write.

00:52:50.580 --> 00:52:54.540
GPU effects, which they can do now,
but it's not really optimized

00:52:54.560 --> 00:52:56.580
to fit within our pipeline.

00:52:56.580 --> 00:52:59.660
We can look at doing codecs,
GPU encoding and decoding

00:52:59.660 --> 00:53:01.470
of certain codecs,
and that would be very

00:53:01.500 --> 00:53:02.860
interesting in some cases.

00:53:02.860 --> 00:53:05.250
But again,
we want to be careful not to let the

00:53:05.300 --> 00:53:09.360
CPUs sit there idle because then we're
wasting resources on the machine.

00:53:09.400 --> 00:53:11.960
We can look if there's an opportunity
to take care of multiple CPUs at once

00:53:11.960 --> 00:53:13.770
when those configurations are available.

00:53:13.880 --> 00:53:16.360
And we have some scopes that
we'd like to -- you know,

00:53:16.360 --> 00:53:19.640
we can see some nice optimizations there.

00:53:19.730 --> 00:53:20.160
Also, there's, you know,
effects that we -- we

00:53:20.160 --> 00:53:20.160
can look at doing codecs,
and that would be very

00:53:20.160 --> 00:53:20.160
interesting in some cases.

00:53:20.160 --> 00:53:20.160
But again,
we want to be careful not to let the

00:53:20.160 --> 00:53:20.870
CPUs sit there idle because then we're
wasting resources on the machine.

00:53:20.980 --> 00:53:22.270
So,
we -- we could probably do an OpenCL that

00:53:22.280 --> 00:53:25.940
actually would never make sense in
software just because they're so slow.

00:53:25.950 --> 00:53:27.300
Customers only want to wait so long.

00:53:27.430 --> 00:53:29.910
And OpenCL actually brings some
opportunities where we can do some

00:53:29.910 --> 00:53:31.390
things we've never done before.

00:53:31.520 --> 00:53:35.840
So that's Adobe Premiere Pro CS6
and how we use OpenCL.

00:53:35.920 --> 00:53:37.280
If you have any questions,
you can come find us.

00:53:37.280 --> 00:53:40.440
I'll be hanging out at the session
tomorrow morning at the lab.

00:53:40.440 --> 00:53:43.890
So I'm going to give it back to Ananau,
and

00:53:47.850 --> 00:53:50.940
If you have any questions about
what you've seen today or any

00:53:50.940 --> 00:53:54.340
questions about OpenCL in general,
contact Alan Schaffer.

00:53:54.590 --> 00:53:58.620
He's our graphics and game
technologies evangelist.

00:53:58.660 --> 00:54:01.320
And also, if you're new to OpenCL,
here's a link to the

00:54:01.320 --> 00:54:03.860
past WWDC presentations.

00:54:03.910 --> 00:54:06.860
And you can also go to the
Apple Developer Forums.

00:54:07.090 --> 00:54:09.300
Thank you very much for coming.