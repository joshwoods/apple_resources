WEBVTT

00:00:04.820 --> 00:00:07.140
All right, welcome, everyone,
to our second session

00:00:07.140 --> 00:00:10.240
on Core Image today,
and we'll be talking about some

00:00:10.240 --> 00:00:12.760
more advanced techniques for how
to get the best performance out

00:00:12.760 --> 00:00:16.570
of Core Image and some advanced
workflows using AV Foundation and

00:00:16.570 --> 00:00:19.230
using OpenGL and for games as well.

00:00:19.240 --> 00:00:22.320
Let me give you a brief outline
of what we're talking about today.

00:00:22.320 --> 00:00:25.920
I'll just do a quick summary for those
of you who weren't in the preceding

00:00:25.920 --> 00:00:29.620
session to introduce you to Core Image,
and then I'll pass the

00:00:29.620 --> 00:00:32.530
stage over to Chendi,
who will be giving an overview of

00:00:32.590 --> 00:00:34.680
what's new in iOS 6 for Core Image.

00:00:34.700 --> 00:00:46.570
So, first off, again,
for those of you who weren't

00:00:46.570 --> 00:00:55.840
in our session earlier,
this is just a little bit of a review.

00:00:55.840 --> 00:00:58.060
What is the key concept of Core Image?

00:00:58.060 --> 00:01:01.780
The idea behind Core Image is that
you can chain together multiple

00:01:01.780 --> 00:01:04.680
filters in an image processing
framework that's very easy to use.

00:01:04.680 --> 00:01:07.610
very easy to use and develop.

00:01:08.090 --> 00:01:11.020
Simple example of that is we have
an original image and we want to

00:01:11.020 --> 00:01:13.000
apply multiple filters to that image.

00:01:13.000 --> 00:01:16.260
For example,
applying a sepia tone image effect to it,

00:01:16.670 --> 00:01:21.000
then a hue adjustment effect,
and then a contrast adjustment effect.

00:01:21.190 --> 00:01:22.950
Even though we've used
just three filters here,

00:01:23.070 --> 00:01:27.000
we can combine these in very unique ways.

00:01:28.130 --> 00:01:30.760
One thing that we do to get
the best possible formance is,

00:01:31.050 --> 00:01:34.240
even though you can conceptually
think of there being an intermediate

00:01:34.240 --> 00:01:37.940
image between every filter,
Core Image will concatenate

00:01:37.950 --> 00:01:42.640
the filter graph at runtime
so that it'll be one program.

00:01:42.780 --> 00:01:45.590
And this greatly improves performance.

00:01:45.840 --> 00:01:48.930
Another thing we've done in this
particular example is we've noted

00:01:48.940 --> 00:01:52.650
the fact that two of these filters
could be represented as a matrix,

00:01:52.680 --> 00:01:56.480
and those two matrixes can be
combined together into one effect.

00:01:57.690 --> 00:02:00.370
One of the key concepts in
Core Image is also it's got very

00:02:00.590 --> 00:02:03.190
flexible inputs and outputs.

00:02:03.300 --> 00:02:05.640
In terms of inputs,
you can bring in content to

00:02:05.640 --> 00:02:10.900
Core Image from your photo library,
from a live video capture session,

00:02:10.990 --> 00:02:13.040
images that you may
have in memory already,

00:02:13.140 --> 00:02:16.040
from files that are
supported via Image.io,

00:02:16.110 --> 00:02:17.970
and also via OpenGL Textures.

00:02:18.080 --> 00:02:22.040
And OpenGL Textures we'll be talking
about in a bit more detail this afternoon

00:02:22.040 --> 00:02:26.270
because this is some really great
addition that we've added to Core Image.

00:02:26.670 --> 00:02:30.190
We also support very flexible outputs,
and this is also key for getting the

00:02:30.190 --> 00:02:31.540
best performance out of Core Image.

00:02:31.540 --> 00:02:34.960
We, in the simplest form,
allow producing a

00:02:34.960 --> 00:02:39.700
CG image from a CI image,
and from a CG image, you can go to many

00:02:39.700 --> 00:02:43.130
different destinations,
like into a UI image view or

00:02:43.460 --> 00:02:46.820
out to Image.io for exporting
different file formats,

00:02:46.820 --> 00:02:48.540
or into the photo library.

00:02:48.540 --> 00:02:51.280
But we also support rendering
into Eagle Contexts,

00:02:51.280 --> 00:02:54.670
into CV pixel buffers,
which are useful for

00:02:54.670 --> 00:02:58.420
going into AV Foundations,
and into raw bytes,

00:02:58.460 --> 00:03:00.620
or into OpenGL textures.

00:03:00.620 --> 00:03:04.000
And this flexibility,
both in terms of input and output,

00:03:04.000 --> 00:03:08.160
is critical to get the best performance
out of Core Image for real-time effects.

00:03:08.160 --> 00:03:12.100
So to talk about these performance
enhancements in much more detail,

00:03:12.100 --> 00:03:13.540
I'm going to pass the
stage over to Chendi,

00:03:13.540 --> 00:03:16.400
and he'll show you some great demos.

00:03:16.460 --> 00:03:20.820
Thanks, David.

00:03:24.500 --> 00:03:26.940
So for our first attempt
at writing this app,

00:03:26.940 --> 00:03:30.900
we'll use AV Foundation to
stream frames from the camera.

00:03:30.900 --> 00:03:33.400
We'll process them using Core Image.

00:03:33.400 --> 00:03:38.040
Then we'll set them as the
URImage property on a URImageView.

00:03:38.060 --> 00:03:41.980
And we won't be actually using
Core Image's CI context at all.

00:03:42.270 --> 00:03:45.440
So this will be extremely easy to write.

00:03:45.440 --> 00:03:47.470
Let's see how we do this.

00:03:55.160 --> 00:03:59.190
So basically, we have a single view
controller with an image view,

00:03:59.190 --> 00:04:02.900
a couple of filters that we create once.

00:04:03.290 --> 00:04:09.190
And then in the main callback loop,
what we're going to do is basically grab

00:04:09.200 --> 00:04:12.030
the pixel buffer from AV Foundation,

00:04:12.160 --> 00:04:17.600
Rotate it by 90 degrees because normally
buffers come in rotated from the camera.

00:04:17.630 --> 00:04:20.000
Zero the origin.

00:04:20.070 --> 00:04:22.330
Pass it through our filter chain.

00:04:23.290 --> 00:04:26.100
Grab the final image
from the final filter,

00:04:26.200 --> 00:04:30.710
wrap that inside a UI image,
and set that on our view.

00:04:31.070 --> 00:04:34.860
And note at the very end,
we nil out all the input image values

00:04:35.070 --> 00:04:40.790
in our filters so they don't keep the
retain count on the CVPixel buffer,

00:04:40.790 --> 00:04:45.460
and we don't have buffers floating
around when we should release them.

00:04:45.670 --> 00:04:46.880
So this is a pretty simple app.

00:04:47.030 --> 00:04:48.980
Let's see what it looks like.

00:04:50.240 --> 00:04:54.240
All right, so this definitely looks like
the filter chain that we wanted.

00:04:54.240 --> 00:04:58.510
But as you can probably tell,
it's running pretty slowly.

00:04:58.920 --> 00:05:00.760
If we had instruments,
we could see that it's running

00:05:00.760 --> 00:05:05.710
around five or six frames per second,
and this is just way too slow

00:05:05.710 --> 00:05:07.690
for a real-time use case.

00:05:09.350 --> 00:05:13.660
So we probably want something
a little bit faster than this.

00:05:13.750 --> 00:05:15.820
If we go back to our slides.

00:05:17.810 --> 00:05:20.450
Why was this approach so slow?

00:05:20.620 --> 00:05:24.040
Well, UI image views are
optimized for static images,

00:05:24.200 --> 00:05:27.170
and they're probably not the
best approach for streaming

00:05:27.170 --> 00:05:29.850
real-time video with effects.

00:05:30.250 --> 00:05:33.170
Furthermore, if we had run an
instruments trace on this,

00:05:33.210 --> 00:05:36.020
we would have noticed that we
would have been wasting a lot of

00:05:36.110 --> 00:05:37.810
time in CG context draw image.

00:05:37.880 --> 00:05:42.540
So maybe UI Image View isn't
rendering this optimally.

00:05:42.540 --> 00:05:45.050
And furthermore,
we'd see that CI contexts were

00:05:45.050 --> 00:05:46.920
created for every single frame.

00:05:46.920 --> 00:05:49.880
This makes sense for a static
image because you don't want

00:05:49.880 --> 00:05:52.980
to have a CI context floating
around after your render.

00:05:52.980 --> 00:05:56.350
But if we're rendering real-time video,
we don't want to create a

00:05:56.350 --> 00:05:58.070
context every single frame.

00:05:58.580 --> 00:06:03.180
This will kill our performance since
creating context is pretty expensive.

00:06:03.440 --> 00:06:06.890
So this seems like a prime candidate
for a place where we can drop

00:06:06.890 --> 00:06:11.440
down to a lower-level API for
more performance-sensitive work.

00:06:11.570 --> 00:06:13.680
So for our next attempt...

00:06:14.690 --> 00:06:17.800
We'll keep the
AV Foundation input stream.

00:06:17.870 --> 00:06:21.170
We'll still use CI to render,
but instead of wrapping it

00:06:21.170 --> 00:06:25.720
inside a UI image directly,
we'll create a CG image explicitly with

00:06:25.850 --> 00:06:29.300
the createCgImage method on CI context.

00:06:29.360 --> 00:06:33.430
And then we'll use this as this
UI image for the UI image view.

00:06:33.600 --> 00:06:36.490
And hopefully this will
run a little bit faster.

00:06:36.610 --> 00:06:38.800
So let's see what this looks like.

00:06:38.830 --> 00:06:41.470
So this is largely the same code.

00:06:42.540 --> 00:06:47.580
The only difference is in
the capture output callback.

00:06:47.920 --> 00:06:52.220
Instead of calling URImage
image with CI image,

00:06:52.250 --> 00:06:56.310
we'll manually invoke a render
with CI context createCgImage

00:06:56.540 --> 00:06:59.070
and wrap that inside a UI image.

00:06:59.360 --> 00:07:02.270
This may not look like
a lot of different code,

00:07:02.360 --> 00:07:05.340
but let's see what it
looks like on the iPad.

00:07:05.770 --> 00:07:08.540
This already looks a lot better.

00:07:08.610 --> 00:07:12.120
And I think we're pretty much done,
actually, right?

00:07:12.280 --> 00:07:16.310
Well, this is actually running around 21,
22 frames per second.

00:07:16.480 --> 00:07:17.870
If you look in the console,
you would see a lot of

00:07:18.020 --> 00:07:20.710
frame drop notifications,
so there's still a lot

00:07:20.710 --> 00:07:22.010
of work to be done.

00:07:23.020 --> 00:07:25.300
For something with a
simpler filter chain,

00:07:25.300 --> 00:07:27.400
you might actually get
away with using a CG image,

00:07:27.500 --> 00:07:31.430
UI image view approach, but for this app,
we still want something to

00:07:31.430 --> 00:07:34.860
render-- we want something to
render a little bit faster.

00:07:36.010 --> 00:07:38.900
Why is performance still
non-ideal right here?

00:07:39.020 --> 00:07:41.560
Well,
what is going on for every single render?

00:07:41.700 --> 00:07:44.000
We have an image in our application.

00:07:44.050 --> 00:07:49.200
We want to use Core Image to
process it via OpenGL.

00:07:49.250 --> 00:07:53.490
So the first thing that happens
is this image is uploaded to GL.

00:07:55.500 --> 00:08:18.600
[Transcript missing]

00:08:19.140 --> 00:08:22.650
And this is very non-ideal because we
had this extraneous upload/download at

00:08:22.690 --> 00:08:25.710
the very end that hurts performance.

00:08:27.000 --> 00:08:29.840
So what we really want
is something like this.

00:08:29.900 --> 00:08:32.270
We'll have a texture in our app.

00:08:32.920 --> 00:08:37.150
that we upload to OpenGL will
use Core Image to render it,

00:08:37.210 --> 00:08:38.520
but directly to the display.

00:08:38.650 --> 00:08:41.400
There's no need to download
it back to the CPU and upload

00:08:41.400 --> 00:08:44.970
it to GPU one more time.

00:08:45.060 --> 00:08:48.040
So this is something we want.

00:08:48.110 --> 00:08:50.260
How can we implement this?

00:08:51.910 --> 00:08:55.920
Well, for our third attempt,
let's use AV Foundation again

00:08:56.000 --> 00:08:58.810
to stream frames,
and let's render directly

00:08:58.910 --> 00:09:02.040
to a CA Eagle layer using
Core Image this time.

00:09:02.220 --> 00:09:05.110
And hopefully this will give
us the performance we need.

00:09:06.370 --> 00:09:11.540
So the code is a little more
complex in this version.

00:09:11.640 --> 00:09:13.200
We're no longer using a URImageView.

00:09:13.200 --> 00:09:18.520
Instead, we have a custom GLESView that
wraps around a CA Eagle layer.

00:09:19.900 --> 00:09:24.010
And in our main loop,
instead of creating a CG image,

00:09:24.010 --> 00:09:29.200
we're just using CI context, draw image,
in rect, from rect,

00:09:29.200 --> 00:09:34.680
and then telling the GL ES view
to present its render buffer.

00:09:34.740 --> 00:09:37.580
And so let's see what this
looks like on the iPad.

00:09:39.830 --> 00:09:43.000
This actually looks worse
than the previous approach.

00:09:43.080 --> 00:09:45.560
It looks definitely more stuttery
and looks like there's less frames

00:09:45.640 --> 00:09:47.820
per second going on right now.

00:09:48.130 --> 00:09:50.880
If we use instruments to trace this,
we'll see that we're getting

00:09:50.880 --> 00:09:54.150
around 13 frames per second,
which is about 8 worse than we did

00:09:54.150 --> 00:09:56.860
with the CG Image/URI imagery approach.

00:09:58.660 --> 00:10:03.800
Well, if we go back to the machine,
we notice we're drawing this

00:10:03.800 --> 00:10:06.640
image at the screen resolution.

00:10:06.690 --> 00:10:09.600
And if you're hitting performance
bottlenecks in Core Image,

00:10:09.640 --> 00:10:13.170
you need to realize the number
one key to Core Image performance

00:10:13.780 --> 00:10:16.180
is the output render size.

00:10:16.280 --> 00:10:21.260
For the first two approaches,
we had an input image of 640 by 480,

00:10:21.260 --> 00:10:24.640
and we rendered it to a
640 by 480 destination.

00:10:25.420 --> 00:10:30.670
And we let Core Animation scale it up
to screen resolution via UI Image View.

00:10:30.990 --> 00:10:34.330
In this approach,
we're actually rendering it directly

00:10:34.340 --> 00:10:38.340
to a 1024 by 768 destination.

00:10:38.380 --> 00:10:43.980
And so we're rendering 156% more pixels
than we are in the previous two cases.

00:10:44.240 --> 00:10:46.970
So to get comparable performance,
what we should do is not

00:10:46.980 --> 00:10:52.780
render at screen resolution,
but 640 by 480.

00:10:54.420 --> 00:10:56.540
and have our view scale up.

00:10:56.660 --> 00:10:59.990
So we'll set a content
scale factor on the view

00:11:01.280 --> 00:11:05.540
to scale up from 640x480 to 1024x768.

00:11:05.600 --> 00:11:08.340
And this should give us
much better performance.

00:11:08.610 --> 00:11:15.980
If we go back to the iPad,
we see this is much, much better.

00:11:17.330 --> 00:11:19.100
But if we use instruments
again to trace this,

00:11:19.260 --> 00:11:20.670
we're not quite there.

00:11:20.740 --> 00:11:24.960
This is running at 28 frames per second,
which is still not ideal because, again,

00:11:25.040 --> 00:11:28.040
the front camera on
this iPad streams at 30.

00:11:28.040 --> 00:11:30.950
So ideally, we'd like to render at
30 frames per second.

00:11:31.220 --> 00:11:36.260
So there's still work left to be done
to get this to be running at real time.

00:11:36.320 --> 00:11:39.410
What can we do at this
point to go faster?

00:11:39.730 --> 00:11:43.790
Well, we already discussed
reducing the render size,

00:11:43.790 --> 00:11:47.430
but we can also just now disable color
management and leverage YUV image

00:11:47.470 --> 00:11:50.350
support as ways to speed up our program.

00:11:52.980 --> 00:12:00.060
By default,
Core Image does all its rendering

00:12:00.060 --> 00:12:00.060
in a light linear color space
for accuracy and consistency.

00:12:00.260 --> 00:12:05.140
This is not particularly cheap though
because converting from and to sRGB

00:12:05.430 --> 00:12:07.370
involves these equations you see here.

00:12:07.720 --> 00:12:11.040
And a step, pow,
mix function along with some

00:12:11.040 --> 00:12:15.360
arithmetic can really add to your
shader complexity if you do it a lot.

00:12:15.790 --> 00:12:24.890
So in cases where you need the absolute
highest performance in your app,

00:12:24.890 --> 00:12:25.490
or perhaps your app does
some weird exaggerated

00:12:25.700 --> 00:12:42.000
[Transcript missing]

00:12:46.140 --> 00:12:51.380
New in iOS 6 is the ability to read
directly from YUV 420 textures,

00:12:51.540 --> 00:12:53.340
or sorry, pixel buffers, natively.

00:12:53.380 --> 00:12:57.640
Camera buffers from the
camera are natively YUV,

00:12:57.640 --> 00:13:01.480
but most image processing
algorithms rely on RGBA data.

00:13:01.480 --> 00:13:05.330
So we have to convert between the two,
which isn't free.

00:13:07.150 --> 00:13:12.220
There is dedicated hardware to do
this in both the iPad and the iPhone,

00:13:12.270 --> 00:13:16.400
but it takes some time, and additionally,
it requires more memory

00:13:16.400 --> 00:13:18.430
for intermediate buffer.

00:13:18.530 --> 00:13:22.230
So therefore, on iOS 6,
sometimes it may make sense to just

00:13:22.230 --> 00:13:27.510
pass in your buffer as YUV data
and let Core Image do the two

00:13:27.510 --> 00:13:31.700
reads from the two planes and
apply the color transform for you.

00:13:35.450 --> 00:13:38.880
As we discussed earlier,
performance is proportional to the

00:13:38.880 --> 00:13:41.900
number of output pixels in your render.

00:13:41.990 --> 00:13:46.270
And now with the advent
of high DPI on Mac OS X,

00:13:46.590 --> 00:13:51.710
You'll run into the situations
where you'll need to reduce this

00:13:52.800 --> 00:13:52.890
image size more and more frequently.

00:13:53.210 --> 00:13:55.890
In some applications,
you just won't be able to render

00:13:55.900 --> 00:14:00.450
your image at full resolution without
hitting performance bottlenecks.

00:14:01.550 --> 00:14:03.900
This is often the case in games.

00:14:04.050 --> 00:14:09.440
Many games on the iPad with the
2048 by 1536 screen have to render

00:14:09.440 --> 00:14:14.110
at half size or .75 size and scale
up in order to get their 30 or 60

00:14:14.120 --> 00:14:16.690
frames per second target frame rate.

00:14:16.910 --> 00:14:19.890
So we recommend rendering to
a smaller size and letting

00:14:19.920 --> 00:14:23.510
Core Animation scale up very cheaply.

00:14:28.320 --> 00:14:33.650
In iOS 6 and Mac OS X.8,
we're now deprecating CI context

00:14:33.650 --> 00:14:36.980
draw image at point from rect.

00:14:37.010 --> 00:14:41.860
This is because at point is ambiguous,
especially in high DPI contexts.

00:14:42.770 --> 00:14:47.940
The preferred API now is CI Context
Draw Image in Rect from Rect.

00:14:48.620 --> 00:14:53.690
Core Image always treats its images
with a pixel-based coordinate system for

00:14:53.690 --> 00:14:57.330
extents and distances and radii and such.

00:14:57.400 --> 00:15:00.490
And so the firm-rec coordinates
will always be pixel-based.

00:15:00.810 --> 00:15:03.000
The in-rec coordinates, however,
depend on the type of

00:15:03.050 --> 00:15:05.000
context you've created.

00:15:05.180 --> 00:15:08.680
GL and GL ES based contexts
don't know about points,

00:15:08.680 --> 00:15:11.380
so those will always be pixel based.

00:15:11.400 --> 00:15:15.880
However, if you create a CI context
with a CG context on the Mac,

00:15:15.900 --> 00:15:18.220
it will be based on points.

00:15:18.250 --> 00:15:21.530
Be sure to double check your
code for this if you're hitting

00:15:21.530 --> 00:15:22.980
high DPI issues while rendering.

00:15:26.090 --> 00:15:29.560
So let's go back to the demo and try
to get our app running at 30 frames per

00:15:29.560 --> 00:15:33.230
second with the last two modifications.

00:15:35.840 --> 00:15:41.230
So this is the same as the previous code,
except we're going to change two things.

00:15:41.600 --> 00:15:44.940
First of all,
we're going to have the camera stream

00:15:44.940 --> 00:15:51.240
in its native YUV 420 bi-planar
format instead of 32-bit BGRA.

00:15:51.340 --> 00:15:53.870
And all we have to do is
basically pass in a different

00:15:54.380 --> 00:15:56.530
key value for this dictionary.

00:15:56.910 --> 00:16:00.800
The second thing we're going to
do is disable color management.

00:16:01.000 --> 00:16:07.020
We can do this by passing in KCI image
color space to be null whenever we create

00:16:07.020 --> 00:16:09.720
the CI image from the CVA pixel buffer.

00:16:11.180 --> 00:16:14.620
And we also have to set the
context color space to be null,

00:16:14.620 --> 00:16:20.580
with KCI context working space to be
null when we're creating the context.

00:16:20.690 --> 00:16:23.960
So with these changes,
let's see what the app looks like now.

00:16:25.420 --> 00:16:28.920
And this is actually running at
30 frames per second finally.

00:16:28.970 --> 00:16:31.000
And we're basically done.

00:16:31.090 --> 00:16:35.410
And I'll show you some
great demos if you all .

00:16:36.900 --> 00:16:39.700
Thanks, David.

00:16:39.700 --> 00:16:44.510
So let's start out with the major changes
we've added to iOS 6 in Core Image.

00:16:45.280 --> 00:16:49.520
So we're always trying to make filter
rendering faster on desktop and embedded.

00:16:49.530 --> 00:16:51.750
But for iOS 6,
we've actually gone ahead and made the

00:16:51.780 --> 00:16:53.880
code generation a little more optimized.

00:16:53.970 --> 00:16:57.680
So throughout any of your code,
if you're using Core Image to render,

00:16:57.680 --> 00:17:01.000
you should see slightly
faster rendering times.

00:17:01.000 --> 00:17:04.380
We've also gone ahead and
fixed our OpenGL ES integration

00:17:04.380 --> 00:17:06.340
to be a little bit nicer.

00:17:06.340 --> 00:17:11.750
There were some quirks in iOS 5 and 5.1,
and those should be gone now in iOS 6.

00:17:11.760 --> 00:17:14.970
And as David mentioned earlier,
we have 93 filters now.

00:17:14.980 --> 00:17:19.040
And two of the most requested filters
were Gaussian Blur and Laneco Scaling,

00:17:19.040 --> 00:17:21.290
and both of those are now in.

00:17:22.960 --> 00:17:24.900
So a quick note about Gaussian Blur.

00:17:24.910 --> 00:17:28.920
It was the most requested
filter in our bug reports,

00:17:29.150 --> 00:17:31.400
and now it's in iOS 6.

00:17:31.400 --> 00:17:35.170
It is multi-pass, though,
and it can be somewhat slow if you use

00:17:35.170 --> 00:17:38.490
it on large radius blurs on large images.

00:17:38.820 --> 00:17:43.210
So if you're trying to do, say,
a Gaussian Blur of a 50 radius on the

00:17:43.220 --> 00:17:48.840
back camera of an iPad 3 or the new iPad,
it can probably be a little

00:17:49.270 --> 00:17:51.950
too slow for real-time use.

00:17:52.310 --> 00:17:54.830
and since we do have Gaussian Blur,
we have several other

00:17:54.830 --> 00:17:55.780
filters that use it.

00:17:56.040 --> 00:18:01.640
In iOS 6, we have CI Bloom, CI Gloom,
Unsharp Mask, and Sharpen Luminance.

00:18:01.720 --> 00:18:04.170
And all four of these filters
use Gaussian Blur as their

00:18:04.840 --> 00:18:07.550
internal implementation.

00:18:09.130 --> 00:18:11.540
We also have Lanco scaling now in iOS 6.

00:18:11.840 --> 00:18:17.000
If you've tried to use affine scaling
to downsample images in iOS 5 or 5.1,

00:18:17.110 --> 00:18:20.160
you may have noticed bad artifacting
or an aliasing in your results.

00:18:20.160 --> 00:18:23.510
Lanco's scale transform is
much better for downsampling,

00:18:23.560 --> 00:18:27.430
and it's comparable with Core Graphics'
high-quality resampling mode.

00:18:27.440 --> 00:18:30.250
But it's done on the GPU,
so it should be faster.

00:18:30.260 --> 00:18:32.720
As an example,
I have this image here with some

00:18:32.720 --> 00:18:35.050
high-frequency content on the ground.

00:18:35.920 --> 00:18:38.790
I'm going to downsample it
to 10% of the original size,

00:18:38.790 --> 00:18:41.110
and then zoom it back up so you
can see what the differences are.

00:18:43.290 --> 00:18:48.260
So on the left is the result from
using just a CI Affine Transform,

00:18:48.270 --> 00:18:51.210
and on the right is the new
CI Lanco Scale Transform.

00:18:51.380 --> 00:18:54.280
And as you can see,
it's very alias on the left side

00:18:54.710 --> 00:18:56.440
and a lot nicer on the right side.

00:18:56.440 --> 00:19:00.540
So if you're using Core Image to do any
type of thumbnail-ing or downsampling,

00:19:00.670 --> 00:19:02.740
you should consider using
Lanco Scale Transform instead

00:19:02.740 --> 00:19:05.020
of Affine Transform.

00:19:06.050 --> 00:19:08.980
All right, so for the next part,
I'm going to demonstrate how to

00:19:08.980 --> 00:19:13.570
use Core Image to write a really
performant real-time photo or video app.

00:19:15.780 --> 00:19:19.790
You may be aware that photo apps are
really popular now on the App Store,

00:19:19.790 --> 00:19:22.270
and they could make you a billionaire
if you write the right one.

00:19:22.490 --> 00:19:26.920
CPUs are really fast enough to
do complex real-time effects

00:19:26.960 --> 00:19:29.300
on the iPad and the iPhone.

00:19:29.370 --> 00:19:32.050
And even if you're writing a photo app,
it's nice to have live

00:19:32.050 --> 00:19:36.270
video preview of the effect,
so having a real-time 30-frame-per-second

00:19:36.270 --> 00:19:39.200
preview for photo app can be important.

00:19:39.270 --> 00:19:43.980
And Core Image is ideal for this type
of real-time image processing work.

00:19:45.690 --> 00:19:50.280
So let's say I'm on vacation and
I take this photo of my friend Groot.

00:19:50.580 --> 00:19:51.840
It's a little too nice, I think.

00:19:51.840 --> 00:19:55.360
The detail's too nice,
it's a little too bright.

00:19:55.400 --> 00:19:58.000
It looks like stock photography,
in my opinion.

00:19:58.020 --> 00:20:00.650
I want it to look a little more human,
a little more weathered,

00:20:00.650 --> 00:20:03.840
like it's been sitting in
my wallet for 15 years.

00:20:04.130 --> 00:20:06.700
So I want to apply a set of filters
on this image and get something

00:20:06.700 --> 00:20:10.300
a little bit more cool looking,
something a little bit better.

00:20:10.950 --> 00:20:13.340
So how do we get a
vintage look out of this?

00:20:13.370 --> 00:20:15.600
Well, we'll do it in four steps.

00:20:15.620 --> 00:20:18.640
We'll first apply a color transformation
to make it slightly more yellowish

00:20:18.840 --> 00:20:21.140
and reduce the dynamic range.

00:20:21.190 --> 00:20:25.800
We'll apply a vignette to make it a
little nice and dark on the edges,

00:20:25.820 --> 00:20:28.970
add some film scratches,
and then some sort of border

00:20:28.970 --> 00:20:31.020
so it looks nice and framed.

00:20:31.050 --> 00:20:34.560
Using Core Image,
we can do this with four filters.

00:20:34.560 --> 00:20:38.160
We'll start out with the CI Color Cube,
which I'll talk about more in a moment.

00:20:38.350 --> 00:20:40.120
And this, as you can tell,
makes it slightly more

00:20:40.510 --> 00:20:42.140
older vintage look.

00:20:44.050 --> 00:20:46.400
Add a vignette.

00:20:46.410 --> 00:20:51.440
We'll use CI Lighten Blend Mode with the
daguerreotype texture to give it a nice

00:20:51.460 --> 00:20:54.600
model kind of grizzled look to the film.

00:20:54.630 --> 00:20:56.630
And then we'll use
Source Over Compositing to

00:20:56.650 --> 00:20:59.320
put a nice frame around this.

00:21:00.720 --> 00:21:05.010
And this is something I am now proud
to share on Facebook with my friends.

00:21:05.130 --> 00:21:07.090
And this is something that
really captures the moment

00:21:07.100 --> 00:21:12.410
better than stock photography,
SLR, or camera-type image, I think.

00:21:13.580 --> 00:21:15.630
So, before I go on,
I'd like to talk a little

00:21:15.630 --> 00:21:17.270
bit about CI Color Cube.

00:21:17.490 --> 00:21:19.570
As you saw in the previous talk,
it can be used for a

00:21:19.570 --> 00:21:21.040
wide variety of effects.

00:21:21.260 --> 00:21:23.700
It's one of our most flexible filters.

00:21:23.750 --> 00:21:27.200
And just about any color effect
that doesn't rely on the position

00:21:27.200 --> 00:21:30.400
of a pixel in its calculations,
only on the color,

00:21:30.410 --> 00:21:32.900
can be approximated using CI Color Cube.

00:21:32.970 --> 00:21:35.560
And since we implement this
with two texture reads and

00:21:35.560 --> 00:21:38.330
a little bit of arithmetic,
it's often faster than a

00:21:38.500 --> 00:21:39.900
pure algorithmic filter.

00:21:40.010 --> 00:21:43.350
If you're doing some sort of, like,
polynomial approximation or some

00:21:43.350 --> 00:21:45.400
sort of complex calculation.

00:21:45.430 --> 00:21:50.400
And on iOS 6, we support up to a 64
by 64 by 64 color cube,

00:21:50.400 --> 00:21:54.290
which turns out to be more than
enough for almost any effect.

00:21:55.880 --> 00:21:59.240
So as an example,
I have CI sepia tone here on the left

00:21:59.250 --> 00:22:05.000
and a 64 by 64 by 64 approximation
of it using CI Color Cube.

00:22:05.220 --> 00:22:08.790
As you can probably tell,
they're pretty much the same.

00:22:09.350 --> 00:22:13.140
We wrote CI Sepia Tone with
accuracy as the main concern.

00:22:13.190 --> 00:22:17.290
And so if you use Color Cube,
you'll get something a little bit faster.

00:22:17.480 --> 00:22:20.090
In most cases,
you won't actually notice the difference,

00:22:20.090 --> 00:22:23.890
and so if you want a real-time app,
you might consider switching to

00:22:23.890 --> 00:22:27.070
Color Cube instead of using CISepiaTone.

00:22:28.190 --> 00:22:33.080
Even when we drop down to an 8x8x8 cube,
the results are pretty much the same.

00:22:33.210 --> 00:22:36.300
This is because the linear approximation
we use in Color Cube is still

00:22:36.300 --> 00:22:40.450
accurate enough to approximate the
curves in our CDI sepia tone filter.

00:22:40.920 --> 00:22:45.520
It's only when we drop down to
2x2x2 that we get some sort of

00:22:45.910 --> 00:22:48.300
incorrect coloring in our effect.

00:22:48.360 --> 00:22:51.640
A 2x2x2 cube is essentially
the same as a CI color matrix,

00:22:51.740 --> 00:22:54.350
so you could write it with that as well.

00:22:56.320 --> 00:23:00.080
So let's try to write a
real-time CameraFX app.

00:23:00.110 --> 00:23:03.360
And our first attempt,
we'll use AV Foundation for the input.

00:23:03.390 --> 00:23:05.540
We'll filter it with Core Image.

00:23:05.590 --> 00:23:09.270
And let's use the
URImageView to display it.

00:23:09.340 --> 00:23:12.300
Instead of actually calling
CI explicitly to render,

00:23:12.330 --> 00:23:18.240
we'll just set the URImageView's
image to be a wrapped CI image.

00:23:18.330 --> 00:23:20.640
So let's go to the demo machine.

00:23:34.300 --> 00:23:39.800
[Transcript missing]

00:23:42.180 --> 00:23:42.810
Capital or?

00:23:43.340 --> 00:23:45.640
Capital Apple?

00:23:45.650 --> 00:23:46.480
Okay.

00:23:46.480 --> 00:23:49.150
I can't type apparently.

00:23:49.220 --> 00:23:52.370
Apple1234?

00:23:52.680 --> 00:23:54.000
What is this password?

00:23:54.020 --> 00:23:55.400
Okay.

00:23:58.280 --> 00:24:01.540
So this is an extremely simple setup.

00:24:01.640 --> 00:24:04.560
We'll have a single
UI image view for display,

00:24:04.810 --> 00:24:07.860
a capture session to stream to frames.

00:24:08.830 --> 00:24:14.010
This is all kind of boilerplate code
to stream frames from AV Foundation.

00:24:15.840 --> 00:24:17.420
We'll set up our filters once.

00:24:17.460 --> 00:24:20.040
In our color cube,
I've generated offline,

00:24:20.040 --> 00:24:22.130
and I have like a--

00:24:22.590 --> 00:24:24.810
Pre-populated table with the data.

00:24:24.940 --> 00:24:27.780
And so the most interesting part
comes in the AV Foundation callback

00:24:27.780 --> 00:24:28.610
in the main loop.

00:24:28.780 --> 00:24:33.370
So what happens in every frame
is we capture the pixel buffer,

00:24:33.770 --> 00:24:35.510
We'll create a CI image from it.

00:24:35.710 --> 00:24:38.960
Because pixel buffers come in
from AV Foundation rotated,

00:24:39.020 --> 00:24:42.860
we'll have to rotate it by 90
degrees and zero the origin.

00:24:43.010 --> 00:24:47.090
We have our filter chain that we set up,
the color cube, vignette, blend,

00:24:47.180 --> 00:24:48.100
and source over.

00:24:48.200 --> 00:24:51.790
Finally, we'll grab the image
from the filter chain,

00:24:51.950 --> 00:24:55.800
wrap it in a UI image,
and set it on the image view.

00:24:55.870 --> 00:24:59.800
Note that we haven't even used
CI context yet for this implementation.

00:25:00.100 --> 00:25:04.970
And one nice thing at the end is
we have to zero out or nil out the

00:25:04.970 --> 00:25:09.220
input images for the filters because
otherwise they'll retain the CV pixel

00:25:09.220 --> 00:25:13.550
buffer from the previous frame and
might cause higher memory usage.

00:25:13.680 --> 00:25:16.990
So if we have this implementation,
let's see how it runs.

00:25:17.000 --> 00:25:23.480
I'm going to have to use the projector,
unfortunately.

00:25:28.440 --> 00:25:32.160
And as you can tell, this is pretty slow.

00:25:32.220 --> 00:25:35.750
And there's some tearing as well,
which I probably have to go fix.

00:25:36.850 --> 00:25:40.570
So this runs at maybe five
or six frames per second,

00:25:40.610 --> 00:25:43.990
so it's definitely not good
enough for real-time work.

00:25:51.910 --> 00:25:55.800
Why was this implementation so slow?

00:25:55.860 --> 00:25:59.800
Well, UI Image View is optimized
for static images.

00:25:59.860 --> 00:26:03.310
It may not be ideal for real-time video.

00:26:03.680 --> 00:26:07.150
Furthermore, if you could actually run an
instruments trace on this,

00:26:07.160 --> 00:26:11.980
you would see that CI context is
being created for every single frame.

00:26:12.090 --> 00:26:15.300
This is because your image view doesn't
know when your rendering's gonna stop.

00:26:15.330 --> 00:26:18.870
They don't wanna hold onto a
context longer than they need to.

00:26:19.210 --> 00:26:20.610
And if you could do
another Instruments Trace,

00:26:20.610 --> 00:26:24.840
you'd see that CG Context
Draw Image is being called a lot.

00:26:24.930 --> 00:26:29.210
So maybe there's not the most efficient
drawing going on in the background.

00:26:29.510 --> 00:26:31.450
In order to get something
more performant,

00:26:31.520 --> 00:26:34.270
we should probably drop down
to our lower-level API and

00:26:34.270 --> 00:26:36.370
try to tweak the performance.

00:26:37.230 --> 00:26:40.180
So for our second attempt,
we'll still use

00:26:40.190 --> 00:26:42.180
AV Foundation for the input.

00:26:42.330 --> 00:26:44.030
We'll still use Core Image.

00:26:44.190 --> 00:26:47.740
But instead of using UI Image View to
handle the rendering,

00:26:47.800 --> 00:26:51.280
we'll do it ourselves by calling
CI Context Create CG Image and

00:26:51.480 --> 00:26:53.160
set that on the Image View.

00:26:53.200 --> 00:26:56.920
And hopefully this will
be a little bit faster.

00:26:56.980 --> 00:26:59.490
So if I switch to the demo machine.

00:27:14.940 --> 00:27:18.690
So this is basically the same code.

00:27:18.770 --> 00:27:23.050
The only difference is instead of
wrapping the CI image in a UI image,

00:27:23.050 --> 00:27:29.190
I'm manually creating a CG image
and wrapping that in a UI image.

00:27:29.850 --> 00:27:32.340
And so this is a minor change,
and you might not think that this

00:27:32.340 --> 00:27:33.950
would actually change things.

00:27:33.970 --> 00:27:36.070
But if we go to the machine,

00:27:41.770 --> 00:27:44.900
We see that this actually
runs pretty quickly.

00:27:44.960 --> 00:27:46.720
And so we're done.

00:27:46.860 --> 00:27:50.140
But actually,
this is not 30 frames per second.

00:27:50.330 --> 00:27:54.000
- If we actually open up
instruments or the console,

00:27:54.000 --> 00:27:56.620
we'll see that we actually
have frame drops in this thing.

00:27:56.640 --> 00:28:00.060
This is running at approximately 21,
22 frames per second.

00:28:00.100 --> 00:28:03.380
And the front camera streams
at 30 frames per second.

00:28:03.420 --> 00:28:06.100
So this is still suboptimal.

00:28:07.330 --> 00:28:10.100
And as a note,
everything on this-- every demo

00:28:10.100 --> 00:28:14.030
is running on iPad 2 to highlight
the GPU differences because the

00:28:14.100 --> 00:28:18.050
iPad 3 is a lot faster GPU-wise.

00:28:24.710 --> 00:28:27.450
So why is performance still non-ideal?

00:28:27.600 --> 00:28:29.640
So what is happening for every frame?

00:28:29.900 --> 00:28:32.080
We have our input image.

00:28:32.290 --> 00:28:34.840
We uploaded to OpenGL as a texture.

00:28:34.970 --> 00:28:36.910
And in this case,
even though we're using the

00:28:37.530 --> 00:28:40.750
optimal path of mapping a
CV pixel buffer to a texture,

00:28:40.980 --> 00:28:44.350
there is still a cost
associated with that.

00:28:46.010 --> 00:28:49.580
OpenGL then renders the
result to the render buffer.

00:28:49.630 --> 00:28:52.450
And then when you call createCgImage,
what happens is there's a

00:28:52.530 --> 00:28:56.660
GL readPixels callback that
brings data back to the CPU.

00:28:56.730 --> 00:28:59.600
And then finally,
when you set that on the URI Image View,

00:28:59.640 --> 00:29:04.040
Core Animation has to upload that
image again to the GPU for display.

00:29:04.080 --> 00:29:06.440
And so there's a lot of extraneous
uploads and downloads here

00:29:06.440 --> 00:29:08.490
that really hurts performance.

00:29:09.970 --> 00:29:12.300
So what is the optimal approach?

00:29:12.320 --> 00:29:16.110
What we'd like is just to
upload the texture once and

00:29:16.110 --> 00:29:18.150
render directly to the display.

00:29:18.640 --> 00:29:20.360
There's no need to read back
because we don't actually

00:29:20.360 --> 00:29:21.260
want to save it or anything.

00:29:21.260 --> 00:29:23.970
We just want to display it to screen.

00:29:24.220 --> 00:29:25.940
So how do we do this?

00:29:25.980 --> 00:29:30.240
Well, for our third attempt,
we'll use AV Foundation again.

00:29:30.280 --> 00:29:32.040
We'll render with Core Image.

00:29:32.090 --> 00:29:36.190
But we'll render directly to the
frame buffer of a CA Eagle layer.

00:29:36.420 --> 00:29:39.630
And this is how most games
actually do their rendering via

00:29:39.690 --> 00:29:42.630
OpenGL ES and CA Eagle layers.

00:29:42.910 --> 00:29:45.700
So let's see how that works.

00:29:53.940 --> 00:29:57.130
So this is slightly different now.

00:29:57.170 --> 00:30:02.060
I have a GL ES view and an Eagle context.

00:30:02.100 --> 00:30:04.200
And when I'm setting things up,

00:30:06.280 --> 00:30:10.820
I'm actually creating a CI context,
context with Eagle Context.

00:30:10.860 --> 00:30:14.880
That way, CI can draw to the destination
frame buffer of your

00:30:14.880 --> 00:30:18.800
GL ES view's Eagle Context.

00:30:19.380 --> 00:30:21.940
The main loop is pretty much the same.

00:30:21.990 --> 00:30:26.740
We still take a CVPixel buffer,
rotate and zero the origin,

00:30:26.780 --> 00:30:30.010
pass it through the filter chain,
except now we're drawing it to the

00:30:30.010 --> 00:30:36.130
screen directly with the CI context
Draw Image Interact Firmware Act API.

00:30:36.220 --> 00:30:38.620
So let's see how fast this runs.

00:30:46.410 --> 00:30:49.100
And this is actually pretty poor.

00:30:49.220 --> 00:30:54.170
I realize I'm blocking
the thing with my hand.

00:30:54.900 --> 00:30:57.720
It's running at about
13 frames per second,

00:30:57.720 --> 00:31:01.400
which is actually worse
than the previous example.

00:31:01.450 --> 00:31:03.790
So we must be doing something wrong.

00:31:11.520 --> 00:31:13.560
So whenever you have performance
issues in Core Image,

00:31:13.630 --> 00:31:18.000
you have to keep in mind that performance
is tied to the render output size.

00:31:18.050 --> 00:31:21.540
For the first two examples,
we had a 640 by 480 input image,

00:31:21.560 --> 00:31:25.000
and we were rendering to the
same size 640 by 480 destination,

00:31:25.040 --> 00:31:30.620
which is then upscaled to the full res,
which is 1024 by 768.

00:31:30.680 --> 00:31:33.390
For this example,
we're actually rendering

00:31:34.200 --> 00:31:47.700
[Transcript missing]

00:31:49.800 --> 00:32:12.600
[Transcript missing]

00:32:24.700 --> 00:32:44.100
[Transcript missing]

00:32:51.470 --> 00:32:54.280
While they fix that,
what you would see is it would be

00:32:54.280 --> 00:32:57.990
running at 28 frames per second
instead of 30 frames per second.

00:32:59.140 --> 00:33:02.810
Which is still not ideal because, again,
input video runs at 30 frames per

00:33:02.840 --> 00:33:04.980
second for the front video camera.

00:33:27.040 --> 00:33:30.340
So this runs at around
28 frames per second.

00:33:30.350 --> 00:33:32.460
You actually really have to go
into the console to see that

00:33:32.460 --> 00:33:37.490
it's not performing optimally,
and there are frame drops going on.

00:33:37.570 --> 00:33:41.070
So what can we do at this
point to improve performance?

00:33:46.990 --> 00:33:49.060
There are three things
we can do at this point.

00:33:49.130 --> 00:33:50.130
Well,
we talked about one of them already,

00:33:50.130 --> 00:33:52.300
which is reducing the render size.

00:33:52.370 --> 00:33:55.280
The other two are disabling
color management and

00:33:55.280 --> 00:33:59.170
leveraging YUV image support,
which is new in iOS 6.

00:34:01.180 --> 00:34:03.970
So by default,
Core Image performs all its calculations

00:34:04.060 --> 00:34:07.930
in a light linear color space,
as David mentioned in the earlier talk.

00:34:08.080 --> 00:34:12.980
And this provides the most accurate and
consistent results for your renders.

00:34:13.040 --> 00:34:16.140
But this is not a cheap operation,
necessarily.

00:34:16.210 --> 00:34:20.600
As you can see from example code,
we have a mix, pow, step function,

00:34:20.600 --> 00:34:23.890
and some more arithmetic every
time we have to convert a pixel

00:34:24.220 --> 00:34:27.870
from sRGB to linear RGB and back.

00:34:28.090 --> 00:34:30.030
And so if you have a lot of
images and you're doing a lot of

00:34:30.030 --> 00:34:32.760
these conversions back and forth,
this could really

00:34:32.760 --> 00:34:34.570
affect your performance.

00:34:35.190 --> 00:34:39.610
So if you absolutely need the highest
possible frame rate or performance,

00:34:39.610 --> 00:34:41.560
and you don't really
notice the differences in

00:34:41.560 --> 00:34:44.340
disabling color management,
for example, in Photo Booth,

00:34:44.390 --> 00:34:47.320
if you're rendering
thermal effect or X-ray,

00:34:47.520 --> 00:34:53.090
you probably won't notice that the
RGB values are four or five units off.

00:34:53.180 --> 00:34:55.250
So if you don't really care
about that slight difference,

00:34:55.270 --> 00:34:59.410
you can disable color management
and get faster rendering.

00:35:02.530 --> 00:35:05.610
New in iOS 6 is YUV image support.

00:35:05.770 --> 00:35:12.040
So camera buffers from the iPad and
iPhone natively come in as YUV 420.

00:35:12.080 --> 00:35:15.580
And if you specify that
you want BGRA frames,

00:35:15.630 --> 00:35:18.380
there is a conversion that happens.

00:35:18.410 --> 00:35:22.660
There is a built-in hardware
chip that does this conversion,

00:35:22.660 --> 00:35:24.080
but it's not free.

00:35:24.130 --> 00:35:26.860
And there is a higher memory usage
involved because you have to have

00:35:26.860 --> 00:35:31.340
another intermediate buffer at every,
for every frame.

00:35:31.600 --> 00:35:36.220
And since most image processing
algorithms expect RGBA data,

00:35:36.280 --> 00:35:38.770
This conversion can be kind of costly.

00:35:38.980 --> 00:35:42.390
Well, on iOS 6 Core Image,
we can read directly from YUV pixel

00:35:42.390 --> 00:35:46.700
buffers from the two planes and
do the color conversion ourselves.

00:35:46.750 --> 00:35:48.560
This way, you can save on memory.

00:35:48.820 --> 00:35:51.800
And in some cases, a lot of cases,
it's actually faster than having

00:35:51.800 --> 00:35:55.760
the hardware convert and doing
the reads from an RGB buffer.

00:35:55.860 --> 00:35:58.460
So be sure to test this if
you're just on the cusp of

00:35:58.480 --> 00:36:00.920
getting your app to be real time.

00:36:00.970 --> 00:36:05.760
And this might help your app gain maybe
like two or three frames per second.

00:36:07.920 --> 00:36:11.840
We mentioned this earlier in the demo,
but if you're a renderer,

00:36:11.840 --> 00:36:14.930
you've tried everything else and
your rendering is still too slow,

00:36:15.100 --> 00:36:18.430
you might have to reduce the render
size and scale up your image.

00:36:18.530 --> 00:36:20.170
This is something that
games actually do a lot,

00:36:20.170 --> 00:36:22.280
especially on high DPI screens.

00:36:22.470 --> 00:36:28.430
If you're trying to run a complex
3D game on a 2048 by 1536 screen,

00:36:28.670 --> 00:36:31.450
chances are it's not
going to be too fast.

00:36:31.540 --> 00:36:37.460
So what games do a lot of times is render
at half res or 1.75 res and upscale.

00:36:37.550 --> 00:36:40.110
And you can do the same
thing with Core Image.

00:36:43.090 --> 00:36:45.410
In our example demo,
we rendered at 640x480

00:36:45.840 --> 00:36:48.750
for the CA Eagle layer,
and we had Core Animation scale

00:36:48.750 --> 00:36:49.940
it up to screen size.

00:36:50.010 --> 00:36:52.460
And Core Animation does
this extremely efficiently,

00:36:52.460 --> 00:36:54.510
and it's essentially free.

00:36:56.150 --> 00:37:01.740
As a side note, now that there are high
DPI screens on Mac OS and embedded,

00:37:01.800 --> 00:37:06.500
we're deprecating CI context
draw image at point from rect.

00:37:06.520 --> 00:37:09.180
This is because this is
a little bit ambiguous.

00:37:09.210 --> 00:37:13.550
At point doesn't really specify the
destination size you want to render to.

00:37:13.600 --> 00:37:16.260
Instead,
we want you to use CI context draw

00:37:16.270 --> 00:37:19.580
image in rect from rect instead.

00:37:19.930 --> 00:37:22.430
and as you probably know,
everything in Core Image is

00:37:22.440 --> 00:37:26.740
pixel based for extents,
for radii, for filters, stuff like that.

00:37:26.850 --> 00:37:31.220
So from rect in this call is
always in pixel coordinates.

00:37:31.300 --> 00:37:35.380
But in rect can change depending on
what type of context you're creating.

00:37:35.430 --> 00:37:37.530
Naturally,
if you're doing a GL based context,

00:37:37.660 --> 00:37:42.140
the CGL context on desktop
or Eagle context on embedded,

00:37:42.170 --> 00:37:48.100
GL doesn't have a concept of scale,
so it's always pixel based for in rect.

00:37:48.180 --> 00:37:52.340
If you're using a CG context on desktop,
however, and not on iOS,

00:37:52.380 --> 00:37:55.390
CG context do have a concept of scale.

00:37:55.480 --> 00:38:00.950
So you want to use points for CI context,
draw image, in rect, from rect.

00:38:01.110 --> 00:38:03.400
So this tripped up a lot of developers,
and this is something you

00:38:03.470 --> 00:38:06.900
should keep in mind when you're
testing your app on OS X,

00:38:06.900 --> 00:38:12.030
10.8, or iOS 6.

00:38:12.110 --> 00:38:14.500
And so to demonstrate the
last set of techniques,

00:38:14.500 --> 00:38:16.670
I have one final demo.

00:38:23.660 --> 00:38:29.600
So this is the same as the previous demo
that did rendering to the Eagle context.

00:38:29.760 --> 00:38:35.360
Instead of specifying the pixel format
to be 32 BGRA for AV Foundation,

00:38:35.430 --> 00:38:41.710
I wanted to return the native
420 YCBCR bi-planar format.

00:38:42.480 --> 00:38:45.810
And then when we're creating the image,
we can just pass in the

00:38:45.810 --> 00:38:50.540
PIS buffer just like before,
except we're going to pass in an option,

00:38:50.600 --> 00:38:56.960
CI Image Color Space to be null to
disable color management on that image.

00:38:57.260 --> 00:39:00.060
And the last thing we need to do
is disable color management on the

00:39:00.120 --> 00:39:04.610
context by specifying CI context,
context with Eagle context

00:39:04.690 --> 00:39:08.680
with the option working color
space to be null as well.

00:39:09.030 --> 00:39:11.960
And with those three options in place,

00:39:18.890 --> 00:39:25.600
We finally have a 30 frames per
second app that renders the filter

00:39:25.600 --> 00:39:28.070
shape we wanted in the beginning.

00:39:34.350 --> 00:39:38.070
We started off with a really naive
implementation using UI Image View.

00:39:38.260 --> 00:39:40.340
We got six frames per second.

00:39:40.390 --> 00:39:46.080
We had a pretty good jump to 21 when
we just used Create CG Image explicitly

00:39:46.290 --> 00:39:48.150
instead of having UIKit render.

00:39:48.650 --> 00:39:52.300
And then we fell back to 13 when we
tried the first Eagle Contacts approach.

00:39:52.350 --> 00:39:55.060
This is because we
rendered at the wrong size.

00:39:55.160 --> 00:39:57.580
And then once we fixed that,
we got to 28.

00:39:57.640 --> 00:40:01.060
And then by disabling color management
and enabling YUV image support,

00:40:01.090 --> 00:40:04.180
we got to our target
30 frames per second.

00:40:13.100 --> 00:40:15.340
For the next part,
I'll talk a little bit about

00:40:15.340 --> 00:40:19.370
how to leverage OpenGL ES and
Core Image at the same time for some

00:40:19.370 --> 00:40:22.340
more advanced rendering techniques.

00:40:23.360 --> 00:40:25.090
So as you may know from
the previous example,

00:40:25.110 --> 00:40:30.460
you can create a CI context with
a user-supplied Eagle context.

00:40:30.770 --> 00:40:33.770
And what happens internally is
we create our own Eagle context

00:40:34.230 --> 00:40:36.200
with the same share group.

00:40:36.200 --> 00:40:39.380
And if you're familiar with OpenGL,
you know that if you create

00:40:39.390 --> 00:40:43.200
one with the same share group,
you can share resources between the two.

00:40:43.200 --> 00:40:47.940
In practice, or in theory,
this could be anything from shaders,

00:40:48.090 --> 00:40:51.500
programs, vertex buffers, vertex arrays.

00:40:51.500 --> 00:40:55.400
But the things we're primarily
concerned with in CI are textures

00:40:55.400 --> 00:40:58.660
and frame buffers/render buffers.

00:40:59.100 --> 00:41:02.650
And so using this, this

00:41:03.000 --> 00:41:09.900
[Transcript missing]

00:41:10.140 --> 00:41:13.940
creating CI images from textures,
which is new in iOS 6.

00:41:13.980 --> 00:41:18.880
The API is CI Image, Image with Texture,
with options for size, flipped,

00:41:18.880 --> 00:41:20.000
and color space.

00:41:20.020 --> 00:41:22.770
And the texture ID you pass in
is basically just an unsigned int

00:41:22.840 --> 00:41:24.550
that refers to that GL texture.

00:41:24.720 --> 00:41:29.970
And so this image is only usable at
render time if that texture ID is valid.

00:41:30.200 --> 00:41:33.000
It refers to a texture that
exists in the share group.

00:41:33.270 --> 00:41:37.510
And the advantage of doing this is
that the texture is kept on the GPU.

00:41:37.550 --> 00:41:41.780
There's no unnecessary downloads
and re-uploads of data here.

00:41:41.830 --> 00:41:46.590
So Core Image can use that
texture directly and very cheaply.

00:41:46.720 --> 00:41:49.990
And be sure that the texture data
is valid when you're rendering.

00:41:50.430 --> 00:41:54.250
Otherwise you might
get undesired results.

00:41:54.600 --> 00:41:58.980
We can't actually retain a GL texture,
so if you create a

00:41:58.980 --> 00:42:01.240
CI image with a texture,
make sure you don't delete

00:42:01.280 --> 00:42:04.480
the texture underneath this
when you're trying to use it.

00:42:05.540 --> 00:42:09.840
And now in iOS 6,
we can also render two textures.

00:42:10.020 --> 00:42:12.520
In iOS 5 and 5.1,
we unfortunately limited

00:42:12.790 --> 00:42:15.810
to render buffers,
but now anything that you attach to

00:42:15.850 --> 00:42:18.040
a frame buffer can be rendered to.

00:42:18.220 --> 00:42:20.110
And so it's pretty easy to do this.

00:42:20.270 --> 00:42:24.440
When you're setting up your context,
just bind a texture to the frame buffer,

00:42:24.570 --> 00:42:28.760
call the same CI context drawImage,
indirect from rect,

00:42:28.930 --> 00:42:31.800
and we will draw to that texture instead.

00:42:31.870 --> 00:42:35.240
And currently, only rendering to 8-bit
RGBA textures is supported,

00:42:35.240 --> 00:42:38.840
so you can't render to a
YUV or whatever buffer.

00:42:42.330 --> 00:42:47.310
Because we can render to texture now,
it's advantageous to make

00:42:47.600 --> 00:42:50.250
draw image to be asynchronous.

00:42:50.560 --> 00:42:56.640
Calls like createCGImage, renderToBuffer,
renderToCVPixelBuffer have always

00:42:56.640 --> 00:42:59.660
been synchronous in the past,
but this is not something

00:42:59.720 --> 00:43:01.300
we want for GL rendering.

00:43:01.330 --> 00:43:04.580
So in iOS 6,
we're changing it to be asynchronous.

00:43:04.610 --> 00:43:09.460
We will only issue a GL flush after
our render and not a GL finish anymore.

00:43:09.890 --> 00:43:12.860
If you wrote your app and you linked
against the old version of Core Image,

00:43:12.940 --> 00:43:15.000
we'll continue to maintain
the same behavior,

00:43:15.060 --> 00:43:19.330
but we advise you to update your
app to iOS 6 for better performance.

00:43:19.970 --> 00:43:23.900
And when you're using textures,
rendering from or to textures,

00:43:23.940 --> 00:43:28.160
you should keep the OpenGL ES bind
flush best practices in mind.

00:43:28.300 --> 00:43:31.890
And basically what you need to remember
is if you have an object on context

00:43:31.890 --> 00:43:35.290
A that you're using or modifying,
when you're done modifying it,

00:43:35.300 --> 00:43:38.240
issue a flush,
and context B has to rebind that

00:43:38.240 --> 00:43:40.220
object before it can see the changes.

00:43:40.340 --> 00:43:43.030
So that's all you have to basically do.

00:43:43.920 --> 00:43:47.450
To demonstrate this,
I'll take the basic Xcode

00:43:48.010 --> 00:43:50.300
OpenGL ES app template.

00:43:50.300 --> 00:43:54.300
I'll modify it slightly to use
AV Foundation to stream in frames.

00:43:54.300 --> 00:43:59.560
And I'll use a CVOpenGL ES texture
cache to map pixel buffers to textures.

00:43:59.890 --> 00:44:02.950
That way I can demonstrate
creating CI images from textures,

00:44:03.160 --> 00:44:07.300
rendering it to another texture with
the new render to texture functionality,

00:44:07.470 --> 00:44:11.600
and then just using some more
OpenGL ES shader and draw

00:44:11.600 --> 00:44:15.300
functions to render a bunch of
cubes with that modified texture.

00:44:15.300 --> 00:44:18.990
So I will demonstrate this real quick.

00:44:27.440 --> 00:44:31.220
So we will make this
sample code pretty quickly,

00:44:31.220 --> 00:44:36.290
but this is basically the OpenGL app
template that draws two rotating cubes.

00:44:37.150 --> 00:44:41.330
What we're going to do for every
frame that we get from the camera

00:44:42.140 --> 00:44:47.530
is grab-- use CVOpenGLESTextureCache
to grab the texture

00:44:47.700 --> 00:44:49.960
and map it as a OpenGL texture.

00:44:50.020 --> 00:44:53.200
Sorry, grab the buffer and map
it as an OpenGL texture.

00:44:53.240 --> 00:44:57.590
And then in our draw function,
what we're going to do is pass

00:44:58.350 --> 00:45:01.990
that texture in as a CI image

00:45:02.360 --> 00:45:07.170
Again, rotate-- or sorry, in this case,
merely crop it because we want a

00:45:07.310 --> 00:45:09.930
square texture and not 640 by 480.

00:45:10.060 --> 00:45:14.060
And we'll have a pixelate filter
and a CI vortex distortion to

00:45:14.060 --> 00:45:18.220
modify that image and have the
parameters change in real time.

00:45:18.630 --> 00:45:20.360
When we're rendering,
what we need to do is save

00:45:20.840 --> 00:45:25.020
the old framebuffer binding,
bind our texture to-- our destination

00:45:25.020 --> 00:45:30.720
texture to this framebuffer,
call CI context to render to the texture,

00:45:31.790 --> 00:45:35.940
Restore the old frame buffer
and then draw a bunch of cubes

00:45:35.940 --> 00:45:37.940
with this as the new texture.

00:45:38.050 --> 00:45:40.010
So let's see what this looks like.

00:45:46.580 --> 00:45:53.150
This is kind of hard to see,
but I have cubes with every face has a

00:45:53.150 --> 00:45:57.650
constantly changing vortex distortion
and a pixelate that scales up and down.

00:45:57.660 --> 00:46:02.180
And this runs at 30 frames per second,
and I can actually add probably a

00:46:02.220 --> 00:46:06.200
bunch more cubes since the hard work
is running to that texture once,

00:46:06.240 --> 00:46:10.330
and the rest is a really easy
OpenGL cube drawing process

00:46:10.330 --> 00:46:12.640
with really simple shaders.

00:46:12.640 --> 00:46:15.620
So back to slides.

00:46:23.210 --> 00:46:26.800
For the next part,
I'd like to invite Jacques on

00:46:26.800 --> 00:46:30.260
stage to talk about how you can
integrate Core Image with your

00:46:30.530 --> 00:46:32.340
game for some cool effects.

00:46:32.360 --> 00:46:34.340
Thanks.

00:46:34.350 --> 00:46:35.330
All right.

00:46:35.360 --> 00:46:36.410
Thank you.

00:46:39.270 --> 00:46:40.040
Thank you, Chendi.

00:46:40.040 --> 00:46:44.390
That's an interesting slide title there.

00:46:44.650 --> 00:46:48.290
So I'd like to talk to you about
two of the common use cases for

00:46:48.290 --> 00:46:50.970
Core Image Techniques in your games.

00:46:51.080 --> 00:46:54.410
The first one that most of you
will probably try is to apply

00:46:54.420 --> 00:46:56.330
an effect to the full screen.

00:46:56.340 --> 00:46:58.230
This is probably the easiest for you.

00:46:58.270 --> 00:47:00.050
Let's say you have a sprite-based game.

00:47:00.110 --> 00:47:02.020
You just want to create a
transition or something.

00:47:02.070 --> 00:47:04.960
It's very easy to throw
in a full screen effect.

00:47:05.300 --> 00:47:09.240
The second one is to apply an
effect to an individual texture.

00:47:09.240 --> 00:47:12.800
And this can really be a great way for
you to use an expensive filter as a

00:47:12.800 --> 00:47:16.240
one-off to create a more advanced effect.

00:47:16.270 --> 00:47:19.390
And then because you only
did it once at startup,

00:47:19.560 --> 00:47:23.730
you can retain that sort of the
real-time aspect of your game and

00:47:23.930 --> 00:47:26.240
still have an expensive filter.

00:47:26.240 --> 00:47:30.110
So let's just focus on the first one,
which is to apply an

00:47:30.220 --> 00:47:32.240
effect to the full screen.

00:47:32.240 --> 00:47:35.240
I'm going to go over here and
demo one of those for you.

00:47:45.000 --> 00:47:50.600
[Transcript missing]

00:47:51.320 --> 00:47:56.040
Using some of these recipes
that Alex gave us earlier,

00:47:56.040 --> 00:48:02.740
I'm going to take this kind of vanilla
space shooter game that I've made.

00:48:02.740 --> 00:48:05.920
It's, you know,
it's kind of boring looking.

00:48:05.970 --> 00:48:07.300
I already got inspired.

00:48:07.330 --> 00:48:10.280
I added Gaussian blur as a full
screen effect so you can tell

00:48:10.280 --> 00:48:12.540
I can no longer interact with it.

00:48:13.300 --> 00:48:30.700
[Transcript missing]

00:48:31.300 --> 00:48:33.260
Okay, that's kind of cool.

00:48:33.260 --> 00:48:34.490
That kind of takes us into the game.

00:48:34.500 --> 00:48:37.140
Now you can see, okay, there's a segue.

00:48:37.220 --> 00:48:38.640
Game over.

00:48:38.640 --> 00:48:40.640
Okay, that was kind of snappy.

00:48:40.670 --> 00:48:42.960
All right,
let's try something else there.

00:48:43.010 --> 00:48:43.580
Pixelate.

00:48:43.580 --> 00:48:45.180
We've already tried that.

00:48:45.200 --> 00:48:47.740
Okay, let's try Bar Swipe.

00:48:47.740 --> 00:48:48.690
Cool.

00:48:48.980 --> 00:48:50.180
We do the same thing.

00:48:50.250 --> 00:48:51.600
We get into the game.

00:48:51.610 --> 00:48:54.520
The interesting thing here is
with the full screen effect,

00:48:54.580 --> 00:48:57.000
it's helping you add a
transition into the game that

00:48:57.040 --> 00:48:58.500
you didn't have to do yourself.

00:48:58.510 --> 00:49:01.180
So a lot of the times,

00:49:01.450 --> 00:49:03.970
You might engineer your
own kind of fade to black,

00:49:04.090 --> 00:49:05.170
fade to white, whatever.

00:49:05.290 --> 00:49:09.070
Throw in the standard game programmer
transition because I don't want

00:49:09.070 --> 00:49:10.520
to write the shader for it.

00:49:10.570 --> 00:49:17.890
But you can use CI really easily with
these recipes to add a bar swipe,

00:49:17.890 --> 00:49:19.820
apparently.

00:49:19.880 --> 00:49:23.400
Or perhaps you can't.

00:49:23.960 --> 00:49:25.970
Let's just try that again.

00:49:27.380 --> 00:49:29.780
So let's go back here.

00:49:29.780 --> 00:49:31.700
We'll try a flash
transition on the way in,

00:49:31.760 --> 00:49:33.200
because we've all seen Pixelate.

00:49:33.200 --> 00:49:35.460
I think this can be kind of cool.

00:49:35.460 --> 00:49:37.320
Oh, wow, cool.

00:49:37.320 --> 00:49:38.700
All right.

00:49:38.700 --> 00:49:41.950
And then as I die, game over.

00:49:43.160 --> 00:49:44.500
Truly game over.

00:49:44.510 --> 00:49:45.600
Okay.

00:49:45.600 --> 00:49:46.500
There you go.

00:49:46.520 --> 00:49:47.980
That's some of the
easy things you can do.

00:49:47.980 --> 00:49:51.450
Now, DCI filters are actually
extremely easy to do.

00:49:51.460 --> 00:49:53.530
There are only a few lines.

00:49:53.580 --> 00:49:56.060
I should probably go back
and turn on the slides.

00:49:56.190 --> 00:49:58.130
Let's do that.

00:50:03.300 --> 00:50:07.060
Okay.

00:50:07.100 --> 00:50:08.740
So that was applying
effects to full screen.

00:50:08.750 --> 00:50:10.040
They're very easy.

00:50:10.040 --> 00:50:12.400
It only takes you a couple
of lines to set it up.

00:50:12.420 --> 00:50:15.300
And then you can swap these filters out.

00:50:15.300 --> 00:50:19.990
And you can use Quartz Composer
to test them out offline.

00:50:20.170 --> 00:50:22.500
And then you can really hand it
over to a designer or someone

00:50:22.550 --> 00:50:24.060
to just pick the right effect.

00:50:24.260 --> 00:50:27.100
And once you have it set up,
just reuse it.

00:50:27.480 --> 00:50:30.950
So let's go through how
we actually do this.

00:50:31.030 --> 00:50:34.500
So the first one is your
standard game renders straight.

00:50:34.500 --> 00:50:38.240
It's just geocalls that
you flush to the screen.

00:50:38.550 --> 00:50:42.100
We just want to pipe that around
so that you go to a frame buffer,

00:50:42.100 --> 00:50:45.850
a texture frame buffer,
and then you put that through a

00:50:46.340 --> 00:50:48.060
filter and then to the screen.

00:50:48.140 --> 00:50:50.740
So very simple stuff.

00:50:50.780 --> 00:50:55.800
So the first one is there's
really four steps to this.

00:50:55.800 --> 00:50:59.940
The first one is you
create a texture FBO.

00:50:59.940 --> 00:51:00.660
We've gone through this.

00:51:00.660 --> 00:51:02.940
Chendi has showed this before.

00:51:02.940 --> 00:51:07.200
Number two is you create the
CI image to reference this FBO.

00:51:07.200 --> 00:51:10.560
And number three is you
want to create the filter.

00:51:10.560 --> 00:51:18.300
And it's important here that you set
the input image key to the texture.

00:51:18.300 --> 00:51:20.060
And then fourth,
you create the context to

00:51:20.060 --> 00:51:22.400
actually draw it to the screen.

00:51:22.400 --> 00:51:25.500
Okay, so we'll just briefly
go through the steps.

00:51:25.500 --> 00:51:25.970
uh...

00:51:28.170 --> 00:51:30.130
We have OS X first.

00:51:30.280 --> 00:51:35.230
You'll note that there's some
setup here to create the texture.

00:51:35.400 --> 00:51:38.160
I'm sure you all know how to do that,
so I've omitted that.

00:51:38.160 --> 00:51:42.980
You bind it, and note here,
Texture Rectangle Arb,

00:51:42.980 --> 00:51:45.060
which is special for OS X.

00:51:45.100 --> 00:51:49.450
It's a way to access an arbitrary
non-power-of-two texture.

00:51:49.600 --> 00:51:52.520
Also note that there's RGBA here.

00:51:53.490 --> 00:51:57.400
Then we just bind this
texture as a frame buffer.

00:51:57.570 --> 00:51:59.900
And then for iOS,
there's only a tiny change,

00:52:00.080 --> 00:52:03.400
which is that you use GL Texture 2D
because of the non-power-to-support

00:52:03.400 --> 00:52:06.400
that's out of the box there.

00:52:06.540 --> 00:52:09.390
Okay, so then we create the CA image.

00:52:10.910 --> 00:52:12.020
This is very simple.

00:52:12.200 --> 00:52:15.800
You just reference the texture,
make sure that the size is set to

00:52:15.800 --> 00:52:18.700
the width and height of the texture,
and then of course you

00:52:18.700 --> 00:52:20.800
don't need to flip it,
you know what's in there,

00:52:21.050 --> 00:52:23.340
and set the color space to nil.

00:52:24.440 --> 00:52:26.150
Next, we create the filter.

00:52:26.180 --> 00:52:29.550
In this case, I've chosen Bloom,
which is one of these kind of

00:52:29.560 --> 00:52:32.380
expensive Gaussian blur-based filters.

00:52:32.480 --> 00:52:35.490
Set the defaults, set the input image.

00:52:35.760 --> 00:52:37.260
Very easy.

00:52:37.800 --> 00:52:42.260
Okay, then for OS X,
you then create a context that

00:52:42.280 --> 00:52:45.050
references your current OpenGL context.

00:52:45.120 --> 00:52:50.160
I've called it NSGLCTX here for brevity.

00:52:50.670 --> 00:52:51.770
This is the OS X approach.

00:52:51.810 --> 00:52:57.700
Make a note here that I've
set the working space to null,

00:52:57.710 --> 00:53:00.430
so I'm avoiding color management.

00:53:00.810 --> 00:53:02.850
And iOS...

00:53:03.960 --> 00:53:06.200
Even simpler.

00:53:06.270 --> 00:53:09.080
Same thing, just a simpler call.

00:53:09.730 --> 00:53:15.330
Okay, so that was the first
one of the two use cases.

00:53:15.600 --> 00:53:20.940
So let's go and focus on the second case.

00:53:21.140 --> 00:53:24.860
So I'm going to walk over
here and show you a demo.

00:53:32.300 --> 00:53:36.300
So some of you might have seen
this little trees concept I've been

00:53:36.300 --> 00:53:38.980
working on in a previous session.

00:53:39.410 --> 00:53:43.640
And here I want to focus on effects
that can be quite expensive that

00:53:43.640 --> 00:53:47.540
you apply to a texture once in
order to create a deeper feeling.

00:53:47.540 --> 00:53:50.640
So here I have very simple flat geometry.

00:53:50.640 --> 00:53:54.780
There are some hand-painted textures
here and a little particle effect.

00:53:55.150 --> 00:53:59.360
One of the things I can do here
that we can go into is with a

00:53:59.400 --> 00:54:05.590
filter chain consisting of a random
generator that we then tone up

00:54:05.880 --> 00:54:10.110
to just gray and a linear gradient,
we can create this ground fog

00:54:10.260 --> 00:54:12.050
effect of this sheet that you see.

00:54:12.060 --> 00:54:14.870
We see this gray sheet
between each layer,

00:54:15.080 --> 00:54:18.820
so I can just apply a noise there,
and as I just move it,

00:54:18.950 --> 00:54:22.040
we get that little sense of movement,
and we get the ground fog happening.

00:54:22.640 --> 00:54:23.640
So that's very easy.

00:54:23.640 --> 00:54:25.200
I only need to set that up once.

00:54:25.270 --> 00:54:29.000
The random generator is an
infinitely repeating texture,

00:54:29.140 --> 00:54:33.370
so it's very easy for me to set that
up once and sort of apply it wholesale.

00:54:34.030 --> 00:54:37.740
The next step is, of course,
that these trees are very, very flat.

00:54:37.780 --> 00:54:41.140
You can see that there's no depth
cueing on the trees whatsoever.

00:54:41.250 --> 00:54:47.150
And so, either you could try just to blur
the whole screen or the whole layer,

00:54:47.150 --> 00:54:50.360
but really the easiest thing here is to
take each one of these trees that I've

00:54:50.570 --> 00:54:54.980
composited using a random generator.

00:54:55.070 --> 00:54:59.110
So it consists of three components:
a trunk, a left branch,

00:54:59.110 --> 00:55:01.660
and a right branch at varying heights.

00:55:02.050 --> 00:55:04.810
I render those out to a texture.

00:55:05.390 --> 00:55:11.890
Then I apply a Gaussian Blur filter once,
and I vary the radius depending

00:55:11.890 --> 00:55:13.350
upon the depth of that tree.

00:55:13.510 --> 00:55:16.570
Now you can see that
there's depth cueing here.

00:55:16.760 --> 00:55:22.250
So as we go deeper into the scene,
the trees are blurred out,

00:55:22.250 --> 00:55:25.460
and you get that sense of
it meshing with the fog.

00:55:25.630 --> 00:55:27.450
So depth cueing, very easy.

00:55:27.470 --> 00:55:30.280
It's only done once,
so now it's rendering using

00:55:30.280 --> 00:55:32.220
that modified texture.

00:55:32.280 --> 00:55:35.760
Then, of course,
we can go on to the next effect.

00:55:36.400 --> 00:55:45.600
[Transcript missing]

00:55:45.810 --> 00:55:50.340
Using CI Bloom,
I create this glow effect around it.

00:55:50.400 --> 00:55:54.180
Of course,
I could just create this as an asset,

00:55:54.240 --> 00:55:58.400
but it was just really easy for
me to take a circle and gloom it,

00:55:58.400 --> 00:55:59.770
or bloom it.

00:56:00.160 --> 00:56:03.550
And so this is something that
you can just try out and really

00:56:03.550 --> 00:56:06.990
get there much quicker than going
back to your artist and saying,

00:56:07.000 --> 00:56:08.730
you know, is this what you want?

00:56:08.760 --> 00:56:09.080
No.

00:56:09.140 --> 00:56:09.730
Okay, go back.

00:56:09.760 --> 00:56:11.590
Oh, okay, I added some more.

00:56:11.650 --> 00:56:12.840
Oh, is this what you want?

00:56:12.840 --> 00:56:13.140
No.

00:56:13.190 --> 00:56:14.220
You can just play with it.

00:56:14.350 --> 00:56:19.210
The CI filters have input parameters
that you can play with as a programmer

00:56:19.690 --> 00:56:21.750
to get the effect you're after.

00:56:21.890 --> 00:56:24.850
The next one is, of course,
a film effect.

00:56:24.970 --> 00:56:28.600
So we're using vignette
here on the whole screen.

00:56:28.640 --> 00:56:30.940
So we're doing a full-screen
effect at the same time,

00:56:31.000 --> 00:56:33.830
which is vignette,
and we also transform the

00:56:33.830 --> 00:56:37.700
screen slightly to get a jitter,
a camera jitter in.

00:56:38.160 --> 00:56:41.330
And then, of course,
we vary the luminance a little bit,

00:56:41.330 --> 00:56:44.340
just to get that kind of
incandescent light bulb errors

00:56:44.340 --> 00:56:46.000
that you'd get in an old projector.

00:56:46.180 --> 00:56:49.470
And then you get the
kind of old-style effect.

00:56:49.600 --> 00:56:51.200
And then, of course,
another full-screen filter

00:56:51.200 --> 00:56:54.400
that we applied in the demo
earlier was a distortion.

00:56:54.480 --> 00:56:57.000
And Chendi's shown you
a distortion earlier.

00:56:57.000 --> 00:57:00.200
This one is a bump
distortion that you can use.

00:57:00.330 --> 00:57:03.360
And this one can either go
into or out of the page.

00:57:03.450 --> 00:57:07.690
In this case,
I decided to go out of the page.

00:57:08.090 --> 00:57:09.140
And this is really easy.

00:57:09.160 --> 00:57:12.180
Yet again, once you've set it up,
it's only a few lines to try

00:57:12.180 --> 00:57:15.290
out a different filter or try
out a different filter chain.

00:57:15.870 --> 00:57:17.300
Okay.

00:57:17.750 --> 00:57:19.320
So...

00:57:21.130 --> 00:57:22.810
What's the idea here?

00:57:22.910 --> 00:57:26.330
Well, you have your input geometry.

00:57:26.350 --> 00:57:28.670
I've made some space there.

00:57:28.810 --> 00:57:31.880
The texture coordinates, positions,
and your indices,

00:57:31.880 --> 00:57:33.940
they don't have to change, of course.

00:57:34.140 --> 00:57:36.440
It's just the texture that
you're going to modify.

00:57:36.470 --> 00:57:42.060
So we move it over, prepare some space,
create a new texture for

00:57:42.110 --> 00:57:46.940
the text cords to reference,
and then we apply a Core Image filter

00:57:46.940 --> 00:57:50.040
between the original texture
and the filtered texture.

00:57:50.050 --> 00:57:53.130
Now the great thing about this
is if you keep this set up,

00:57:53.150 --> 00:57:56.660
you could modify that original texture,
such as changing a frame

00:57:56.660 --> 00:57:59.190
in a frame-based animation,
and at that point,

00:57:59.630 --> 00:58:03.410
filter again to get the filtered texture,
so you have this kind of beautiful

00:58:03.520 --> 00:58:06.960
amortized cost of saying,
"I can apply this expensive filter chain,

00:58:06.960 --> 00:58:10.300
Gaussian Blur, which is a multi-pass,
to my filtered texture."

00:58:10.340 --> 00:58:13.630
And you really only have to do
that when the content changes.

00:58:13.890 --> 00:58:17.530
And everything else stays the same,
so you just render it out.

00:58:17.870 --> 00:58:21.050
Okay,
so the workflow is the same as before,

00:58:21.160 --> 00:58:23.040
just in a slightly different order.

00:58:23.120 --> 00:58:26.160
So you create the
CI image using a texture,

00:58:26.160 --> 00:58:27.640
the original texture.

00:58:27.720 --> 00:58:30.380
You create the CI filter
that you intend to use,

00:58:30.410 --> 00:58:33.580
Gaussian Blur in the case
of the game and the trees.

00:58:33.630 --> 00:58:38.720
And then you create a texture FBO to
render this filtered texture out to.

00:58:38.750 --> 00:58:41.440
And that is the new texture
that you're going to use.

00:58:41.510 --> 00:58:44.670
You create a CI context as before,
or you might already have one

00:58:44.760 --> 00:58:47.540
around from your previous trials.

00:58:47.840 --> 00:58:51.730
And then you target the new texture
by binding that frame buffer when

00:58:51.730 --> 00:58:54.090
you render with the CI context.

00:58:54.600 --> 00:59:00.000
So I've just shown you here a
small example of the differences.

00:59:00.090 --> 00:59:04.300
So really all the same as before,
except you just bind the texture

00:59:04.390 --> 00:59:08.920
frame buffer that you're targeting
and you do your CI context.

00:59:09.320 --> 00:59:15.040
Draw Image in Rect from Rect using
the output image of the filter.

00:59:15.080 --> 00:59:19.160
And there you will have your new
texture that you can then use.

00:59:19.160 --> 00:59:23.470
So there's some great features
in Core Image for your games.

00:59:24.060 --> 00:59:25.900
You have 93 combineable filters.

00:59:25.900 --> 00:59:27.240
They're all made for you.

00:59:27.290 --> 00:59:30.770
Please don't try to make your
own Gaussian Blur filter.

00:59:30.840 --> 00:59:33.020
Please save yourself the time.

00:59:33.110 --> 00:59:34.860
It's really not that fun.

00:59:34.940 --> 00:59:37.380
So if you combine these and change,
you can get billions

00:59:37.380 --> 00:59:40.220
of different effects,
and it's really fun to play with.

00:59:40.280 --> 00:59:43.530
And because in iOS 6 now,
you have rendered to and

00:59:43.530 --> 00:59:46.050
from textures as well,
you can also get high

00:59:46.090 --> 00:59:48.220
performance to add to your game.

00:59:48.260 --> 00:59:51.090
And of course, as we've shown you,
there are some properties and

00:59:51.130 --> 00:59:54.400
options that you can set so that
you can select between quality or

00:59:54.400 --> 00:59:58.220
performance to suit your application.

00:59:58.230 --> 01:00:02.100
Okay, so I'm going to hand
you back over to David.

01:00:07.530 --> 01:00:08.900
Thank you so much, Jacques.

01:00:08.900 --> 01:00:11.410
It's fun working on
these filters at Apple,

01:00:11.410 --> 01:00:14.470
but it's even more terrific to
actually see them being applied to

01:00:14.570 --> 01:00:17.580
really kind of beautiful-looking
content in a real live game.

01:00:17.580 --> 01:00:18.800
It's fun to see.

01:00:18.800 --> 01:00:21.870
Really appreciate his
team's help on that.

01:00:23.970 --> 01:00:26.020
Few things we want to
just tip you off about.

01:00:26.080 --> 01:00:29.820
We have a session that was preceded
this session that you might want

01:00:29.820 --> 01:00:31.620
to look for on the archives.

01:00:31.620 --> 01:00:34.340
It was getting started in
Core Image if you want to get all

01:00:34.390 --> 01:00:36.120
the fundamentals of Core Image.

01:00:36.160 --> 01:00:40.520
Also, there's another session you
might want to check out as well,

01:00:40.520 --> 01:00:44.870
which is Advances in OpenGL and GL ES,
which is another good session

01:00:44.870 --> 01:00:48.760
to look at for tips and tricks
with OpenGL programming.

01:00:48.760 --> 01:00:50.640
That's all.

01:00:53.800 --> 01:00:55.880
Thank you all again for coming,
and look forward to

01:00:55.880 --> 01:00:57.050
talking with you again.