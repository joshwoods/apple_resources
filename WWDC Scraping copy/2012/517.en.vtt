WEBVTT

00:00:10.450 --> 00:00:14.300
Good morning.

00:00:14.300 --> 00:00:17.530
It's so great to see so many of you here.

00:00:19.610 --> 00:00:26.500
So by now, many of you have gained some
familiarity with AV Foundation.

00:00:26.500 --> 00:00:30.810
And perhaps you've even used
AV Foundation successfully

00:00:30.810 --> 00:00:32.590
in your applications.

00:00:33.010 --> 00:00:38.870
What we're going to discover
today is the new features that

00:00:38.870 --> 00:00:42.270
we've built into the framework,
we've been working hard on

00:00:42.270 --> 00:00:45.490
this over the last year,
that allow you to customize

00:00:45.490 --> 00:00:49.480
and really control the playback
experience for your users.

00:00:50.500 --> 00:00:56.340
I'm Simon, and I'm part of the
Media Systems team at Apple.

00:00:56.340 --> 00:01:00.330
And we're going to cover
three topics today.

00:01:01.150 --> 00:01:06.070
The first topic is how can
you tightly coordinate,

00:01:06.180 --> 00:01:09.880
tightly synchronize events
in your application,

00:01:09.940 --> 00:01:16.980
like gameplay and generated audio,
with video playback or playback

00:01:17.040 --> 00:01:19.720
in general with AV Foundation.

00:01:20.960 --> 00:01:26.860
The second topic is how can you
achieve realtime audio effects,

00:01:26.960 --> 00:01:31.970
realtime audio visualization
in your application.

00:01:32.270 --> 00:01:37.420
And then finally, the third topic,
something that I know many of you

00:01:37.420 --> 00:01:43.330
have been requesting for some time,
is the ability to do the same for video.

00:01:43.450 --> 00:01:46.410
Real-time video effects,
real-time processing

00:01:46.980 --> 00:01:50.110
on both iOS and Mac OS.

00:01:52.410 --> 00:01:57.410
Now, there's a common goal
to all three of these.

00:01:57.680 --> 00:02:02.840
And if we don't achieve that goal
when building these applications,

00:02:02.840 --> 00:02:07.670
we lose the magic of a movie.

00:02:08.920 --> 00:02:11.450
And that ingredient
that we have to achieve

00:02:11.680 --> 00:02:17.550
is ensuring precise,
accurate audiovisual synchronization.

00:02:17.960 --> 00:02:19.060
We're going to come back.

00:02:19.090 --> 00:02:21.460
I'm going to highlight how
you can achieve that in

00:02:21.460 --> 00:02:24.230
all three of these topics.

00:02:27.700 --> 00:02:31.540
There are some prereqs for this session.

00:02:31.540 --> 00:02:38.190
We'll need familiarity with the objects
that I introduced this session last year.

00:02:38.290 --> 00:02:42.700
The static model objects,
AV asset and the AV asset track.

00:02:42.760 --> 00:02:48.460
The dynamic at play time,
dynamic play time objects,

00:02:48.460 --> 00:02:53.700
the AV player item and
the AV player item track.

00:02:53.810 --> 00:02:57.670
You also need to be familiar
with the media controller,

00:02:57.670 --> 00:02:58.930
the AV player.

00:03:03.560 --> 00:03:07.200
I showed you this diagram last year.

00:03:07.240 --> 00:03:16.260
AV Foundation sits below the application
toolkits UIKit on iOS and AppKit on OS X.

00:03:16.360 --> 00:03:20.380
And it sits above the core frameworks,
Core Audio, Core Media,

00:03:20.380 --> 00:03:22.060
and Core Animation.

00:03:22.150 --> 00:03:27.370
But today, I'd like to introduce two
new public frameworks to you.

00:03:28.930 --> 00:03:32.610
Media Toolbox and Video Toolbox.

00:03:33.280 --> 00:03:38.070
And functions from these
frameworks start with VT and MT.

00:03:38.690 --> 00:03:43.750
We're giving you these frameworks because
they provide low-level functionality that

00:03:43.750 --> 00:03:49.790
will help enhance the customization and
enhance the experience for your users.

00:03:50.310 --> 00:03:52.260
But first,
let me take this opportunity to

00:03:52.260 --> 00:03:55.800
give you a quick playback update.

00:03:55.860 --> 00:04:00.690
Enhancements to playback in
AV Foundation since last year.

00:04:00.990 --> 00:04:03.910
Last year,
I gave you this recipe for achieving

00:04:03.960 --> 00:04:07.500
basic playback in just four easy steps.

00:04:07.610 --> 00:04:11.970
The first step was to
create your AV URL asset.

00:04:12.140 --> 00:04:17.870
Then, using the AV asynchronous
key value loading protocol,

00:04:17.870 --> 00:04:19.970
you loaded the tracks.

00:04:20.500 --> 00:04:22.960
Once your tracks have
been successfully loaded,

00:04:23.060 --> 00:04:26.000
you created your AV player item.

00:04:26.120 --> 00:04:30.510
And then with that AV player item,
you associated it finally

00:04:30.510 --> 00:04:32.230
with your AV player.

00:04:32.860 --> 00:04:35.190
Now,
given the feedback from developers like

00:04:35.290 --> 00:04:39.840
yourselves here at DubDub and since then,
we've made it even easier.

00:04:40.210 --> 00:04:44.700
In fact, we made it easier even in iOS 5.

00:04:44.800 --> 00:04:49.910
You can now achieve basic
playback in just three easy steps.

00:04:51.320 --> 00:04:55.790
But if your application needs to
discover the tracks over your asset,

00:04:55.790 --> 00:05:00.090
because you need to know about the
collection of video tracks or the

00:05:00.370 --> 00:05:04.510
collection of audio tracks to do
some logic in your application,

00:05:04.850 --> 00:05:11.770
you still need to load,
using the AV asynchronous

00:05:11.770 --> 00:05:11.800
key value loading protocol,
the tracks property.

00:05:12.080 --> 00:05:19.470
If you don't, on iOS,
you risk RPC timeout.

00:05:19.470 --> 00:05:29.590
And on OS X, you risk your application
becoming unresponsive and your

00:05:29.590 --> 00:05:29.590
users will experience the spod,
the spinning pinwheel of death.

00:05:30.430 --> 00:05:32.900
So let's move on.

00:05:33.420 --> 00:05:36.160
Before we can talk about how
to customize the playback

00:05:36.240 --> 00:05:43.050
experience for your applications,
I need to introduce you to some

00:05:43.050 --> 00:05:48.730
synchronization primitives,
some objects that will help you

00:05:48.730 --> 00:05:48.730
customize that playback experience and
tightly control the synchronization.

00:05:51.030 --> 00:05:53.940
Last year,
the classes that we talked about

00:05:53.940 --> 00:05:58.430
for playback were the AV Player,
the AV Player Item,

00:05:58.430 --> 00:06:01.010
and the AV Player Layer.

00:06:01.750 --> 00:06:04.590
AV synchronization wasn't something
that you needed to deal with.

00:06:04.740 --> 00:06:07.300
It wasn't something that concerned you.

00:06:07.350 --> 00:06:11.660
AV Foundation did the hard work of
ensuring the right image was displayed

00:06:11.660 --> 00:06:16.410
in your AV player layer and audio that
was coming out of the audio device

00:06:16.410 --> 00:06:19.170
was magically and perfectly in sync.

00:06:19.910 --> 00:06:24.330
But to do this,
your AV player item was managing separate

00:06:25.050 --> 00:06:28.110
video and audio pipelines for you.

00:06:31.980 --> 00:06:36.640
Now, to get all these objects
perfectly in sync,

00:06:36.640 --> 00:06:43.640
they all need a common sense of time,
a common source of time measurement.

00:06:43.740 --> 00:06:47.180
And as you probably know,
like we do in the real world,

00:06:47.340 --> 00:06:50.260
the source of time
measurement is a clock.

00:06:50.660 --> 00:06:53.460
But in our computer systems today,
we have multiple clocks.

00:06:53.520 --> 00:06:57.590
On iOS,
you have a host clock in your SOC.

00:06:57.930 --> 00:07:02.050
You've got an audio clock
in your audio device.

00:07:02.190 --> 00:07:04.480
And on Mac OS, you have much the same.

00:07:04.480 --> 00:07:07.950
You have an audio clock for
every core audio device that's

00:07:07.950 --> 00:07:09.720
attached to your system.

00:07:09.720 --> 00:07:13.200
And you have a host time
clock associated with the CPU.

00:07:13.200 --> 00:07:18.240
To get all these objects
perfectly synchronized,

00:07:18.240 --> 00:07:21.380
they all need a common clock.

00:07:23.370 --> 00:07:27.160
And so we're introducing
to you two new objects,

00:07:27.390 --> 00:07:31.170
the CM Clock and the CM Timebase.

00:07:35.940 --> 00:07:40.780
So a CM clock is a source
of time measurement.

00:07:40.860 --> 00:07:45.330
There are two types,
the host clock and the audio clock.

00:07:45.510 --> 00:07:50.700
Now the time base extends on
this notion by introducing rate.

00:07:50.830 --> 00:07:53.840
It's rate with respect to time.

00:07:54.320 --> 00:07:57.170
Time bases are something that's
under the control either directly

00:07:57.280 --> 00:07:59.900
or indirectly by your application.

00:08:00.050 --> 00:08:03.280
And time bases can be slaved to a master.

00:08:03.330 --> 00:08:08.800
They can be slaved to another
time base or directly to a clock.

00:08:08.850 --> 00:08:12.110
Let's take a look at a
picture that describes this.

00:08:14.260 --> 00:08:15.920
Time always marches on.

00:08:16.100 --> 00:08:20.120
Time constantly progresses so
long as your system is powered.

00:08:20.190 --> 00:08:23.440
Now, currently, the time base is static.

00:08:23.440 --> 00:08:28.520
It's not moving with the clock
because its rate is set to zero.

00:08:28.650 --> 00:08:33.390
If we set the rate to one to
commence regular playback,

00:08:35.320 --> 00:08:39.400
The time base values now progress
in lock step with the clock.

00:08:39.560 --> 00:08:43.550
For every one value on the time base,
there is an equivalent

00:08:43.830 --> 00:08:45.600
value on the clock.

00:08:51.060 --> 00:08:57.040
If our user now pushes the fast forward
button and sets the rate to two,

00:08:57.160 --> 00:09:01.650
we see that values on the time base now
progress at twice the rate of the clock.

00:09:01.800 --> 00:09:07.440
For every clock value,
there are two time base values.

00:09:09.900 --> 00:09:17.400
[Transcript missing]

00:09:19.290 --> 00:09:22.120
Then we see a sort of mesmerizing
pattern on the screen,

00:09:22.130 --> 00:09:25.710
and we have the clock still marching
in the same direction that it

00:09:25.800 --> 00:09:30.130
has been before at the same rate,
but now the time base is moving

00:09:30.190 --> 00:09:33.470
in the opposite direction
with values decrementing.

00:09:40.500 --> 00:09:44.860
How do you get these
CM time bases and CM clocks?

00:09:44.860 --> 00:09:54.060
Well, on iOS,
to create a reference to the audio clock,

00:09:54.060 --> 00:09:54.060
you can call CMAudioClockCreate.

00:09:55.500 --> 00:10:01.970
On OS X, you can optionally specify a
Core Audio Device UID to get a clock

00:10:02.220 --> 00:10:06.660
that references that specific device.

00:10:10.030 --> 00:10:14.140
On either platform,
to get a reference to the host clock,

00:10:14.250 --> 00:10:18.020
you call cm_clock_get_host_time_clock.

00:10:19.000 --> 00:10:23.150
To get a reference to the time base,
we've added a new property

00:10:23.290 --> 00:10:25.100
on the AVPlayer item.

00:10:26.690 --> 00:10:31.780
And you can also create a time base and
slave it directly to a clock by calling

00:10:31.780 --> 00:10:35.080
cm_timebase_create with master_clock.

00:10:35.080 --> 00:10:39.780
And you can use this for an
AV sample buffer display layer.

00:10:42.070 --> 00:10:47.040
Now that we've armed ourselves with
the knowledge of how to create clocks,

00:10:47.040 --> 00:10:50.600
create time bases,
and their relationship to one another,

00:10:50.640 --> 00:10:57.460
and their relationship with time,
let's use this knowledge to tightly

00:10:57.540 --> 00:11:05.550
synchronize events in your application,
such as gameplay, with video playback.

00:11:05.820 --> 00:11:09.280
In these series of slides,
what I want to show you is how

00:11:09.350 --> 00:11:13.460
to synchronize video playback
with our own generated audio.

00:11:13.470 --> 00:11:17.890
Your application might use the
audio toolbox to do this directly.

00:11:18.350 --> 00:11:20.940
Let's return to this diagram.

00:11:21.030 --> 00:11:24.610
And let me add back in the
AV Player and the AV Player layer

00:11:24.610 --> 00:11:26.540
where our video will be displayed.

00:11:26.650 --> 00:11:31.690
And let me introduce to you
new API on the AV Player that

00:11:31.690 --> 00:11:37.690
allow us to synchronize with
now your own generated audio.

00:11:38.430 --> 00:11:41.260
It's custom,
so you generate the audio yourself

00:11:41.310 --> 00:11:44.310
using the audio toolbox APIs.

00:11:44.850 --> 00:11:50.840
And that, by generating that,
you're driving the CM Audio Clock.

00:11:51.050 --> 00:11:56.340
What we want to do now is tell
AV Foundation you need to synchronize

00:11:56.340 --> 00:12:01.810
your playback with that clock
so that we all stay in sync.

00:12:03.520 --> 00:12:06.400
What's the relationship
between these two?

00:12:06.430 --> 00:12:10.630
How do we tell AV Foundation that's
the clock we need to do our time

00:12:10.630 --> 00:12:16.230
calculations to and that's where
playback needs to be synchronized to?

00:12:17.210 --> 00:12:19.990
To do that, it's really easy.

00:12:20.080 --> 00:12:23.700
There's a new property on
the AV Player Master Clock.

00:12:23.810 --> 00:12:29.250
Once you set that property to
a clock that you have created,

00:12:30.270 --> 00:12:33.900
AV Foundation will ensure that
all time calculations occur

00:12:33.900 --> 00:12:36.340
with respect to that clock.

00:12:36.630 --> 00:12:40.180
or playback will be in
sync with that clock.

00:12:40.180 --> 00:12:43.340
So that's a great way to stay in sync.

00:12:43.460 --> 00:12:47.370
But how do we start in
sync in the first instance?

00:12:48.010 --> 00:12:52.660
For that,
you can call setRateTimeAtHostTime.

00:12:52.700 --> 00:12:56.790
We'll take a look at this
in just a quick moment.

00:12:57.600 --> 00:13:02.260
By calling setRate at time at host time,
what you're instructing AV Foundation to

00:13:02.350 --> 00:13:13.030
do is tightly coordinate so that
when you're playing out your audio,

00:13:13.030 --> 00:13:14.580
we'll set the rate to 1 at the
exact same host time that you

00:13:14.580 --> 00:13:14.580
anticipate audio hitting the speaker.

00:13:17.040 --> 00:13:23.400
Now, this API takes host time
in a CM time structure.

00:13:23.480 --> 00:13:26.660
And to convert that,
we provide a utility, CM Clock,

00:13:26.660 --> 00:13:29.700
make host time from system units.

00:13:29.780 --> 00:13:33.860
Once you've used this utility
to convert into CM time,

00:13:34.050 --> 00:13:37.800
you can now use set rate
item time at host time.

00:13:37.920 --> 00:13:41.890
And what you're doing by calling this API

00:13:43.240 --> 00:13:49.500
is your declaring to AV Foundation that
there exists a pair of times,

00:13:49.540 --> 00:13:54.260
one on the time-based timeline
and one on the clock timeline,

00:13:54.290 --> 00:13:57.940
that occur at the same moment in time.

00:13:57.970 --> 00:14:04.470
Once you've made this declaration,
all the times in the past

00:14:05.030 --> 00:14:08.670
and in the future now can be
calculated by AV Foundation.

00:14:10.490 --> 00:14:17.500
You can set the rate for both a past
host time as well as a future host time.

00:14:18.300 --> 00:14:23.700
If you set rate for a past host time,

00:14:23.900 --> 00:14:27.260
What AV Foundation will do
is it will pretend as if,

00:14:27.420 --> 00:14:33.180
it will behave as if playback had been
going from that past time all along.

00:14:33.320 --> 00:14:39.680
It will quickly catch up, load the media,
and start playback as if it had been

00:14:39.680 --> 00:14:42.930
running since that time in the past.

00:14:43.430 --> 00:14:48.280
But if you want to commence playback
in the not too distant future,

00:14:48.290 --> 00:14:51.010
you want to be able to give
AV Foundation the opportunity to

00:14:51.360 --> 00:14:58.880
load up some media and prepare
itself for that pending playback.

00:15:00.210 --> 00:15:03.020
And to do that, we provide two APIs.

00:15:03.140 --> 00:15:09.060
You can ask your AV player to pre-roll
for some playback rate and then execute

00:15:09.160 --> 00:15:11.930
a completion handler when it's done.

00:15:13.280 --> 00:15:16.110
This is an asynchronous operation.

00:15:16.160 --> 00:15:22.370
So you can also ask the AV player to
cancel that operation if your user backs

00:15:22.370 --> 00:15:26.310
out of UI or stops playback altogether.

00:15:28.300 --> 00:15:40.390
Let's take a look now at a coding example
of where we can tightly synchronize

00:15:40.390 --> 00:15:40.390
video playback using AV Foundation with
audio that you generate yourself.

00:15:42.160 --> 00:15:46.430
You get a reference to your audio clock
where you want AV Foundation to do all

00:15:46.430 --> 00:15:50.350
its video playback to be synchronized to.

00:15:51.710 --> 00:15:55.380
You tell the AV player,
that's the clock where I need you to do

00:15:55.380 --> 00:15:59.090
your time calculations with respect to,
and that's where we should

00:15:59.090 --> 00:16:01.510
synchronize our video playback to.

00:16:03.600 --> 00:16:09.280
To get AV Foundation to pre-roll and
get ready for this future host time,

00:16:09.360 --> 00:16:11.830
we say pre-roll at rate,
and the playback rate

00:16:11.950 --> 00:16:14.430
in this case is one,
regular playback,

00:16:14.540 --> 00:16:18.190
and then execute a completion
handler when it's done.

00:16:18.750 --> 00:16:23.550
When that completion handler
fires and it was successful,

00:16:23.550 --> 00:16:30.150
we calculate a future host time
for when we anticipate our audio

00:16:30.150 --> 00:16:30.150
to hit the speaker and finally,

00:16:31.020 --> 00:16:32.580
We set the rate to 1.

00:16:32.600 --> 00:16:35.870
We ask AV Foundation to set
the rate to 1 when that future

00:16:35.870 --> 00:16:38.380
host time becomes current.

00:16:42.850 --> 00:16:48.070
So now that we can tightly synchronize
our own audio with video playback

00:16:48.540 --> 00:16:53.610
or our own gameplay with video
playback using AV Foundation,

00:16:53.810 --> 00:16:58.580
How can we manipulate audio
in an audiovisual item or an

00:16:58.580 --> 00:17:01.620
audio-only item during playback?

00:17:01.620 --> 00:17:04.940
How can we affect those samples,
add an effect,

00:17:04.940 --> 00:17:08.580
or just simply visualize them,
draw a waveform?

00:17:14.900 --> 00:17:26.200
[Transcript missing]

00:17:27.200 --> 00:17:33.980
And what we want to do here is we sort
of want to tap into that audio pipeline.

00:17:33.980 --> 00:17:39.110
We want to intercept those decoded
audio samples as they're on their

00:17:39.110 --> 00:17:41.430
way out to the audio device.

00:17:42.510 --> 00:17:46.550
We want AV Foundation to
do the audio decode for us,

00:17:46.550 --> 00:17:49.260
but we just want to get
at those audio samples,

00:17:49.260 --> 00:17:52.950
manipulate them a little bit,
and then send them on their way.

00:17:53.740 --> 00:17:57.460
And for that,
we're introducing a new object,

00:17:57.490 --> 00:18:00.200
the MT Audio Processing Tap.

00:18:00.230 --> 00:18:05.130
It's your opportunity for
doing customized audio

00:18:05.370 --> 00:18:09.120
effects using AV Foundation.

00:18:13.340 --> 00:18:19.740
To associate the MT Audio Processing
tap to our AV Player item,

00:18:19.760 --> 00:18:23.550
we're going to use an object
that you might be familiar with,

00:18:23.820 --> 00:18:26.290
and that's the AV Audio Mix.

00:18:27.420 --> 00:18:30.000
If you're unfamiliar
with the AV Audio Mix,

00:18:30.000 --> 00:18:34.750
you can pop back to last year's
session and re-familiarize yourself.

00:18:37.200 --> 00:18:41.690
To configure an AV audio mix,
like we did last year for audio

00:18:41.690 --> 00:18:43.600
ducking and audio volume ramping.

00:18:45.460 --> 00:18:52.410
We use an AV audio mix input parameters
object and we can configure that for

00:18:52.410 --> 00:18:57.800
each AV asset track that our asset has.

00:18:57.910 --> 00:19:01.640
Now to do the association with
the empty audio processing tap,

00:19:01.720 --> 00:19:08.370
we've introduced a new
property on that object,

00:19:08.370 --> 00:19:08.370
the audio tap processor.

00:19:10.900 --> 00:19:18.000
even yet told you how to create
an empty audio processing tap.

00:19:18.000 --> 00:19:18.000
Let's take a look at that now.

00:19:19.150 --> 00:19:25.360
To create an empty audio processing tap,
which is a CF type, and even under ARC,

00:19:25.360 --> 00:19:30.570
you need to manage its lifetime
with retains and releases,

00:19:30.570 --> 00:19:34.710
you call empty audio
processing tap create.

00:19:38.080 --> 00:19:46.950
And to configure this object,
you fill out a structure called the

00:19:46.950 --> 00:19:46.950
MT Audio Processing Tap Callbacks.

00:19:47.420 --> 00:19:52.190
This structure defines a
set of function pointers,

00:19:52.410 --> 00:19:57.370
callbacks into your application,
which will get executed throughout

00:19:57.510 --> 00:19:58.850
the lifetime of the tap.

00:20:03.140 --> 00:20:07.380
I want to focus on the last
three of these function pointers.

00:20:07.410 --> 00:20:12.690
The prepare, unprepare, and finally,
the process callback.

00:20:13.220 --> 00:20:18.980
The prepared callback is paired
with an unprepared callback.

00:20:20.310 --> 00:20:23.550
In the prepared callback,
it's your opportunity

00:20:23.870 --> 00:20:28.940
to allocate buffers,
to create objects that you'll

00:20:28.940 --> 00:20:32.300
use during audio processing.

00:20:32.460 --> 00:20:35.620
You do all your expensive
operations here.

00:20:38.500 --> 00:20:41.260
The unprepared callback,
as you've probably guessed by now,

00:20:41.300 --> 00:20:45.080
is your opportunity to do the converse,
to deallocate these buffers

00:20:45.080 --> 00:20:47.200
and throw away those objects.

00:20:47.970 --> 00:20:52.200
But in the processing callback,
you need to stay completely focused

00:20:52.640 --> 00:20:55.700
on doing your audio processing.

00:20:55.760 --> 00:20:58.520
You can't, or you shouldn't,

00:20:59.300 --> 00:21:04.590
Obtain locks, do allocations,
do deallocations.

00:21:04.590 --> 00:21:10.110
Because if you do,
you risk your user encountering audio

00:21:10.110 --> 00:21:14.060
stutters which are very noticeable.

00:21:14.060 --> 00:21:18.110
They'll hear these audible glitches.

00:21:22.730 --> 00:21:26.430
process callback provides
another structure,

00:21:26.430 --> 00:21:28.380
the audio buffer list.

00:21:28.380 --> 00:21:30.620
And this structure,
at the time of you getting

00:21:30.760 --> 00:21:33.950
the process callback,
is only partially populated.

00:21:33.950 --> 00:21:38.510
At this point in time,
it only describes the number of buffers

00:21:38.510 --> 00:21:43.710
and the number of samples in each
buffer that have been made available

00:21:43.730 --> 00:21:46.250
to you for processing by the tap.

00:21:46.250 --> 00:21:51.300
To actually get those samples,
you need to pull them out of the tap.

00:21:51.300 --> 00:21:54.420
And for that,
you call MT Audio Processing

00:21:54.420 --> 00:21:57.080
Tap Get Source Audio.

00:21:57.080 --> 00:21:59.960
This is your opportunity to pull
those samples that have been made

00:21:59.960 --> 00:22:01.860
available to you out of the tap.

00:22:04.900 --> 00:22:07.360
It's important to note that
the buffers that have been made

00:22:07.360 --> 00:22:10.950
available to you are only good
for the lifetime of this callback.

00:22:11.050 --> 00:22:14.910
If you need them for longer,
you should copy them.

00:22:15.520 --> 00:22:20.120
Now, if you want all the samples that
have been made available to you,

00:22:20.120 --> 00:22:23.960
you can pass in the audio buffer
list structure in its form

00:22:23.960 --> 00:22:28.190
that it was given to you at the
time of the process callback.

00:22:29.050 --> 00:22:32.500
And this will get all the audio samples,
all the audio buffers that

00:22:32.500 --> 00:22:34.690
have been made available.

00:22:36.750 --> 00:22:42.200
So during the process callback,
remain focused,

00:22:42.340 --> 00:22:46.140
keep your code honed in on just
doing the audio processing.

00:22:46.240 --> 00:22:48.920
You're executing on a
very high priority thread,

00:22:49.110 --> 00:22:56.820
and those of you who have worked with
Core Audio before are well aware of

00:22:56.820 --> 00:22:56.820
the constraints of doing this sort
of audio processing in real time.

00:22:58.260 --> 00:23:02.150
If you do need to do audio
processing on some ancillary

00:23:02.150 --> 00:23:07.180
thread or ancillary work queue,
you might want to consider

00:23:07.180 --> 00:23:09.360
the use of a CM simple queue.

00:23:09.420 --> 00:23:14.070
This data structure is both
lockless and thread safe.

00:23:19.500 --> 00:23:24.930
Let's take a look now at a coding
example where we use the process

00:23:24.940 --> 00:23:31.040
callback to just iterate over the
buffers and the samples in each buffer

00:23:31.120 --> 00:23:34.470
and do some trivial manipulation.

00:23:35.150 --> 00:23:38.210
The first thing we're going to do
is call the empty audio processing

00:23:38.210 --> 00:23:42.680
tab get source audio routine to
pull out of the tab all the samples

00:23:42.780 --> 00:23:45.330
that have been made available.

00:23:48.880 --> 00:23:52.420
Now that we've learned how many
buffers and how many samples

00:23:52.420 --> 00:23:56.720
in each buffer are available,
and we've even pulled

00:23:56.720 --> 00:24:01.530
out the data itself,
we can now iterate over them.

00:24:09.740 --> 00:24:12.340
We can do something
interesting with each to,

00:24:12.710 --> 00:24:18.210
say, ramp up the volume a little bit.

00:24:18.210 --> 00:24:18.210
We can multiply each sample by a scalar.

00:24:19.540 --> 00:24:24.380
And if we need to discover if
our samples are interleaved,

00:24:24.440 --> 00:24:26.590
you can do that in the init callback.

00:24:26.880 --> 00:24:29.090
Sorry, the prepare callback.

00:24:29.400 --> 00:24:33.900
In this particular example,
I've discovered this already

00:24:33.900 --> 00:24:36.400
and I've sorted in Naiva.

00:24:39.340 --> 00:24:45.400
Now I'd like to give you a demo of how
you can easily combine the empty audio

00:24:45.400 --> 00:24:51.260
processing tap with a core audio unit.

00:24:53.070 --> 00:25:00.420
The audio unit that I'm going to use
in this demo is a bandpass filter.

00:25:00.480 --> 00:25:04.790
And what a bandpass filter does is
it allows us to define a subrange of

00:25:05.140 --> 00:25:10.170
audio frequencies that are allowed
to play out through the speaker.

00:25:10.330 --> 00:25:15.140
High frequencies and low
frequencies are cut out.

00:25:15.610 --> 00:25:19.740
There are two properties we can
affect on the bandpass filter.

00:25:19.780 --> 00:25:25.300
We can affect the center of that filter,
whether it's centered around

00:25:25.300 --> 00:25:29.740
the high frequencies or down
in the mellow low frequencies.

00:25:29.920 --> 00:25:34.160
And we can also affect the
range or the bandwidth of that

00:25:34.160 --> 00:25:37.000
subrange of audio frequencies.

00:25:37.070 --> 00:25:39.690
Let me show this to you right now.

00:25:42.990 --> 00:25:47.200
I first want to play the item
for you at its full fidelity so

00:25:47.380 --> 00:25:52.490
that you can appreciate the audio
track before I apply the effect.

00:25:55.190 --> 00:26:01.400
As I play, you can watch the video meters
bounce in sync with the audio.

00:26:01.500 --> 00:26:05.790
I also want to highlight the audio
synchronization with the visuals.

00:26:05.910 --> 00:26:10.580
You can see the artist here
strumming along on the strings.

00:26:34.600 --> 00:26:36.900
Now I'm going to play
you the same item again,

00:26:36.900 --> 00:26:40.120
but I'm going to apply
the bandpass filter.

00:26:40.120 --> 00:26:44.190
I'm going to leave the
bandwidth roughly as is.

00:26:44.330 --> 00:26:49.020
And then as it's playing,
I'm going to move the center of the range

00:26:49.470 --> 00:26:55.110
up and down the full frequency range
so that you can hear how the audio unit

00:26:55.240 --> 00:26:58.050
affects the audio as it's playing out.

00:26:59.150 --> 00:27:02.000
As I move the center of the
frequency up to the highs,

00:27:02.110 --> 00:27:05.170
you'll notice that the audio
becomes nice and bright.

00:27:05.310 --> 00:27:09.010
As I move it down into the lows,
it will become more mellow.

00:27:42.310 --> 00:27:49.690
So how easy is it for you to
use a Core Audio audio unit with

00:27:49.690 --> 00:27:52.430
the empty audio processing tab?

00:27:52.940 --> 00:27:53.900
It's really quite simple.

00:27:53.900 --> 00:27:57.390
The two technologies were
made to work together.

00:27:57.420 --> 00:28:00.130
To create an audio unit,
you fill out an audio

00:28:00.130 --> 00:28:02.240
component description.

00:28:02.810 --> 00:28:08.370
Once you've discovered
and created this unit,

00:28:08.370 --> 00:28:09.480
you configure the render callback.

00:28:10.640 --> 00:28:14.340
In the render callback,
it's really quite simple.

00:28:14.380 --> 00:28:17.200
Every time the audio unit
calls you back and says,

00:28:17.240 --> 00:28:21.420
I need more audio to
render the effect onto,

00:28:21.470 --> 00:28:26.890
all you need to do is call empty audio
processing tab get source audio to pull

00:28:26.890 --> 00:28:31.780
out those samples and then give them
back to the audio unit for processing.

00:28:39.310 --> 00:28:43.130
Now that we've discovered
how to do real-time audio

00:28:43.130 --> 00:28:48.170
effects in your application,
either by manipulating the samples

00:28:48.260 --> 00:28:55.320
directly or by applying an audio unit
developed by Apple in your application,

00:28:55.690 --> 00:28:57.600
Let's do the same for video.

00:28:57.690 --> 00:29:05.950
Let's discover now two ways to achieve
realtime video effects to either do

00:29:05.950 --> 00:29:12.080
image analysis or to use the video
images that have been decoded in realtime

00:29:12.080 --> 00:29:15.430
with some other imaging technology.

00:29:16.280 --> 00:29:21.630
But before we dive in,
let's discuss what the challenges

00:29:21.630 --> 00:29:25.850
are of all types of video processing,
irrespective of which

00:29:25.850 --> 00:29:28.230
platform we're doing this on.

00:29:30.820 --> 00:29:38.050
The next goal of this is to
obtain images during playback.

00:29:39.780 --> 00:29:43.320
The second goal,
if we're trying to do some non-trivial

00:29:43.320 --> 00:29:48.150
type of processing or image analysis,
is that we'd want those images a little

00:29:48.150 --> 00:29:53.330
bit ahead of the playhead so that we
have some time to manipulate them.

00:29:55.660 --> 00:29:59.810
If we're doing this,
we'd like to still maintain AV sync

00:30:00.040 --> 00:30:02.700
because that's our number one goal.

00:30:03.150 --> 00:30:07.310
and we'd like to have each
of those images time stamped

00:30:07.500 --> 00:30:09.590
with a display deadline.

00:30:13.950 --> 00:30:19.630
Returning to our diagram,
you've probably already used the

00:30:19.630 --> 00:30:22.900
AV player layer in your application.

00:30:22.960 --> 00:30:26.760
You've used it with CoreAnimation
and you've produced some

00:30:26.760 --> 00:30:32.650
very creative forms of video
presentation in your application.

00:30:36.110 --> 00:30:38.870
For those of you who are familiar
with the AV player layer,

00:30:38.890 --> 00:30:43.300
you've never been able to
associate multiple AV player

00:30:43.300 --> 00:30:46.080
layers with an AV player.

00:30:46.550 --> 00:30:52.890
Well, new in iOS 6 and Mountain Lion,
we now are giving you the support

00:30:53.470 --> 00:30:59.100
to associate multiple AV player
layers with a single AV player.

00:31:01.920 --> 00:31:07.140
We haven't even gotten
to the cool part yet.

00:31:07.150 --> 00:31:12.450
By doing this, you can achieve some
really creative effects.

00:31:12.670 --> 00:31:17.840
It's very simple and you don't even have
to worry about the audiovisual sync.

00:31:17.960 --> 00:31:20.670
AV Foundation will still do this for you.

00:31:23.190 --> 00:31:25.220
It's extremely easy to use.

00:31:25.280 --> 00:31:31.420
And I would go so far as to say that
if you're considering doing real-time

00:31:31.420 --> 00:31:35.060
video effects in your application,
you should first consider

00:31:35.150 --> 00:31:37.040
the use of an AVPlayer layer.

00:31:37.180 --> 00:31:42.440
What this means for you is that the
code that you need to maintain is less,

00:31:42.440 --> 00:31:49.520
increasing your maintainability,
and freeing you up to do other things.

00:31:49.590 --> 00:31:54.960
You can do some really compelling, easy,
functional things with just multiple

00:31:54.960 --> 00:31:57.970
player layers associated with the player.

00:32:00.190 --> 00:32:03.660
In fact,
I want to show this to you right now.

00:32:03.770 --> 00:32:07.580
In this demo,
I'm going to show you two things.

00:32:08.220 --> 00:32:12.910
The first thing I want to show you
is how very simple it is to create a

00:32:13.110 --> 00:32:20.100
functional feature in your application
with just a screen full of code.

00:32:20.960 --> 00:32:26.390
The second thing I want to highlight
in this demo is how multiple player

00:32:26.390 --> 00:32:29.100
layers are perfectly in sync.

00:32:29.210 --> 00:32:34.590
What I'm going to do in this demo
is I'm going to show you an AV loop.

00:32:34.760 --> 00:32:38.240
A magnifying glass is a video loop
that you might be familiar with from

00:32:38.390 --> 00:32:40.600
applications such as iPhoto and Aperture.

00:32:40.920 --> 00:32:44.700
It's a magnifying glass,
so now we can appreciate

00:32:44.880 --> 00:32:48.100
the detail in a video image.

00:32:48.100 --> 00:32:51.140
Let me switch to the iPad and
show this to you right now.

00:32:54.570 --> 00:32:56.400
So here's our loop.

00:32:56.490 --> 00:32:57.660
It's multiple player layers.

00:32:57.860 --> 00:33:00.360
In this case, it's two.

00:33:00.500 --> 00:33:06.750
And we can see this chap out
on the San Francisco Bay.

00:33:07.200 --> 00:33:10.000
He has a lovely yacht,
which you too can own if

00:33:10.000 --> 00:33:15.590
you are successful in using
AV Foundation in your application.

00:33:15.710 --> 00:33:21.560
And even though it's full
HD at a very high bit rate,

00:33:21.830 --> 00:33:25.000
Because we're not reading back
from the GPU or any of that stuff,

00:33:25.110 --> 00:33:26.340
it's super fluid.

00:33:26.500 --> 00:33:30.130
It's like butter, or at least butter that
you've left out overnight.

00:33:30.130 --> 00:33:35.860
And I can zoom in on the
skipper here and magnify him.

00:33:35.860 --> 00:33:39.340
I can also zoom in on his vessel ID.

00:33:39.340 --> 00:33:42.620
And now,
because we have this sort of a feature,

00:33:42.680 --> 00:33:46.280
we can appreciate the depth
of field in this image.

00:33:46.280 --> 00:33:49.370
If you're an avid
videographer like myself,

00:33:49.440 --> 00:33:51.460
you can appreciate the depth
of field in this image.

00:33:51.850 --> 00:33:56.060
Shallow depth of field,
how sharp the foreground is,

00:33:56.260 --> 00:33:57.600
and how beautifully

00:33:59.020 --> 00:34:02.160
Bookade out the background is,
we've got Fort Baker in the

00:34:02.160 --> 00:34:05.230
background and Tiburon over here.

00:34:06.300 --> 00:34:11.440
But all of this is just
one screen full of code.

00:34:11.630 --> 00:34:16.460
And the synchronization between the
multiple player layers is automatic.

00:34:16.570 --> 00:34:20.700
There's nothing that we needed to do in
this application to achieve this effect.

00:34:20.830 --> 00:34:27.690
Let me switch back to the slides and
describe to you how you can do this.

00:34:33.590 --> 00:34:35.440
It's really quite simple.

00:34:35.530 --> 00:34:38.610
Like I said, there are two player layers.

00:34:39.070 --> 00:34:42.690
The first player layer is sized
to fit the bounds of the display.

00:34:42.710 --> 00:34:46.160
It's our main view into the video.

00:34:47.520 --> 00:34:53.050
The second player layer is sized to
fit four times the bounds of the first,

00:34:53.120 --> 00:34:56.100
so that we get that
magnified zoom effect.

00:34:57.230 --> 00:35:05.160
To make it look like a magnifying glass,
we're going to create a CA shape layer.

00:35:05.410 --> 00:35:12.590
and associate with that a core
graphics path for creating the circle.

00:35:13.800 --> 00:35:17.670
Applying that mask to
the magnified layer,

00:35:17.860 --> 00:35:20.090
we get a visible region.

00:35:20.370 --> 00:35:25.070
And this visible region obscures
a smaller magnified region

00:35:25.600 --> 00:35:27.970
on the main player layer.

00:35:30.530 --> 00:35:37.100
To make this interactive,
we're going to have a UI image view

00:35:37.240 --> 00:35:41.900
and a UI pan gesture recognizer
that's associated only to that

00:35:42.300 --> 00:35:47.670
view so that touches and other user
interface events only go to that view.

00:35:48.760 --> 00:35:53.440
Then we apply some fancy artwork,
composite the two,

00:35:53.670 --> 00:35:58.320
and now we have an interactive
application that's producing the loop.

00:35:58.350 --> 00:36:00.920
It's that easy.

00:36:02.440 --> 00:36:07.360
Our user can move around and it's
very fluid and it looks as if they're

00:36:07.360 --> 00:36:11.130
magnifying the main player layer.

00:36:14.510 --> 00:36:20.000
But as powerful as associating multiple
player layers to a single player are,

00:36:20.140 --> 00:36:24.180
and as efficient it is because
there's only one media decode

00:36:24.320 --> 00:36:29.080
to feed all these player layers,
you might need something more powerful.

00:36:29.180 --> 00:36:35.950
You might need video images
themselves so that you can do

00:36:35.950 --> 00:36:38.330
your own custom manipulation and
your own custom image analysis.

00:36:40.150 --> 00:36:44.520
And for that,
we're introducing a whole new object.

00:36:44.610 --> 00:36:47.800
It's the AV Player Item Video Output.

00:36:48.620 --> 00:36:54.590
You can associate multiple
of these to a single player,

00:36:54.810 --> 00:37:04.900
and it's your key for achieving
custom video processing.

00:37:04.900 --> 00:37:07.670
So how do you use one of these things?

00:37:10.790 --> 00:37:15.170
To use an AV Player Item video output,
you can use it with other

00:37:15.980 --> 00:37:20.040
imaging technologies such
as OpenGL or Core Graphics.

00:37:20.100 --> 00:37:26.100
And you can use it to do your own video
effects and your own video analysis.

00:37:26.200 --> 00:37:30.730
It's available both on
Mountain Lion and iOS 6.

00:37:33.710 --> 00:37:37.940
To use it, there are two questions
that you can ask of it.

00:37:37.970 --> 00:37:45.110
Do you have a new pixel buffer
that I haven't seen before,

00:37:45.110 --> 00:37:45.110
a new image?

00:37:45.110 --> 00:37:45.110
And if you do,

00:37:45.470 --> 00:37:51.950
Can I copy that pixel buffer for a
particular time in the item's timeline

00:37:52.890 --> 00:37:58.390
and then optionally receive a CM time
describing its exact display deadline?

00:38:00.020 --> 00:38:04.400
To drive your use of the
AVPlayer Item Video Output,

00:38:04.770 --> 00:38:11.800
you need to use a fixed rate
hardware synchronized service.

00:38:12.780 --> 00:38:15.760
And on OS X,
the candidates are a CV display

00:38:15.900 --> 00:38:17.900
link and a CA OpenGL layer.

00:38:17.950 --> 00:38:24.130
On iOS, you've got the CA display link or
the GL kit view or view controller.

00:38:24.220 --> 00:38:32.600
And each of these services guarantees
to you a callback to your application

00:38:33.150 --> 00:38:37.590
that's in at the same lockstep frequency
as the vertical sync on your hardware.

00:38:39.950 --> 00:38:42.890
Here's a picture that describes this.

00:38:42.990 --> 00:38:47.700
As a device is displaying
the most appropriate image,

00:38:47.830 --> 00:38:56.590
for HostTime T0, you get a callback that
prepares your application,

00:38:56.590 --> 00:38:56.590
prepares your object for the
next vertical sync at T1.

00:39:02.290 --> 00:39:07.300
The next thing you're going to
do is try and convert that host

00:39:07.320 --> 00:39:10.710
time into the item timeline.

00:39:11.050 --> 00:39:14.100
You can call item time for host time.

00:39:14.170 --> 00:39:16.560
And what you're doing here

00:39:17.070 --> 00:39:22.750
is you're asking AV Foundation to do that
time calculation that we saw earlier.

00:39:22.890 --> 00:39:25.190
For T1,

00:39:25.650 --> 00:39:28.350
What's the appropriate image time?

00:39:28.700 --> 00:39:32.120
What's the value in the green circle?

00:39:34.860 --> 00:39:38.570
Once AV Foundation has done the
arithmetic to do that for you,

00:39:38.570 --> 00:39:41.350
you now know what the
image time is -- sorry,

00:39:41.350 --> 00:39:44.020
the item time is that
you need to ask for.

00:39:44.020 --> 00:39:47.010
And you can ask the
AV player item video output,

00:39:47.010 --> 00:39:50.560
do you have a new pixel buffer
that I haven't seen yet?

00:39:50.620 --> 00:39:55.330
Each time you copy an image from
your AV player item video output,

00:39:55.470 --> 00:39:58.630
it will maintain a record of consumption.

00:39:58.630 --> 00:40:01.430
And that's true for every instance.

00:40:01.430 --> 00:40:03.080
It's a separate record of consumption.

00:40:04.050 --> 00:40:13.900
If there is a new pixel buffer
available for that item time,

00:40:14.250 --> 00:40:17.720
you now have the opportunity to copy
that pixel buffer and receive a CM time

00:40:17.720 --> 00:40:17.720
describing its exact display deadline.

00:40:18.040 --> 00:40:23.270
You render that image out using the
imaging technology of your choice.

00:40:23.530 --> 00:40:27.870
It gets split to the screen
at the next vertical sync,

00:40:27.870 --> 00:40:33.990
and then the service prepares
you for the next iteration at D2.

00:40:38.600 --> 00:40:44.160
On iOS, you might use a CA display link.

00:40:44.300 --> 00:40:52.680
Let's take a look now at a coding
example on how you set up the

00:40:52.680 --> 00:40:52.680
CA display link and use it with
an AVPlayer item video output.

00:40:55.190 --> 00:41:03.900
The first thing you do is you set
up the display link with a target

00:41:03.900 --> 00:41:08.210
of yourself and a selector where
you'd like the callback to occur.

00:41:08.400 --> 00:41:24.200
[Transcript missing]

00:41:24.940 --> 00:41:30.120
And if we want to be efficient,
we can set the initial state to pause

00:41:30.220 --> 00:41:32.890
so it's not calling us back immediately.

00:41:32.970 --> 00:41:35.820
And this will save power.

00:41:35.950 --> 00:41:39.400
And we'll unpause it when
our user commences playback.

00:41:39.490 --> 00:41:41.600
And then we add it to a run loop.

00:41:42.700 --> 00:41:45.250
In the selector,
the first thing that we need to

00:41:45.250 --> 00:41:50.360
do is calculate the time value
for the next vertical sync.

00:41:50.480 --> 00:41:54.060
Using a CA display link,
you ask it for its time stamp

00:41:54.060 --> 00:41:56.460
and you ask it for its duration.

00:41:56.630 --> 00:42:01.860
How often, what time value is it
between each callback?

00:42:02.600 --> 00:42:09.150
Then we ask our AV player item
output the standard questions.

00:42:09.740 --> 00:42:13.610
Do the calculation from
item time into--sorry,

00:42:13.800 --> 00:42:16.440
from host time into item time.

00:42:17.750 --> 00:42:20.850
Once we've done the calculation,
is there a new pixel buffer that

00:42:20.850 --> 00:42:24.080
I haven't seen yet for that item time?

00:42:24.210 --> 00:42:27.220
If there is, well, let me have it.

00:42:27.320 --> 00:42:33.730
I want to pull the next pixel buffer
by calling copy pixel buffer and

00:42:34.140 --> 00:42:38.370
optionally receive the display deadline.

00:42:39.270 --> 00:42:42.280
If you're on iOS and
you want to use OpenGL,

00:42:42.290 --> 00:42:46.150
now is your opportunity to
create separate textures,

00:42:46.380 --> 00:42:51.430
one for Luma, one for Chroma,
because that's the most efficient path,

00:42:51.480 --> 00:42:54.360
using a CVE OpenGL ES texture cache.

00:42:57.280 --> 00:43:00.260
And what's also important to
note here is CV pixel buffers,

00:43:00.360 --> 00:43:05.660
CF types, that need to be maintained
with CF retains and releases,

00:43:05.660 --> 00:43:07.110
even under ARC.

00:43:11.760 --> 00:43:16.540
Of course, like I promised,
you can pull ahead of the playhead.

00:43:16.540 --> 00:43:22.700
You can ask the AVPlayer
item video output for images

00:43:22.700 --> 00:43:26.060
ahead of the current time.

00:43:26.170 --> 00:43:32.370
You can add an interval that's
appropriate for your custom processing.

00:43:32.670 --> 00:43:37.270
and have the AV Player Item Video Output
calculate the item time

00:43:37.270 --> 00:43:38.930
that's appropriate.

00:43:39.110 --> 00:43:43.490
If you fall behind in your processing,
you don't need to worry.

00:43:43.760 --> 00:43:47.670
The AV Player item video output
will automatically discard all

00:43:47.750 --> 00:43:49.750
images behind the playhead.

00:43:53.700 --> 00:43:57.560
And as I alluded to earlier,
you can pause many of these

00:43:57.560 --> 00:44:00.150
hardware synchronized services.

00:44:00.150 --> 00:44:05.520
And you want to do this whenever your
application isn't doing any playback,

00:44:05.520 --> 00:44:09.920
it's paused,
or maybe that view has gone away,

00:44:09.920 --> 00:44:14.380
your user has closed
down the playback view.

00:44:14.490 --> 00:44:16.990
You want to try and shut down
those services so that they're

00:44:16.990 --> 00:44:16.990
not constantly calling you back.

00:44:18.180 --> 00:44:22.690
And to help you start up again,
you can ask the AVPlayer item

00:44:22.690 --> 00:44:28.590
video output to give you a callback
when it anticipates more images

00:44:28.590 --> 00:44:31.690
will come down from the decode.

00:44:31.790 --> 00:44:34.440
You can ask it to request
notification of media data

00:44:34.460 --> 00:44:36.540
change with an advanced interval.

00:44:36.650 --> 00:44:39.930
And the advanced interval that
you provide here is the amount of

00:44:39.930 --> 00:44:45.380
time that you require to restart
your processing operations,

00:44:45.500 --> 00:44:49.520
to recreate objects, to recreate buffers.

00:44:49.750 --> 00:44:53.360
If you make this request of
an AVPlayer item video output,

00:44:53.590 --> 00:44:57.720
your delegate will be messaged,
output media data will change.

00:45:02.910 --> 00:45:06.530
So on this slide,
we have ten of our most talented

00:45:07.060 --> 00:45:09.200
Media Systems engineers.

00:45:09.200 --> 00:45:13.380
Not only can these guys code
but they can also dance.

00:45:13.380 --> 00:45:16.880
And as funny as they are,
I want to show you a demo of how to

00:45:16.880 --> 00:45:20.420
use an AVPlayer item video output.

00:45:20.420 --> 00:45:23.420
And this demo seeks to
highlight four things.

00:45:23.420 --> 00:45:27.760
First, I want to show you how to use
the AVPlayer item video output.

00:45:27.780 --> 00:45:35.000
And I want to show it to you within the
context of an interactive OpenGL ES 2.0

00:45:35.000 --> 00:45:41.230
game that uses GLKit to drive consumption
from the AVPlayer item video output.

00:45:42.170 --> 00:45:49.610
For each image, we're going to apply a
real-time chroma key effect.

00:45:50.150 --> 00:45:54.680
This chroma key effect is
implemented in GLSL in a fragment

00:45:55.440 --> 00:45:58.000
program and executed on the

00:46:00.770 --> 00:46:04.010
And I want to show you a technique
that's particularly useful on

00:46:04.010 --> 00:46:08.740
iOS called Texture Atlas Sync,
where we can push together or offer

00:46:08.880 --> 00:46:14.740
together multiple smaller movies
into a much larger movie so that we

00:46:14.740 --> 00:46:20.800
maximize each texture that has been
decoded and uploaded onto the GPU.

00:46:20.960 --> 00:46:25.330
Let me switch to the demo iPad
and show you this game right now.

00:46:26.960 --> 00:46:29.380
And here we have our game.

00:46:29.480 --> 00:46:32.530
You might begin to recognize it.

00:46:32.620 --> 00:46:36.100
Here's our interactive OpenGL ES game.

00:46:37.420 --> 00:46:40.700
It's interactive because
I can swipe around and reveal

00:46:40.700 --> 00:46:43.550
other members of the team.

00:46:43.560 --> 00:46:48.620
The movie that's playing is the same
movie you saw earlier on the slides.

00:46:49.050 --> 00:46:54.040
On the GPU, we're slicing up each of the
individual movies and pushing them

00:46:54.040 --> 00:46:57.120
out to the pieces on this puzzle.

00:46:58.070 --> 00:47:03.050
I can move it around and manipulate it
just by swiping around on the display.

00:47:05.100 --> 00:47:09.870
And like I promised,
I can apply a real-time video effect at

00:47:09.870 --> 00:47:12.900
any time just by tapping on the display.

00:47:21.070 --> 00:47:24.980
So maybe by now you're recognizing
what kind of a game this is.

00:47:24.980 --> 00:47:28.320
And as much as I'd love to try
and solve this for you right here,

00:47:28.320 --> 00:47:31.310
right now on stage,
I'm going to do what any good

00:47:31.310 --> 00:47:35.120
programmer will do and get
the computer to do it for him.

00:47:35.120 --> 00:47:41.790
I'm going to push and hold and
have the application do it for me.

00:47:45.020 --> 00:47:47.500
Everything is happening in
real time and I can turn on

00:47:47.500 --> 00:47:50.110
and off the effects at will.

00:47:50.380 --> 00:47:53.580
If you want to try texture
atlasing in your application,

00:47:53.590 --> 00:47:56.690
there are three things that
you might want to consider.

00:47:57.100 --> 00:48:01.320
The first is that you can create
the texture atlas very easy with

00:48:01.320 --> 00:48:04.500
an AV mutable video composition.

00:48:04.560 --> 00:48:09.850
The second is, if you're using H.264,
you want to try and line your tiles

00:48:09.850 --> 00:48:13.200
of video along 16 pixel boundaries.

00:48:13.300 --> 00:48:18.070
And then for the maximum effect,
you want to try and have your videos

00:48:18.330 --> 00:48:21.700
loop with common start and end frames.

00:48:22.600 --> 00:48:25.320
I see it's solved,
but I need to switch back to

00:48:25.370 --> 00:48:29.190
slides because I have a little bit
more I want to summarize with you.

00:48:38.120 --> 00:48:39.400
That's pretty neat, isn't it?

00:48:39.440 --> 00:48:43.210
There's so many very creative
things that you can do with this.

00:48:43.420 --> 00:48:51.630
I think it's quite powerful,
and I think the applications

00:48:52.670 --> 00:48:53.010
that you can create with this,
with an AVPlayer item video output,

00:48:53.010 --> 00:48:53.010
are almost limitless.

00:48:55.020 --> 00:48:59.740
Let's summarize the key points of today.

00:48:59.890 --> 00:49:04.300
The first thing that we saw was
that AV Sync is the key goal.

00:49:04.420 --> 00:49:09.310
It's what you need to maintain in your
applications so that your users continue

00:49:09.380 --> 00:49:12.400
to experience the magic of a movie.

00:49:12.520 --> 00:49:16.530
If you lose it, the effect is lost.

00:49:18.510 --> 00:49:24.810
To ensure tight synchronization with
playback within your application,

00:49:24.810 --> 00:49:31.430
you tell AV Foundation specifically
which clock you want to synchronize to,

00:49:31.430 --> 00:49:38.200
which clock AV Foundation should do
its time calculations with respect to.

00:49:38.200 --> 00:49:40.920
With the empty audio processing tap,

00:49:41.990 --> 00:49:46.600
We stressed how important it is to
remain efficient during processing,

00:49:46.780 --> 00:49:52.500
to completely focus that one callback
on just doing audio processing

00:49:52.500 --> 00:49:55.650
and not any ancillary operations.

00:49:56.930 --> 00:50:00.040
And then using the
AV Player Item Video output,

00:50:00.110 --> 00:50:05.920
you drive consumption by using
one of a collection of hardware

00:50:06.780 --> 00:50:10.230
synchronized fixed rate services.

00:50:13.690 --> 00:50:18.750
For more information about AV Foundation,
please contact our media

00:50:18.750 --> 00:50:22.360
technology evangelist,
Eric Verschen.

00:50:22.560 --> 00:50:27.500
I hope you are having and will
continue to have a wonderful dub dub.

00:50:27.560 --> 00:50:30.760
And I'm really looking forward
to the applications that you guys

00:50:30.760 --> 00:50:32.500
can develop with these new APIs.

00:50:32.500 --> 00:50:35.100
Thank you very much.