WEBVTT

00:00:07.000 --> 00:00:07.500
Hello.

00:00:07.500 --> 00:00:15.520
I'd like to thank you all
for coming this afternoon.

00:00:15.540 --> 00:00:21.220
This is a very exciting
session to be involved in.

00:00:21.220 --> 00:00:24.400
And it's actually one of
three sessions to do with

00:00:24.470 --> 00:00:28.810
Mac OS X support for audio and MIDI.

00:00:29.250 --> 00:00:34.210
This session will cover both the
MIDI support the we're providing in

00:00:34.210 --> 00:00:37.420
OS X-- and it's the first time in a
number of years that Apple has made

00:00:37.420 --> 00:00:39.880
a serious commitment in this area.

00:00:39.880 --> 00:00:43.620
And as I said, it's very exciting to be
involved in this project.

00:00:43.620 --> 00:00:45.700
There's two more sessions on Friday.

00:00:45.700 --> 00:00:50.670
They're late in the afternoon,
starting at 3:30, covering both the

00:00:50.820 --> 00:00:55.420
I/O Kit area of audio support,
audio device support,

00:00:55.420 --> 00:01:00.280
and the user application
interface to audio devices.

00:01:00.280 --> 00:01:02.020
I'd encourage you to go to that.

00:01:02.020 --> 00:01:06.460
And then after those sessions on Friday,
there'll actually be a party that we'll

00:01:06.460 --> 00:01:09.500
give you some more details on later.

00:01:09.520 --> 00:01:11.980
So without any further ado,
I'd like to introduce Doug White,

00:01:11.980 --> 00:01:14.470
who will be talking about
the MIDI services on OS X.

00:01:14.490 --> 00:01:15.860
Thank you.

00:01:23.860 --> 00:01:25.540
Good afternoon.

00:01:25.540 --> 00:01:26.100
I'm Doug Wyatt.

00:01:26.100 --> 00:01:29.200
I'm a software engineer
in the Core Audio Group,

00:01:29.230 --> 00:01:34.160
and I'm here to tell you about the
new MIDI system services on Mac OS X.

00:01:34.810 --> 00:01:36.750
Before coming to Apple,
I worked at AppCode

00:01:36.750 --> 00:01:39.590
Systems for about 12 years,
working on MIDI applications

00:01:39.590 --> 00:01:41.750
like many of you.

00:01:41.750 --> 00:01:44.270
And I was the author of OMS there.

00:01:45.390 --> 00:01:47.050
So here's what I'm going
to talk about today.

00:01:47.100 --> 00:01:49.890
I'm going to talk a little
about the history and the goals

00:01:50.120 --> 00:01:54.250
of our new system services,
the history of MIDI on the Macintosh,

00:01:54.290 --> 00:01:57.260
because that will help
you understand our goals.

00:01:57.450 --> 00:02:00.590
I'll go over some of the key
concepts in the MIDI API.

00:02:00.660 --> 00:02:03.100
We'll look at some of
the functions in it.

00:02:03.160 --> 00:02:06.480
I'll talk a little about the
performance challenges of getting

00:02:06.480 --> 00:02:11.390
MIDI working on a multitasking,
modern operating system like OS X.

00:02:11.490 --> 00:02:15.890
And I'll tell you about the availability
of the new MIDI system services.

00:02:17.910 --> 00:02:24.050
So going back to the mid-1980s,
the first Macintosh MIDI interfaces

00:02:24.200 --> 00:02:27.070
connected directly to the serial hardware

00:02:27.280 --> 00:02:30.000
and developers would write
to the serial ports directly

00:02:30.000 --> 00:02:33.260
because it was most efficient,
and I don't think the serial drivers

00:02:33.260 --> 00:02:36.240
supported external clock in those days.

00:02:36.390 --> 00:02:40.180
And that was fine and good for a while,
until the late '80s we started

00:02:40.180 --> 00:02:43.080
to see multiport MIDI interfaces,
like Mark of the

00:02:43.080 --> 00:02:44.960
Unicorn's MIDI Timepiece.

00:02:45.080 --> 00:02:48.180
We started to see sound cards,
like Digidesign's

00:02:48.180 --> 00:02:50.230
Mac Proteus and SampleCell.

00:02:50.270 --> 00:02:55.540
And there started--there became a need
for system software for applications

00:02:55.640 --> 00:03:00.560
to deal with these different kinds of
hardware in a hardware-independent way.

00:03:01.260 --> 00:03:05.890
So in 1989, we saw Apple's MIDI Manager,
which was the first attempt

00:03:05.890 --> 00:03:07.480
to solve this problem.

00:03:07.560 --> 00:03:11.150
Unfortunately, MIDI Manager had some
performance problems,

00:03:11.150 --> 00:03:14.860
and it got kind of unwieldy
when you tried to use it in

00:03:14.860 --> 00:03:17.220
large studio environments.

00:03:17.490 --> 00:03:21.740
Opcode's OMS, which came out in 1990,
addressed some of these

00:03:21.870 --> 00:03:23.620
limitations of MIDI Manager.

00:03:23.730 --> 00:03:27.310
And in the course of a few years,
it became a de facto standard.

00:03:27.360 --> 00:03:33.300
Most programs on the Macintosh
are now supported on OS 9.

00:03:33.460 --> 00:03:37.390
People making MIDI hardware tend to
write OMS drivers for their hardware,

00:03:37.480 --> 00:03:42.850
and it's the way -- it's the only way
that a lot of these applications and

00:03:42.960 --> 00:03:46.240
hardware are talking to each other now.

00:03:46.310 --> 00:03:49.790
But because OMS was
controlled by a competitor,

00:03:50.060 --> 00:03:54.440
and now that it isn't
being supported anymore,

00:03:54.440 --> 00:03:59.000
as far as I can tell -- well,
it continues to work on OS 9,

00:03:59.000 --> 00:04:03.700
but it's not being developed any further,
and the prospects for it to

00:04:03.700 --> 00:04:07.190
work well on OS 10 are not good.

00:04:07.660 --> 00:04:11.800
So that combined with the fact
that developers have been sort of

00:04:11.800 --> 00:04:17.240
chipping away at MIDI compatibility
with the problem of SUFs,

00:04:17.240 --> 00:04:21.680
Wiresense, and so on,
we see a need for Apple to

00:04:21.680 --> 00:04:25.860
provide a single set of
MIDI system services for OS X.

00:04:26.920 --> 00:04:31.010
So that's our main goal on OS X,
to provide a single standard so that

00:04:31.010 --> 00:04:34.830
everyone's hardware and software
can play together nicely again.

00:04:35.080 --> 00:04:39.180
So in support of that goal,
we want to focus just on

00:04:39.180 --> 00:04:44.750
those basic MIDI I/O services,
and doing those basic MIDI I/O services

00:04:44.890 --> 00:04:50.240
with highly accurate timing,
meaning low latencies and low jitter.

00:04:50.280 --> 00:04:53.900
Also, we want to make the MIDI services
open source so that you,

00:04:53.900 --> 00:04:57.160
as developers, can see what we're doing,
help us fix things if

00:04:57.160 --> 00:05:01.580
we're not doing them right,
and not have to be afraid of repeats

00:05:01.620 --> 00:05:04.680
of the fates of MIDI Manager and OMS.

00:05:06.210 --> 00:05:11.690
So looking at the MIDI system services
in the big picture of the rest of the OS,

00:05:12.020 --> 00:05:15.490
they're layered above the
I/O Kit in the kernel.

00:05:16.140 --> 00:05:20.390
I/O Kit is where we see drivers
for talking to hardware.

00:05:20.700 --> 00:05:23.920
We have a concept of MIDI drivers.

00:05:24.060 --> 00:05:26.860
Applications can talk
to the MIDI services,

00:05:26.890 --> 00:05:29.420
which in turn talk to MIDI drivers.

00:05:29.420 --> 00:05:34.140
For higher-level applications of MIDI,
like just playing MIDI files,

00:05:34.140 --> 00:05:36.650
you can use the
QuickTime MusicDevice APIs,

00:05:36.650 --> 00:05:42.140
and Chris Rogers will be telling you a
bit about that in the second half of the

00:05:45.220 --> 00:05:48.600
So here are the main pieces
of the new MIDI services.

00:05:48.630 --> 00:05:51.600
We have a driver model for MIDI drivers.

00:05:51.670 --> 00:05:54.440
Applications can share
access to hardware,

00:05:54.620 --> 00:05:57.550
meaning that multiple
MIDI applications can send

00:05:57.550 --> 00:06:01.780
simultaneously to the same device,
and a MIDI device can send

00:06:01.910 --> 00:06:06.850
MIDI into the computer and multiple
applications can all receive it.

00:06:07.380 --> 00:06:09.340
We timestamp all the MIDI input.

00:06:09.350 --> 00:06:14.210
We schedule all MIDI output in advance
for applications that want to do that.

00:06:14.300 --> 00:06:16.750
And all that scheduling is
done using the most accurate

00:06:16.920 --> 00:06:22.330
timing hardware on the computer,
the host clock as returned by uptime.

00:06:22.970 --> 00:06:28.300
The MIDI services provide a small
central repository of information

00:06:28.300 --> 00:06:29.830
about the MIDI hardware that's present.

00:06:29.890 --> 00:06:34.940
They don't try to replicate
the full functionality that

00:06:34.940 --> 00:06:39.120
you see in OMS and FreeMIDI,
where the user with 15 synthesizers

00:06:39.120 --> 00:06:41.670
can enter information about all that.

00:06:41.900 --> 00:06:45.260
When I say there's a repository
of device information,

00:06:45.260 --> 00:06:48.180
that's just about the
actual MIDI interfaces and

00:06:48.280 --> 00:06:49.840
cards that are present.

00:06:49.890 --> 00:06:53.370
We don't, at least for now,
become concerned about the

00:06:53.430 --> 00:06:56.900
devices that are attached
externally to MIDI interfaces.

00:06:56.900 --> 00:07:01.900
And the MIDI services provide some
basic inter-process communication.

00:07:01.900 --> 00:07:05.860
So if you have a small MIDI utility,
maybe it transposes or

00:07:05.860 --> 00:07:10.950
arpeggiates or something,
you can use the MIDI services to

00:07:11.090 --> 00:07:11.900
create an application like this.

00:07:11.900 --> 00:07:13.390
like that.

00:07:14.960 --> 00:07:20.740
So now I'd like to get into some of the
objects and functions in the MIDI API.

00:07:20.880 --> 00:07:23.140
First, there's the MIDI client.

00:07:23.250 --> 00:07:25.880
And for those of you who've
used MIDI Manager in OMS,

00:07:25.880 --> 00:07:28.650
this is a familiar concept.

00:07:28.740 --> 00:07:32.080
The first thing you typically do
is create a MIDI client object.

00:07:32.080 --> 00:07:36.080
And that's done with MIDI client create.

00:07:37.490 --> 00:07:42.300
After creating your client,
then you can create MIDI port objects.

00:07:42.440 --> 00:07:46.500
This is also similar to
MIDI Manager and OMS.

00:07:46.760 --> 00:07:49.440
MIDI ports are objects through
which your program can send

00:07:49.440 --> 00:07:53.800
and receive MIDI messages,
and they're created at startup.

00:07:53.910 --> 00:07:58.700
So here we see MIDI input port
create and MIDI output port create.

00:07:59.060 --> 00:08:03.840
which create input and output ports,
obviously.

00:08:03.910 --> 00:08:08.930
The my_readproc parameter that's
passed to my_midi_input_port_create

00:08:09.050 --> 00:08:16.060
is a procedure through which that port
will -- let me rephrase that -- that

00:08:16.060 --> 00:08:22.100
procedure will get called when MIDI comes
into your program through that port.

00:08:23.260 --> 00:08:27.610
All of the MIDI I/O functions
use the structure MIDI Packet

00:08:27.610 --> 00:08:30.700
List when sending and receiving MIDI.

00:08:30.700 --> 00:08:32.860
It's simply a list of
MIDI packet structures.

00:08:33.060 --> 00:08:35.960
It can be any length for now.

00:08:35.960 --> 00:08:40.200
And those MIDI packet structures are
themselves variable length structures.

00:08:40.200 --> 00:08:45.400
A MIDI packet contains one or
more simultaneous MIDI events.

00:08:45.930 --> 00:08:49.610
With one exception,
if you have a system-exclusive message,

00:08:49.700 --> 00:08:53.640
it has to be in its own MIDI packet,
and the reason for that is just to

00:08:53.640 --> 00:08:55.830
make our own internal parsing simpler.

00:08:55.900 --> 00:08:59.850
And for similar reasons,
running status is not

00:08:59.850 --> 00:09:02.190
allowed inside a MIDI packet.

00:09:02.290 --> 00:09:10.500
But otherwise, the data array portion of
the MIDI packet is a little

00:09:10.510 --> 00:09:10.790
MIDI stream directed at one device.

00:09:11.170 --> 00:09:16.920
and MIDI packets are time-stamped,
and I'll talk a bit about that later.

00:09:17.890 --> 00:09:20.510
So dealing with variable-length
structures like MIDI packet

00:09:20.560 --> 00:09:23.590
and MIDI packet list can
be a little annoying,

00:09:23.650 --> 00:09:26.650
so we've provided a few
simple helper functions.

00:09:26.920 --> 00:09:30.240
There's nextMIDIPacket,
which you can use when dealing

00:09:30.240 --> 00:09:32.080
with the MIDI packet list.

00:09:32.160 --> 00:09:35.320
You can get a pointer to the
first packet in the packet list,

00:09:35.360 --> 00:09:38.890
then use nextMIDIPacket to
advance to the next one,

00:09:38.890 --> 00:09:39.800
and so on.

00:09:39.830 --> 00:09:44.070
When sending MIDI,
you can use MIDI packet list init and

00:09:44.070 --> 00:09:48.800
MIDI packet list add to dynamically
build up a MIDI packet list.

00:09:48.830 --> 00:09:51.970
And here's an example of using them.

00:09:53.320 --> 00:09:57.800
Here, I'm creating a buffer of
1k bytes on the stack.

00:09:57.800 --> 00:10:00.690
I'm casting it to a MIDI packet list.

00:10:00.750 --> 00:10:04.130
I call MIDI packet list
init on it to initialize it.

00:10:04.200 --> 00:10:09.400
And then I'm adding a simple MIDI note
on event to it with MIDI packet list add.

00:10:09.520 --> 00:10:13.020
And if this were a more complex example,
then I could go adding more events

00:10:13.020 --> 00:10:18.040
to be played at different times
and build up a MIDI packet list.

00:10:18.120 --> 00:10:22.400
When MIDI packet list add returns null,
then I know that it's become

00:10:22.470 --> 00:10:24.630
full and it's time to send it.

00:10:24.700 --> 00:10:27.940
And I can call MIDI packet
list init and start calling

00:10:27.940 --> 00:10:30.190
MIDI packet list add on it again.

00:10:32.960 --> 00:10:35.230
Okay,
now that we've looked at the structures

00:10:35.390 --> 00:10:39.440
representing MIDI data itself,
here are the objects that

00:10:39.440 --> 00:10:43.770
represent MIDI sources and
destinations in the system.

00:10:49.940 --> 00:10:52.880
The lowest-level object
is a MIDI endpoint,

00:10:52.990 --> 00:10:56.090
which is a simple
MIDI source or destination,

00:10:56.280 --> 00:10:59.790
and a single 16-channel MIDI stream.

00:10:59.870 --> 00:11:03.880
So your program can have one
simple view of the system as an

00:11:03.880 --> 00:11:06.920
array of sources and destinations.

00:11:08.880 --> 00:11:14.270
And here's an example of finding all
the destinations in the system and

00:11:14.270 --> 00:11:16.600
sending a MIDI packet to each one.

00:11:16.660 --> 00:11:20.320
MIDI Get Number of Destinations and
MIDI Get Destination are used to

00:11:20.320 --> 00:11:22.800
walk through all the destinations.

00:11:22.920 --> 00:11:25.310
And then MIDI Send

00:11:25.580 --> 00:11:28.550
takes as its first
argument an output port,

00:11:28.570 --> 00:11:33.120
which you created at startup,
the destination, which was just returned

00:11:33.200 --> 00:11:36.130
from MIDI Get Destination,
and then a MIDI packet list.

00:11:36.350 --> 00:11:41.050
So in this example,
we're sending the same MIDI packet

00:11:41.050 --> 00:11:41.050
list to all of the destinations.

00:11:42.600 --> 00:11:46.620
Similarly, here's an example of how
to find and receive all the

00:11:46.620 --> 00:11:48.600
MIDI sources in the system.

00:11:48.730 --> 00:11:52.590
Find and open input connections to
all of the MIDI sources in the system.

00:11:52.600 --> 00:11:58.880
We call MIDI_GET_NUMBER_OF_SOURCES and
MIDI_GET_SOURCE to walk

00:11:58.930 --> 00:12:01.510
through all of the sources.

00:12:01.610 --> 00:12:06.490
And then we call
MIDI_PORT_CONNECT_SOURCE to establish

00:12:06.490 --> 00:12:06.590
a connection from that MIDI source
to your program's input port.

00:12:07.260 --> 00:12:10.380
Now the reason we ask you to
create such connections explicitly,

00:12:10.380 --> 00:12:14.530
and this concept is familiar to
those of you who've used OMS,

00:12:14.700 --> 00:12:16.930
is so that if you've got a
bunch of MIDI sources sending

00:12:16.930 --> 00:12:21.920
stuff into the computer,
we only incur the overhead of

00:12:21.980 --> 00:12:27.160
delivering that MIDI to your application
when it's from a source that your

00:12:27.160 --> 00:12:27.160
application cares about listening to.

00:12:27.840 --> 00:12:30.470
And if you remember,
when we created that input port

00:12:31.040 --> 00:12:34.770
at the beginning of the program,
we passed myReadProc,

00:12:34.820 --> 00:12:38.580
and it gets called when
that MIDI comes in.

00:12:38.740 --> 00:12:41.300
A note about the ReadProc.

00:12:41.630 --> 00:12:48.900
The MIDI library creates a thread on your
program's behalf to receive that data.

00:12:48.900 --> 00:12:52.300
So similarly to the way on Mac OS 9,
for those of you who've done

00:12:52.390 --> 00:12:55.270
MIDI programming there before,
your MIDI would come in at

00:12:55.270 --> 00:12:58.540
interrupt level and you'd have
to be careful about critical

00:12:58.740 --> 00:13:00.890
regions and not accessing memory.

00:13:01.420 --> 00:13:04.280
On OS X,
you need to be aware that your read

00:13:04.280 --> 00:13:07.920
proc is called from a separate thread,
and you may have synchronization

00:13:08.010 --> 00:13:11.300
issues with any data you
access from that thread.

00:13:11.300 --> 00:13:13.740
And also be aware it's
a high-priority thread.

00:13:13.870 --> 00:13:16.780
Don't do too much work there.

00:13:18.620 --> 00:13:21.710
Okay, so those are MIDI sources
and destinations.

00:13:21.990 --> 00:13:27.040
The next higher-level object
in the MIDI API is MIDI Entity,

00:13:27.090 --> 00:13:32.870
which is a logical subcomponent
of a device which groups together

00:13:32.870 --> 00:13:32.870
some number of endpoints.

00:13:33.030 --> 00:13:36.050
For example,
you might have a USB device which

00:13:36.050 --> 00:13:40.240
has a general MIDI synthesizer
in it and a pair of MIDI ports.

00:13:40.280 --> 00:13:42.660
That device can be thought
of as having two entities:

00:13:42.710 --> 00:13:44.870
the synthesizer and
the pair of MIDI jacks.

00:13:45.000 --> 00:13:50.330
An eight-port MIDI interface with
eight ins and eight outs might be

00:13:50.330 --> 00:13:55.370
thought of as having eight entities,
each of them with a source

00:13:55.370 --> 00:13:57.840
and a destination endpoint.

00:13:58.020 --> 00:14:02.390
The reason we have this concept of an
entity is so that if your program wants

00:14:02.390 --> 00:14:07.140
to communicate in a bidirectional manner
with some piece of hardware out there,

00:14:07.200 --> 00:14:10.980
you have a way of associating
the sources and destinations.

00:14:11.010 --> 00:14:14.470
You know, which ones constitute a pair.

00:14:17.750 --> 00:14:20.950
The next level up in the
MIDI API is a MIDI device,

00:14:21.180 --> 00:14:24.100
which represents an
actual physical device,

00:14:24.120 --> 00:14:29.760
like a MIDI interface, a card,
something that sits on FireWire.

00:14:29.760 --> 00:14:31.610
It's something that's
controlled by a driver.

00:14:31.730 --> 00:14:35.960
The driver for that device
will have located it and

00:14:36.320 --> 00:14:38.880
registered it with the system.

00:14:39.300 --> 00:14:42.470
And so this diagram here
illustrates how MIDI devices contain

00:14:42.480 --> 00:14:45.590
entities which contain endpoints.

00:14:45.960 --> 00:14:49.020
And so here's a quick look at the
functions you would use to walk

00:14:49.130 --> 00:14:54.100
through the system and locate the
devices and entities that are present.

00:14:54.140 --> 00:14:57.040
MIDI GetNumberOfDevices
and MIDI GetDevice will

00:14:57.040 --> 00:15:03.370
iterate through the devices,
and MIDI GetNumberOfEntities

00:15:03.380 --> 00:15:07.780
and MIDI GetEntity will walk
through the entities that are

00:15:07.780 --> 00:15:07.780
associated with the device.

00:15:11.890 --> 00:15:15.800
Okay, now we've looked at devices,
entities, and endpoints.

00:15:16.020 --> 00:15:20.360
There's a set of calls to find out
information about those devices,

00:15:20.360 --> 00:15:24.120
entities, and endpoints,
and we call these attributes

00:15:24.180 --> 00:15:27.570
"properties." The property
system is extensible,

00:15:27.570 --> 00:15:32.000
meaning that anyone can make up a
property to attach to their device,

00:15:32.020 --> 00:15:35.990
but we've defined a few simple ones,
like its name, its manufacturer name,

00:15:36.000 --> 00:15:39.580
its model number,
the MIDI channels that it's listening on,

00:15:39.650 --> 00:15:41.640
if someone knows that.

00:15:41.850 --> 00:15:43.760
So the system is extensible.

00:15:44.070 --> 00:15:48.520
Properties can be inherited,
which means that if you ask, for example,

00:15:48.520 --> 00:15:51.800
an endpoint,
"What is your manufacturer name?"

00:15:51.800 --> 00:15:54.830
we'll probably end up with the
device's manufacturer name,

00:15:54.830 --> 00:15:56.800
because the driver/writer
will probably have just said,

00:15:56.800 --> 00:15:57.800
"Here's my device.

00:15:57.800 --> 00:16:03.920
I'm the ABC Corporation,
and here's my D4 device." And, you know,

00:16:03.920 --> 00:16:08.370
he's just attached that to the device,
but the entity will inherit

00:16:08.470 --> 00:16:10.800
that property from the device.

00:16:11.010 --> 00:16:15.230
And as I said, for now,
properties are most likely only

00:16:15.230 --> 00:16:17.840
going to be set by drivers.

00:16:19.220 --> 00:16:23.520
Here's a simple example of
obtaining a property of an object.

00:16:23.670 --> 00:16:27.090
We use the MIDI object
getStringProperty call,

00:16:27.180 --> 00:16:32.210
pass the constant kMIDIPROPERTY name,
and we get back a CFString,

00:16:32.350 --> 00:16:33.750
which is the name.

00:16:33.850 --> 00:16:37.290
We can convert it to a C string,
print it, and then release the

00:16:37.460 --> 00:16:39.100
CFStringRef that we got back.

00:16:39.100 --> 00:16:44.690
CFStringRef is part of core foundation,
which you can read about

00:16:44.690 --> 00:16:44.690
in our documentation.

00:16:48.500 --> 00:16:52.960
Okay, the highest-level object in
the MIDI API is the MIDI setup,

00:16:52.960 --> 00:16:57.870
which represents a saved or
saveable state of the system.

00:16:58.140 --> 00:17:01.530
It's essentially just a list of
the devices the drivers locate,

00:17:01.650 --> 00:17:04.810
the MIDI interfaces and
cards that are present.

00:17:05.050 --> 00:17:09.140
But we do have facilities there for
keeping track of some other details,

00:17:09.140 --> 00:17:13.160
like which driver's device
owns the serial port,

00:17:13.160 --> 00:17:17.750
which device does the user prefer
for playing back general MIDI files,

00:17:17.750 --> 00:17:17.990
or whatever.

00:17:19.200 --> 00:17:23.040
Here are some examples of how your
program can manipulate MIDI setups.

00:17:23.110 --> 00:17:26.080
Those of you who've used OMS might
be afraid of the term "setup"

00:17:26.300 --> 00:17:29.590
because there was that OMS setup
program that not everyone liked.

00:17:29.920 --> 00:17:33.100
But we don't have any user
interface involved here,

00:17:33.100 --> 00:17:38.370
although at worst I could envision
a dialog where the user has to

00:17:38.480 --> 00:17:42.100
authorize serial ports to be searched.

00:17:42.250 --> 00:17:44.350
But in any case,
I don't think we're going to

00:17:44.350 --> 00:17:46.100
have any user interface here.

00:17:46.330 --> 00:17:50.360
MIDI setup create simply tells the
system to go interrogate all the drivers,

00:17:50.360 --> 00:17:53.900
find out what hardware is present,
make a MIDI setup containing

00:17:53.900 --> 00:17:55.960
all those devices,
and return it.

00:17:56.100 --> 00:18:00.080
Then you would almost always
call MIDI setup install after

00:18:00.080 --> 00:18:03.660
calling MIDI setup create,
which just tells the system,

00:18:03.660 --> 00:18:05.070
"Here's the MIDI setup.

00:18:05.180 --> 00:18:09.140
Make that the current state until
someone else tells you otherwise."

00:18:09.760 --> 00:18:14.100
MIDI Setup Get Current returns a
reference to the current MIDI setup.

00:18:14.110 --> 00:18:17.600
And then there's MIDI Setup
To Data and MIDI Setup From Data,

00:18:17.600 --> 00:18:20.680
which allow you to convert
a MIDI setup object to and

00:18:20.680 --> 00:18:27.640
from a textual representation,
which is in XML,

00:18:27.640 --> 00:18:27.640
and can be saved to a file.

00:18:31.040 --> 00:18:34.800
Okay,
so that's a tour of some of the objects

00:18:34.800 --> 00:18:36.220
to which you send and receive MIDI.

00:18:36.300 --> 00:18:40.190
Now I'd like to talk about some
of the issues of timing when

00:18:40.350 --> 00:18:44.180
sending MIDI and receiving it.

00:18:45.280 --> 00:18:47.860
Your programs will probably
want to schedule MIDI output

00:18:47.860 --> 00:18:53.010
a little bit in advance,
and you can do that by using timestamps.

00:18:53.150 --> 00:18:56.900
The timestamps in the MIDI packets
we looked at earlier use the host

00:18:57.030 --> 00:18:59.700
clock time as returned by uptime.

00:18:59.750 --> 00:19:02.580
We suggest that you don't schedule
events too far in advance.

00:19:02.730 --> 00:19:06.100
If you schedule a whole
five-minute MIDI file to be played,

00:19:06.150 --> 00:19:10.170
it'll play and you won't have any
way to tell it to stop unless the

00:19:10.170 --> 00:19:12.790
user quits your program probably.

00:19:12.940 --> 00:19:17.180
So we suggest that you use a number, say,
100 milliseconds or so,

00:19:17.180 --> 00:19:20.260
as a guideline of how far
in advance to schedule.

00:19:20.920 --> 00:19:24.260
That's a short enough period
of time to be relatively

00:19:24.260 --> 00:19:28.060
responsive if the user says stop,
but it's also far enough in

00:19:28.060 --> 00:19:31.690
advance so that if you're talking
to a piece of hardware that's got

00:19:31.690 --> 00:19:36.530
some latency in talking to it,
we can still have timing accuracy

00:19:36.530 --> 00:19:39.130
in sending events to that device.

00:19:39.240 --> 00:19:43.550
An important thing here about --
whoops -- pushing buttons here.

00:19:44.080 --> 00:19:48.300
An important thing about
scheduling MIDI output is that

00:19:48.300 --> 00:19:51.870
we have support for devices,
for pieces of hardware that

00:19:51.890 --> 00:19:55.900
are capable of doing their own
scheduling and sending of MIDI.

00:19:56.100 --> 00:20:01.820
A driver may attach a property
to its device that says,

00:20:01.910 --> 00:20:07.030
"I want to schedule events for this
device some number of milliseconds in

00:20:07.030 --> 00:20:12.820
advance." So you as an application writer
should check that property and see if

00:20:12.870 --> 00:20:19.130
the driver writer has attached that
property to the device and respect it.

00:20:19.240 --> 00:20:21.730
So if the driver is saying,
"I want my MIDI 5

00:20:21.730 --> 00:20:25.380
milliseconds in advance,
please," then you as an application

00:20:25.430 --> 00:20:28.880
writer can get the best timing
from the system by making sure

00:20:28.880 --> 00:20:33.690
that your MIDI events get sent
5 milliseconds in advance or more.

00:20:35.860 --> 00:20:38.660
We have a few timing
issues on incoming MIDI.

00:20:38.760 --> 00:20:42.070
We timestamp it with the host
clock as soon as possible.

00:20:42.530 --> 00:20:45.040
And to schedule your own tasks,
there's a lot of different

00:20:45.040 --> 00:20:46.690
ways to do this in Mac OS X.

00:20:46.700 --> 00:20:49.800
There's a number of APIs,
but we're recommending that you

00:20:49.800 --> 00:20:55.170
use the calls in multiprocessing.h,
which is part of Carbon Core.

00:20:58.440 --> 00:21:05.140
Those of you who are writing
MIDI drivers for your own MIDI hardware,

00:21:05.130 --> 00:21:09.210
MIDI drivers are packaged as CFPlugins,
which are a little bit

00:21:09.210 --> 00:21:11.870
intimidating at first glimpse.

00:21:11.990 --> 00:21:13.990
When I first looked at it, I said,
"This looks like COM.

00:21:13.990 --> 00:21:17.130
I'm scared." But it's not that bad.

00:21:17.300 --> 00:21:20.140
We have some example drivers
that you can build on,

00:21:20.240 --> 00:21:21.900
and it makes it pretty easy.

00:21:21.950 --> 00:21:23.900
Usually,
you won't need a kernel extension,

00:21:23.900 --> 00:21:28.760
and this is true if you're
writing a driver for a USB,

00:21:28.760 --> 00:21:32.300
FireWire, or serial MIDI driver.

00:21:32.300 --> 00:21:37.480
If you're writing a PCI card driver,
then you will need a kernel extension.

00:21:38.060 --> 00:21:41.000
But usually, for USB,
in my example drivers,

00:21:41.100 --> 00:21:44.200
I'm just a USB user client.

00:21:44.200 --> 00:21:48.900
And these terms are familiar to those
of you who've seen the I/O Kit sessions.

00:21:49.420 --> 00:21:54.980
And for those of you writing drivers,
I recommend you go find out more

00:21:54.980 --> 00:21:54.980
about I/O Kit if you haven't already.

00:21:55.190 --> 00:21:58.860
The driver programming interface from
the MIDI side of things is pretty simple.

00:21:58.860 --> 00:22:00.940
There's just a few calls to implement.

00:22:01.080 --> 00:22:03.000
There's one to locate your hardware.

00:22:03.180 --> 00:22:06.100
There are calls to start and stop
communicating with your hardware.

00:22:06.110 --> 00:22:10.050
There's a call to send some
MIDI events to your hardware.

00:22:10.090 --> 00:22:14.830
And when you receive
incoming MIDI events,

00:22:14.830 --> 00:22:17.810
then there's a way you can call
back into the MIDI system to

00:22:17.810 --> 00:22:17.810
have those MIDI events delivered.

00:22:18.370 --> 00:22:22.710
I just wanted to make a quick note
here about how having looked at

00:22:22.850 --> 00:22:28.450
the source code for USB drivers
on OS 9 and written one on OS X,

00:22:28.540 --> 00:22:31.430
it's an order of magnitude easier,
at least I thought so,

00:22:31.440 --> 00:22:35.510
on OS X to write a USB driver,
and that was very encouraging.

00:22:37.870 --> 00:22:43.530
Okay, here's a diagram that shows the
pieces of the MIDI implementation.

00:22:43.860 --> 00:22:47.780
At the top we see your
client applications in green.

00:22:47.900 --> 00:22:53.940
We supply a MIDI framework,
which is a client-side dynamic library.

00:22:55.560 --> 00:22:59.150
That library communicates
with the MIDI server process.

00:22:59.230 --> 00:23:04.090
And the reason we have that server
process is so that incoming MIDI from

00:23:04.170 --> 00:23:09.570
some piece of hardware can be efficiently
distributed to multiple applications.

00:23:09.860 --> 00:23:12.830
Below the MIDI server,
we see that it loads and controls

00:23:12.920 --> 00:23:17.140
the MIDI driver plug-ins,
and those driver plug-ins

00:23:17.140 --> 00:23:19.070
communicate with I/O Kit.

00:23:19.770 --> 00:23:23.840
Now, you'll notice in the diagram
the horizontal gray lines.

00:23:23.900 --> 00:23:28.790
Those indicate address space boundaries,
or different processes.

00:23:28.900 --> 00:23:32.230
So we have the kernel and the
MIDI server and your client

00:23:32.230 --> 00:23:34.370
applications' address spaces.

00:23:34.520 --> 00:23:38.240
Which brings us to one of the main
performance issues in dealing with MIDI,

00:23:38.240 --> 00:23:41.980
which is moving data between
different protected address spaces.

00:23:42.140 --> 00:23:46.100
Now, we've got some pretty good and
fast mechanisms for doing it,

00:23:46.280 --> 00:23:49.840
but nonetheless,
it's still important to be aware of that,

00:23:49.840 --> 00:23:52.890
and there are some things you can
do in your program to squeeze extra

00:23:53.000 --> 00:23:55.150
performance out of the system.

00:23:55.670 --> 00:23:58.360
When possible,
do schedule your output a few

00:23:58.360 --> 00:24:03.000
milliseconds in advance and look at that
property of the driver to see if it wants

00:24:03.000 --> 00:24:05.730
to get data a little bit ahead of time.

00:24:05.810 --> 00:24:09.040
And this will especially enable
you to send multiple MIDI events

00:24:09.100 --> 00:24:13.850
that happen close together in time
with a single call to MIDI send.

00:24:13.950 --> 00:24:16.660
So instead of sending
one MIDI event at a time,

00:24:16.790 --> 00:24:19.960
if you package up just even a
few milliseconds of data at a

00:24:19.960 --> 00:24:23.760
time with calls to MIDI send,
that will help the system

00:24:23.760 --> 00:24:25.600
be a bit more efficient.

00:24:25.600 --> 00:24:30.120
And we do have--I should mention--I
just wanted to say we have a dependency

00:24:30.120 --> 00:24:33.100
on the Core OS's scheduling mechanism.

00:24:33.100 --> 00:24:36.490
And things are good there,
but they're getting better.

00:24:38.420 --> 00:24:40.860
Okay,
to show you that MIDI is actually up

00:24:40.940 --> 00:24:45.950
and running on OS X to some extent,
I've got a demo set up here.

00:24:46.390 --> 00:24:50.030
At the bottom you see I've got a
MIDI keyboard and sound module.

00:24:50.360 --> 00:24:55.500
I've got a MIDI interface,
that blue thing, and that connects to

00:24:55.500 --> 00:24:57.490
the computer via USB.

00:24:59.110 --> 00:25:01.780
Over on the right there we see
the various layers of software

00:25:01.990 --> 00:25:07.630
through which MIDI messages
travel when I run these programs.

00:25:13.820 --> 00:25:15.980
Okay,
first I've got a simple program which

00:25:17.020 --> 00:25:20.970
just plays a series of MIDI notes
at very regular intervals using the

00:25:21.050 --> 00:25:23.990
scheduler built into the MIDI server.

00:25:24.050 --> 00:25:27.040
And hopefully we'll hear that
they're very nice and regular.

00:25:28.700 --> 00:25:33.800
[Transcript missing]

00:25:33.800 --> 00:25:40.770
That's pretty good and regular sounding,
I think.

00:25:43.430 --> 00:25:45.500
I've got another program here.

00:25:45.500 --> 00:25:49.600
So here I've just got a keyboard
playing its own internal sounds.

00:25:50.400 --> 00:25:54.040
Now I'm going to play-- I'm
going to run a program that will

00:25:54.040 --> 00:25:57.570
take the MIDI from the keyboard,
send it to the USB interface,

00:25:57.580 --> 00:26:00.600
to the computer,
through the whole stack of software,

00:26:00.740 --> 00:26:04.150
back down to the interface,
and out to the sound module.

00:26:05.880 --> 00:26:09.400
So when I play the keyboard
along-- When I play the keyboard,

00:26:09.400 --> 00:26:12.390
I should be hearing its sound as long
as-- as well as one in the sound module.

00:26:12.600 --> 00:26:21.280
And we shouldn't be hearing any
delays or variations in that delay.

00:26:30.010 --> 00:26:31.440
That sounds pretty good too, I think.

00:26:31.440 --> 00:26:35.000
I don't think the latencies
are excessive or anything.

00:26:35.000 --> 00:26:43.560
I've got a MIDI file I really like here,
so I'm going to play

00:26:43.560 --> 00:26:44.710
a little bit of that.

00:27:28.800 --> 00:27:30.800
Thanks, MIDI Bing Play.

00:27:30.800 --> 00:27:34.570
I've got one more little MIDI file
I'd like to play for you.

00:27:34.620 --> 00:27:39.990
This time I'm also going to play along
with it a little bit using MIDI Through.

00:27:43.340 --> 00:27:47.340
I'm not showing you what I'm actually
doing here because it's ugly.

00:27:47.340 --> 00:27:49.740
I'm just running terminal-based
programs on OS X.

00:27:49.740 --> 00:27:57.690
Which means I have to
remember what to type.

00:29:16.430 --> 00:29:28.240
MIDI is real on Mac OS X.

00:29:28.340 --> 00:29:30.950
So the next thing you're probably
wondering is how can I start to make

00:29:31.050 --> 00:29:34.310
my applications work with MIDI on OS X?

00:29:36.140 --> 00:29:39.300
The MIDI services are not
part of Developer Preview 4.

00:29:39.430 --> 00:29:42.560
They've just been coming together
in the last couple of weeks.

00:29:42.610 --> 00:29:45.390
But we are just about
ready to start seeding.

00:29:45.570 --> 00:29:49.100
So please write to Dan Brown,
who's here in the front row.

00:29:49.100 --> 00:29:53.820
And we've given you an
easy-to-remember email address,

00:29:53.820 --> 00:29:53.820
audio@apple.com.

00:29:54.080 --> 00:29:57.610
We are still holding out the
possibility of tweaking the APIs a

00:29:57.610 --> 00:30:01.270
little bit based on your feedback
and our own release process,

00:30:01.270 --> 00:30:03.360
but we're basically in
a mode of optimizing,

00:30:03.360 --> 00:30:06.360
stabilizing,
and getting ready to release it as part

00:30:06.450 --> 00:30:09.060
of the Mac OS X public beta this summer.

00:30:09.280 --> 00:30:12.240
And I want to remind you
that it is open source.

00:30:12.390 --> 00:30:17.090
So I hope I've given you a good
introduction to MIDI on Mac OS X,

00:30:17.180 --> 00:30:21.450
and I'm really looking forward to
seeing your applications that use it.

00:30:21.450 --> 00:30:21.450
Thank you.

00:30:33.770 --> 00:30:36.270
Doug,
I'd like to bring Chris Rogers out now.

00:30:36.270 --> 00:30:39.680
Chris has been working with
Apple for about a year and has

00:30:39.680 --> 00:30:44.310
been doing quite a lot of work on
the QuickTime Music Architecture,

00:30:44.310 --> 00:30:51.190
and he's also going to be discussing
some of the higher-level audio

00:30:51.190 --> 00:30:52.890
services that we're providing to
applications developers in general.

00:31:00.430 --> 00:31:03.540
Hi, good afternoon.

00:31:03.540 --> 00:31:06.100
As Bill said, my name is Chris Rogers,
and I'm happy to be here to

00:31:06.100 --> 00:31:12.810
talk to you today about music
services available on OS X.

00:31:15.060 --> 00:31:23.210
The topics that we'll be covering today
are new synthesizer replacing the current

00:31:23.260 --> 00:31:25.120
QuickTime Music Architecture synthesizer.

00:31:25.270 --> 00:31:28.050
It's a DLS software synthesizer,
and we'll be discussing

00:31:28.120 --> 00:31:29.000
that in some detail.

00:31:29.000 --> 00:31:35.960
We'll talk about the audio unit and
MusicDevice component architecture,

00:31:36.000 --> 00:31:40.000
how to actually hook these guys
together in different configurations,

00:31:40.000 --> 00:31:43.000
and what that means will
become clear later on.

00:31:43.000 --> 00:31:49.000
The sequencing services,
and the Downloadable Sounds toolbox.

00:31:49.000 --> 00:31:51.000
So let's get on with it.

00:31:52.720 --> 00:31:53.420
OK.

00:31:53.600 --> 00:32:00.120
So where do music services fit in with
the rest of the Core Audio system?

00:32:00.120 --> 00:32:04.370
The music services are higher
level services that sit both

00:32:04.510 --> 00:32:09.020
on top of the MIDI server,
the rest of the MIDI services

00:32:09.020 --> 00:32:13.760
that Doug just presented,
and also the audio I/O devices.

00:32:14.050 --> 00:32:18.720
And Jeff Moore will be discussing this in
great detail in a later talk on Friday.

00:32:18.740 --> 00:32:20.750
I really encourage you to go to that.

00:32:20.800 --> 00:32:24.080
That would be the
Core Audio multi-channel and beyond

00:32:25.100 --> 00:32:28.190
presentation at 2:00 on Friday.

00:32:28.190 --> 00:32:31.360
And both of those systems
actually sit on top of I/O Kit.

00:32:31.480 --> 00:32:35.380
And you may be interested in
how to implement audio drivers.

00:32:35.540 --> 00:32:39.340
And there will be a talk
also on Friday about audio

00:32:39.880 --> 00:32:42.800
family I/O Kit drivers.

00:32:42.800 --> 00:32:45.940
So the music services--.

00:32:46.780 --> 00:32:53.100
are available to all clients,
and also the MIDI server and the

00:32:53.100 --> 00:32:56.120
audio/IO devices are directly
accessible by the client.

00:32:56.140 --> 00:32:59.550
So depending on the level
of access that you require,

00:32:59.690 --> 00:33:04.780
very low-level control,
you may just want to go right down to

00:33:04.780 --> 00:33:09.610
the MIDI server and the IO devices,
or higher-level control,

00:33:09.610 --> 00:33:09.610
you can talk to the MIDI services.

00:33:12.340 --> 00:33:16.640
We're dedicated to
supporting open standards.

00:33:16.690 --> 00:33:20.010
That includes MIDI, of course,
standard MIDI files, RMID files.

00:33:20.260 --> 00:33:25.620
RMID files are standard MIDI files
with a DLS section in the file.

00:33:25.890 --> 00:33:28.140
DLS stands for Downloadable Sounds.

00:33:28.290 --> 00:33:35.200
And that's a sample bank
format where people can include

00:33:35.500 --> 00:33:35.530
their own custom samples,
sound effects.

00:33:35.680 --> 00:33:39.060
High-quality sample, 16-bit stereo,
if you want.

00:33:39.060 --> 00:33:42.120
And also,
we're incorporating some of the ideas

00:33:42.120 --> 00:33:44.870
included in MPEG-4 Structured Audio.

00:33:44.970 --> 00:33:48.830
We're not going as far as implementing
SAIL or anything like that,

00:33:48.830 --> 00:33:55.430
for those who know what MPEG-4 is about,
but we've incorporated some

00:33:55.430 --> 00:33:55.430
of the better ideas in MPEG-4.

00:33:56.890 --> 00:34:02.460
The DLS SoftSynth, this is a synthesizer,
software synthesizer that's

00:34:02.460 --> 00:34:05.880
been completely rewritten
from scratch to replace the

00:34:05.890 --> 00:34:08.430
synthesizer currently in QTMA.

00:34:08.600 --> 00:34:12.810
It, among other things,
it's got a much better reverb,

00:34:12.810 --> 00:34:16.760
several different types,
basically just sounds smoother.

00:34:16.890 --> 00:34:25.020
And what's even better is the reverb
isn't hard-coded into the synth,

00:34:25.020 --> 00:34:25.020
it's

00:34:25.530 --> 00:34:29.040
It's implemented in a modular way
so that third parties can slip in

00:34:29.070 --> 00:34:31.640
their own reverb and other effects.

00:34:31.800 --> 00:34:34.550
We'll see how that works in a little bit.

00:34:36.080 --> 00:34:38.000
What else is in the SoftSynth?

00:34:38.000 --> 00:34:42.810
It has much tighter scheduling
of notes so that you don't get

00:34:42.810 --> 00:34:46.990
this kind of slop that you might
have seen on other synthesizers.

00:34:47.000 --> 00:34:49.900
Scheduling is sample accurate.

00:34:50.090 --> 00:34:55.600
It is a downloadable soundsynth and
it allows for easy importation of

00:34:55.600 --> 00:34:59.000
high-quality third-party sample banks.

00:34:59.000 --> 00:35:03.000
So you don't necessarily have to be
stuck with cheap 8-bit sound set.

00:35:03.230 --> 00:35:06.120
You can load in not only
general MIDI sound sets,

00:35:06.240 --> 00:35:11.000
but arbitrary sample banks
for your own custom music.

00:35:11.000 --> 00:35:15.000
It's a very general
sample-based synthesizer.

00:35:15.000 --> 00:35:19.000
Envelopes are exponential as per the
downloadable sounds specification.

00:35:19.000 --> 00:35:21.670
There's a two-pole
resonant filter in there.

00:35:21.670 --> 00:35:25.000
Unlimited key ranges, velocity ranges,
and layers.

00:35:25.000 --> 00:35:29.000
The layers let you actually
stack multiple samples.

00:35:29.000 --> 00:35:32.110
When you hit the same key,
you can have individual panning and

00:35:32.200 --> 00:35:36.260
modulation parameters on each one,
so you can get nice, fat,

00:35:36.260 --> 00:35:37.990
rich pads that way.

00:35:38.080 --> 00:35:41.390
And also,
DLS provides for much more flexible

00:35:41.390 --> 00:35:46.000
modulation routing possibilities
than the old QTMA synth.

00:35:47.700 --> 00:35:49.940
Okay, now we're going to
talk about audio units.

00:35:49.940 --> 00:35:52.540
Now, what is an audio unit?

00:35:52.560 --> 00:35:55.100
We're going to see in the next couple
of slides what that really means.

00:35:55.100 --> 00:35:58.400
At its most abstract level,
it's kind of a box that

00:35:58.400 --> 00:36:00.450
deals with audio in some way.

00:36:00.460 --> 00:36:07.250
It would take in n audio streams
and output m audio streams.

00:36:07.260 --> 00:36:10.540
And the number of inputs
and outputs can be variable.

00:36:10.540 --> 00:36:16.110
In fact, you may have an audio unit that
has no inputs or no outputs.

00:36:17.190 --> 00:36:19.630
And in at least one case,
there may be an audio unit that

00:36:19.700 --> 00:36:22.230
has no inputs and no outputs.

00:36:22.300 --> 00:36:25.440
And you may think, well,
what would that be?

00:36:25.440 --> 00:36:29.260
And that would be maybe
an audio unit that...

00:36:30.590 --> 00:36:36.320
represents an external MIDI device,
and we'll see how that can wrap up a

00:36:36.320 --> 00:36:45.980
MIDI endpoint through the MIDI services
that Doug spoke about earlier.

00:36:46.140 --> 00:36:51.590
There are other types of audio
units that have no inputs.

00:36:51.680 --> 00:36:56.200
An audio unit that's representing
a hardware input device

00:36:56.390 --> 00:36:58.820
would only have outputs,
and vice versa.

00:36:58.940 --> 00:37:02.510
A hardware output device
would have only inputs,

00:37:02.600 --> 00:37:05.670
and DSP processors would
typically have both inputs and

00:37:05.670 --> 00:37:07.610
outputs for processing audio.

00:37:07.820 --> 00:37:13.440
Some examples of DSP processing
modules would be reverb,

00:37:13.440 --> 00:37:18.290
chorus, delay, ring modulator,
parametric EQ.

00:37:18.460 --> 00:37:19.700
Put a stereo mixer in there.

00:37:19.700 --> 00:37:21.680
It's not really,
probably shouldn't really

00:37:21.700 --> 00:37:26.460
belong in that category,
but stereo mixer would be an audio

00:37:26.470 --> 00:37:33.650
unit that takes in multiple inputs
and mixes according to volume and

00:37:33.650 --> 00:37:34.380
pan information to stereo output.

00:37:35.390 --> 00:37:39.300
Another type of audio unit
would be format converters

00:37:39.540 --> 00:37:43.700
for sample rate conversion,
bit depth conversion, this type of thing.

00:37:43.810 --> 00:37:49.560
And also codecs,
like MP3 coders and decoders.

00:37:51.430 --> 00:37:57.060
Another type of audio unit is one which,
at high level, abstracts the notion of

00:37:57.060 --> 00:37:58.710
a hardware input device.

00:37:58.970 --> 00:38:02.400
That would be layered on top of
the Audio I/O Device APIs that

00:38:02.400 --> 00:38:04.670
Jeff will be talking about on Friday.

00:38:04.880 --> 00:38:11.110
Another audio source would
be a software synthesizer.

00:38:11.690 --> 00:38:13.600
This is an audio unit which
is called the MusicDevice,

00:38:13.640 --> 00:38:17.550
which supports some additional
APIs over the audio unit.

00:38:17.880 --> 00:38:20.970
An audio destination,
that's another type of audio unit.

00:38:20.970 --> 00:38:25.290
And that could, at a high level,
abstract the notion of a

00:38:25.290 --> 00:38:27.120
hardware output device.

00:38:27.460 --> 00:38:35.200
And that would be implemented in
terms of the low-level audio I/O APIs.

00:38:35.260 --> 00:38:36.290
And also a file.

00:38:36.290 --> 00:38:41.540
You could have an audio unit which just
writes its output directly to a file.

00:38:41.670 --> 00:38:45.800
So this just gives you kind of
a flavor of what the range of

00:38:45.800 --> 00:38:51.070
behavior that an audio unit can,
can exhibit.

00:38:52.740 --> 00:38:58.670
Okay, so these individual auto units,
they're kind of interesting on their own,

00:38:58.790 --> 00:39:03.790
but they get even more interesting
when you're able to hook them together.

00:39:04.130 --> 00:39:09.450
In arbitrary configurations,
it was our goal to have an architecture

00:39:09.740 --> 00:39:14.320
that lets developers connect these
modules up in arbitrary ways,

00:39:14.320 --> 00:39:17.760
not just in linear chains like
the current sound manager can do,

00:39:17.960 --> 00:39:22.420
and not just some kind of a
monolithic mixer architecture

00:39:22.450 --> 00:39:24.680
with fixed send returns.

00:39:24.770 --> 00:39:30.010
But we're going for a fully modular
approach where these audio units can

00:39:30.010 --> 00:39:35.270
be connected in pretty sophisticated
ways to create all kinds of different

00:39:35.270 --> 00:39:41.270
interesting high-level software.

00:39:42.750 --> 00:39:48.690
The connections between these audio units
is represented by an AU graph object.

00:39:48.800 --> 00:39:54.330
And there's a whole set of
APIs for dealing with AU graphs.

00:39:54.530 --> 00:39:54.530
I'm not going to go over

00:39:54.770 --> 00:39:58.810
too many specific APIs in my talk,
because there are so many of them,

00:39:58.840 --> 00:40:03.480
and I'm covering so
many different topics,

00:40:03.620 --> 00:40:05.560
actually.

00:40:05.600 --> 00:40:09.350
With the AU graph, in essence,
it represents a set of audio

00:40:09.350 --> 00:40:11.950
units and their connections.

00:40:11.950 --> 00:40:15.540
Like I said,
there's a simple API to create

00:40:15.540 --> 00:40:20.550
and connect them together,
and APIs for actually persisting the

00:40:20.580 --> 00:40:24.900
state of the graph so that you could
save the state to a file or to memory,

00:40:24.900 --> 00:40:27.970
and then reconstruct
the graph based on that.

00:40:30.340 --> 00:40:33.340
Let's look a little bit at the
client API of the Audio Unit.

00:40:33.640 --> 00:40:41.110
Audio Units have properties,
and you can get at those properties

00:40:41.110 --> 00:40:41.110
with Audio Unit GetProperties,
SetProperty, and HasProperty.

00:40:41.570 --> 00:40:45.610
Properties are keyed by ID,
and an ID is really just an integer.

00:40:45.680 --> 00:40:47.300
That's all it is.

00:40:47.480 --> 00:40:52.110
Some of our IDs are predefined,
and others can be defined by

00:40:52.140 --> 00:40:57.130
particular implementers of audio units,
so third parties can define

00:40:57.390 --> 00:41:00.160
their own custom properties.

00:41:00.300 --> 00:41:02.540
Some examples of
properties would be a name,

00:41:02.570 --> 00:41:05.830
number of inputs, so the client,
if the client is interested in

00:41:05.830 --> 00:41:10.710
how many inputs and what kind of
data format these inputs take,

00:41:10.790 --> 00:41:15.290
the client would call getProperty
with the appropriate ID.

00:41:15.630 --> 00:41:19.710
Data is passed by void star and length,
so arbitrary data could be

00:41:19.710 --> 00:41:23.280
passed back and forth between
the client and audio unit,

00:41:23.280 --> 00:41:27.600
and third parties can pass custom
data back and forth in this way.

00:41:29.100 --> 00:42:36.700
[Transcript missing]

00:42:37.940 --> 00:42:43.770
Some examples of parameters are channel
gain and pan for a stereo mixer,

00:42:43.960 --> 00:42:48.130
filter cut-off frequency,
or for that matter,

00:42:48.310 --> 00:42:53.450
resonance in a low-pass filter,
delay time for a chorus delay effect,

00:42:53.700 --> 00:42:55.690
and there are many others.

00:42:58.510 --> 00:43:01.400
One of the most important things you want
to do with these audio units is actually

00:43:01.400 --> 00:43:08.630
get access to the rendered audio coming
out of one of the audio output streams.

00:43:08.630 --> 00:43:12.670
And the Audio Unit Render
call is used for this.

00:43:13.380 --> 00:43:19.030
The client passes in timestamping
information for when the audio buffer

00:43:19.230 --> 00:43:23.790
is to be presented in the audio stream.

00:43:23.870 --> 00:43:27.060
The audio is also rendered
for a specific output.

00:43:27.210 --> 00:43:32.860
So if an audio unit has, say,
four different outputs,

00:43:33.610 --> 00:43:38.060
Audio Unit Render would
be called four times,

00:43:38.060 --> 00:43:38.060
once for each output.

00:43:38.580 --> 00:43:42.440
In the internal implementation
of an audio unit,

00:43:42.630 --> 00:43:50.420
in order to do signal processing,
like say a low-pass filter,

00:43:50.610 --> 00:43:54.040
how does the audio unit actually read
its input in order to do the processing

00:43:54.040 --> 00:43:57.830
and then pass the results back to the
client who calls Audio Unit Render?

00:43:57.960 --> 00:44:02.370
Well, the audio unit actually reads its
input by calling Audio Unit Render

00:44:02.380 --> 00:44:06.250
on another audio unit,
which provides its input.

00:44:06.360 --> 00:44:09.610
And the audio unit
knows which one that is,

00:44:09.610 --> 00:44:12.750
which is its source,
because a connection has been

00:44:12.750 --> 00:44:14.850
established for it ahead of time.

00:44:16.810 --> 00:44:19.360
Okay.

00:44:19.390 --> 00:44:22.900
I'm going to talk about the
two-phase schedule render model.

00:44:23.180 --> 00:44:27.490
On the bottom of this diagram,
you'll see there's a timeline.

00:44:27.490 --> 00:44:29.650
Time is progressing from left to right.

00:44:29.800 --> 00:44:35.890
And this is representing an audio stream
for a particular output of an audio unit.

00:44:36.020 --> 00:44:40.130
And we see that the audio
stream is divided into,

00:44:40.130 --> 00:44:43.090
well in this diagram,
there's three different time slices.

00:44:43.440 --> 00:44:46.510
But conceptually,
you can imagine an audio stream being

00:44:46.650 --> 00:44:51.100
divided up into many different small
time slices for which processing occurs.

00:44:51.330 --> 00:44:59.320
So first of all, for each time slice,
events are scheduled.

00:44:59.320 --> 00:44:59.320
Time, very specific.

00:44:59.700 --> 00:45:06.270
Timestamping information is
provided for these events.

00:45:06.900 --> 00:45:10.150
And secondly, the audio is rendered
for each of the outputs.

00:45:10.340 --> 00:45:14.680
So, for instance, in the first phase,
if this is a software synthesizer,

00:45:14.880 --> 00:45:19.390
all note events which apply for
this given time slice are scheduled,

00:45:19.480 --> 00:45:22.100
and then the audio is rendered.

00:45:25.990 --> 00:45:30.920
The MusicDevice is actually an
audio unit which extends upon the

00:45:31.050 --> 00:45:36.200
audio unit APIs with additional
APIs that are specific to synthesis.

00:45:37.350 --> 00:45:42.150
And the MusicDevice also replaces
the note allocator and music

00:45:42.210 --> 00:45:46.750
component that currently exist
in QuickTime Music Architecture.

00:45:48.580 --> 00:45:53.490
What kind of additional APIs does
the MusicDevice support?

00:45:53.640 --> 00:45:57.950
Mainly,
the APIs center around scheduling notes,

00:45:57.950 --> 00:46:01.360
when notes start and when notes stop.

00:46:02.110 --> 00:46:05.190
The first protocol that's used
is just the MIDI protocol,

00:46:05.200 --> 00:46:07.110
which everybody's familiar with.

00:46:07.260 --> 00:46:12.870
And all music devices would be
expected to support this protocol.

00:46:13.290 --> 00:46:16.200
The second protocol is
an extended protocol,

00:46:16.280 --> 00:46:20.200
which allows for variable
argument node instantiation.

00:46:20.200 --> 00:46:23.000
What does that mean, really?

00:46:23.000 --> 00:46:25.160
In the MIDI protocol,

00:46:26.500 --> 00:46:30.700
There's only two bits of information
which are provided for a note on event.

00:46:30.990 --> 00:46:34.020
That is a note number and a velocity.

00:46:34.190 --> 00:46:38.810
So which note on the keyboard is it,
and how hard did you hit it?

00:46:38.810 --> 00:46:38.810
But,

00:46:38.990 --> 00:46:44.550
That may be insufficient for certain
types of more complex instruments.

00:46:44.610 --> 00:46:47.800
And there may be certain interesting
applications where more information could

00:46:47.960 --> 00:46:49.850
be provided for a note instantiation.

00:46:49.970 --> 00:46:54.900
For instance,
where to position a note in 3D space.

00:46:54.900 --> 00:46:57.780
So additional information can be
provided in this variable argument,

00:46:57.890 --> 00:46:58.900
note instantiation.

00:46:58.970 --> 00:47:06.870
Another example would be in a physical
modeling synthesizer of a drum.

00:47:07.000 --> 00:47:09.050
For anybody who's actually
played a hand drum,

00:47:09.050 --> 00:47:13.220
they know how subtle changes in
the position and how hard you

00:47:13.300 --> 00:47:15.900
hit it and how flat your palm is.

00:47:15.900 --> 00:47:17.680
Or if you hit it with
the tip of your fingers,

00:47:17.800 --> 00:47:20.800
it makes very subtle changes in the
resonances that come out of the drum.

00:47:20.970 --> 00:47:23.900
The sound completely changes
and the character of the tone.

00:47:23.900 --> 00:47:27.020
So this type of information
could be passed in the variable

00:47:27.020 --> 00:47:28.890
argument note instantiation.

00:47:28.900 --> 00:47:30.730
instantiation.

00:47:30.910 --> 00:47:36.290
This extended protocol also supports
more than 16 MIDI channels and

00:47:36.290 --> 00:47:38.790
more than 128 controller messages.

00:47:39.800 --> 00:47:41.000
Music sequencing services.

00:47:41.000 --> 00:47:42.840
We're moving on to a
different topic here.

00:47:42.970 --> 00:47:45.300
This represents a whole
other set of APIs,

00:47:45.300 --> 00:47:49.090
which, once again,
I'm not able to go through in detail

00:47:49.140 --> 00:47:51.680
because there are so many of them,
and I only have limited time to talk.

00:47:51.700 --> 00:47:55.120
But essentially,
this is a set of APIs for constructing

00:47:55.120 --> 00:47:59.340
and editing multitrack sequences,
whether they're MIDI sequences

00:47:59.390 --> 00:48:01.700
or using this extended protocol.

00:48:01.800 --> 00:48:05.910
There's also a runtime for the
real-time playback of these sequences,

00:48:06.020 --> 00:48:08.700
and that's otherwise
known as a sequencer.

00:48:08.910 --> 00:48:12.670
The events themselves can be,
like I said, MIDI events.

00:48:12.720 --> 00:48:17.750
They can be in the extended format,
and there can also be user events,

00:48:17.880 --> 00:48:19.700
which have user-defined data in them.

00:48:19.700 --> 00:48:24.240
And that's up to the developer
to decide how to use.

00:48:25.340 --> 00:48:33.500
The events actually address audio units
and music devices and external MIDI gear

00:48:33.500 --> 00:48:38.340
through MIDI endpoints and directly
through a music device encapsulation.

00:48:40.610 --> 00:48:42.390
So what can we do with these sequences?

00:48:42.560 --> 00:48:46.870
I've already shown previously
a slide where there are these

00:48:46.870 --> 00:48:49.930
audio units connected together
in arbitrary configurations.

00:48:49.930 --> 00:48:51.710
And here we have three audio units.

00:48:51.820 --> 00:48:55.500
So, you know, we have one and two
feeding into the third one.

00:48:55.500 --> 00:49:00.490
And off to the side here we see
a sequence that has three tracks.

00:49:00.510 --> 00:49:03.500
And events from track one are
addressing audio unit one.

00:49:03.500 --> 00:49:05.450
Track two is addressing number two.

00:49:05.500 --> 00:49:09.500
And track three is addressing
audio unit number three.

00:49:09.590 --> 00:49:13.280
The yellow,
the thick yellow arrows represent

00:49:13.280 --> 00:49:16.500
the flow of audio through the system.

00:49:16.500 --> 00:49:18.720
And these blue lines,
these thin blue lines

00:49:18.720 --> 00:49:22.340
represent control information,
scheduling information being

00:49:22.680 --> 00:49:24.350
supplied to the audio units.

00:49:24.550 --> 00:49:28.030
And if you remember back to the
render schedule diagram that

00:49:28.070 --> 00:49:32.070
I had a few slides earlier,
you'll see that the sequence is actually

00:49:32.070 --> 00:49:35.480
providing the schedule part of this,
and the render part is

00:49:35.480 --> 00:49:38.500
actually being pulled through
by the audio units themselves.

00:49:38.500 --> 00:49:39.820
Thank you.

00:49:44.490 --> 00:49:47.360
Here are some features that the
sequencing services provide.

00:49:47.540 --> 00:49:52.540
Just basic cut, copy, paste, merge,
replace, as you would expect in

00:49:52.580 --> 00:49:54.960
a sequencer application.

00:49:55.010 --> 00:49:57.630
Once again,
this does not supply any user interface.

00:49:57.650 --> 00:50:02.400
This is just the low-level engine
which will perform this editing.

00:50:02.400 --> 00:50:06.980
So you just slap a UI on top
of this and you're ready to go.

00:50:07.510 --> 00:50:08.500
Tracks.

00:50:08.500 --> 00:50:11.180
Oh yeah, also,
these edits can be done live

00:50:11.280 --> 00:50:15.560
while the sequence is playing,
so there's no difficulty there.

00:50:16.240 --> 00:50:20.110
Each track in a sequence can
have attributes like mute,

00:50:20.300 --> 00:50:23.390
solo, and looping attributes.

00:50:23.390 --> 00:50:23.390
So you can have

00:50:23.890 --> 00:50:26.050
and I will be joined by a track
which is actually looping over

00:50:26.050 --> 00:50:27.800
and over again on the same events.

00:50:27.800 --> 00:50:32.110
And the loop time is of course
completely configurable.

00:50:35.970 --> 00:50:39.840
The sequencing services could be
used as a core sequencing engine

00:50:39.840 --> 00:50:42.290
for a sequencing application.

00:50:42.420 --> 00:50:46.070
So one of the most difficult things
in a sequencing application is

00:50:46.160 --> 00:50:47.900
to write this sequencing engine.

00:50:48.020 --> 00:50:51.590
Not that writing user
interface code is easy,

00:50:51.590 --> 00:50:57.610
but at least this much work is done,
so this is an opportunity for developers

00:50:57.720 --> 00:51:00.600
to leverage our technology here.

00:51:01.460 --> 00:51:08.110
The scheduling uses units of
beats in floating-point formats.

00:51:08.690 --> 00:51:12.720
There's an implicit tempo
map in the sequence,

00:51:12.720 --> 00:51:14.800
and the sequence format is persistent.

00:51:14.880 --> 00:51:18.530
It can be saved either as a
standard MIDI file format or a

00:51:18.530 --> 00:51:24.150
new data format in QuickTime,
which we're in the process of defining.

00:51:24.510 --> 00:51:28.760
And we welcome your input there as well.

00:51:28.770 --> 00:51:32.090
Okay,
now I'm going to show you a demonstration

00:51:32.090 --> 00:51:34.660
using these sequencing services.

00:51:34.740 --> 00:51:37.760
It's actually a simple
little C program I wrote.

00:51:37.760 --> 00:51:39.620
It's just one or two pages long.

00:51:39.730 --> 00:51:45.740
And it's just basically calling
into these sequencing APIs.

00:51:45.740 --> 00:51:49.030
It's not meant to be
a musical composition.

00:51:49.690 --> 00:51:57.910
Just kind of a basic run-through
of what the sequencer can do.

00:51:58.700 --> 00:52:04.000
What you're going to hear is a cycling
through of general MIDI percussion,

00:52:04.000 --> 00:52:06.700
and after a while you'll hear
a resonant filter come in,

00:52:06.780 --> 00:52:11.120
with the filter sweeping through,
back and forth,

00:52:11.140 --> 00:52:14.630
and on top of that you'll hear

00:52:14.840 --> 00:52:22.640
and I implemented this by actually
connecting the DLS SoftSynth,

00:52:22.650 --> 00:52:25.550
which is an audio unit,

00:52:26.080 --> 00:52:30.000
I created a resonant filter,
and following the resonant filter,

00:52:30.000 --> 00:52:35.790
I put an amplitude modulator audio unit.

00:52:35.790 --> 00:52:35.790
And then I created a sequence

00:52:35.900 --> 00:52:46.600
[Transcript missing]

00:52:46.870 --> 00:52:49.550
I realize it's kind of
echo-y in this place,

00:52:49.670 --> 00:52:50.880
so please bear with me.

00:52:50.960 --> 00:52:53.710
Hope you can get the
gist of what I'm doing.

00:54:05.300 --> 00:54:07.570
This is just a simple example.

00:54:07.590 --> 00:54:10.420
It's a C program,
one or two pages of code,

00:54:10.490 --> 00:54:12.530
something I slapped
together pretty quick.

00:54:12.750 --> 00:54:16.680
Didn't have any user interface
available to me to author

00:54:16.680 --> 00:54:21.080
anything more interesting,
but you can imagine if you had a

00:54:21.100 --> 00:54:23.850
more complicated setup of audio
units representing a number of

00:54:23.860 --> 00:54:29.390
different kinds of processing units,
reverbs and delays and so on,

00:54:29.390 --> 00:54:34.290
you could get quite a lot more
interesting set of effects.

00:54:34.460 --> 00:54:38.400
You gotta kinda wait until we have more
of a library of audio units built up.

00:54:38.630 --> 00:54:43.790
Let's see.

00:54:43.790 --> 00:54:43.800
Where did I leave my little remote?

00:54:46.200 --> 00:54:47.900
Okay,
now we're going to move on to talking

00:54:47.900 --> 00:54:50.050
about the Downloadable Sounds toolbox.

00:54:50.260 --> 00:54:53.300
Once again, this represents a whole set
of APIs which I don't have

00:54:53.300 --> 00:54:59.190
time to go over individually,
but I can talk about at a broad level.

00:54:59.460 --> 00:55:02.820
Downloadable Sounds is both
a sample bank data format and

00:55:02.860 --> 00:55:05.200
it's a sample-based synth model.

00:55:05.200 --> 00:55:10.770
The toolbox provides for reading
and writing DLS Level 2 files and

00:55:11.110 --> 00:55:14.040
creating arbitrary DLS instruments.

00:55:14.200 --> 00:55:18.190
It could be used as a foundation
for a really nice custom instrument

00:55:18.200 --> 00:55:22.150
editor application so that users
can drag in their own samples,

00:55:22.280 --> 00:55:29.200
apply envelopes and LFOs,
and panning and layering and so on.

00:55:29.200 --> 00:55:33.200
So this is a really good
opportunity for third parties.

00:55:33.410 --> 00:55:38.100
This format also replaces
QuickTime Music Architecture Atomic

00:55:38.110 --> 00:55:40.020
Instruments format.

00:55:46.130 --> 00:55:49.680
At the top level,
the DLS toolbox uses a number

00:55:49.680 --> 00:55:52.340
of objects in its APIs.

00:55:52.430 --> 00:55:55.440
The DLS collection is at the top.

00:55:55.500 --> 00:55:57.690
The collection references
a number of instruments.

00:55:57.690 --> 00:56:01.640
Instruments represent --
reference a number of regions.

00:56:01.810 --> 00:56:09.010
And the collection also references
the wave data as DLS wave objects.

00:56:09.010 --> 00:56:09.010
A DLS collection

00:56:09.550 --> 00:56:12.250
contains a set of instruments, as I said.

00:56:12.290 --> 00:56:15.110
It references the WAVE data,
and it also includes

00:56:15.170 --> 00:56:16.250
text-based information.

00:56:16.260 --> 00:56:21.000
It could include copyright information,
the name of the collection, author,

00:56:21.000 --> 00:56:27.460
comments, any kind of tagged text that
a user wants to put in there.

00:56:27.460 --> 00:56:33.240
An instrument is assigned to a
particular MIDI bank and program member,

00:56:33.370 --> 00:56:36.370
and contains a set of regions.

00:56:36.790 --> 00:56:43.090
and also some articulation parameters:
low frequency oscillators, envelopes,

00:56:43.090 --> 00:56:43.090
etc.

00:56:44.330 --> 00:56:49.180
and also contains text information
like the collection does.

00:56:49.180 --> 00:56:53.240
DLS region actually references
the sample data that's going to be

00:56:53.400 --> 00:56:58.580
played and contains the loop points
and defines where in the key range,

00:56:58.690 --> 00:57:04.970
where on the keyboard the sample
will play and in what velocity range.

00:57:05.730 --> 00:57:09.600
And like I said, these regions can be
stacked for layering.

00:57:09.600 --> 00:57:12.180
And also,
these regions can contain articulation

00:57:12.180 --> 00:57:15.950
information like envelopes and LFOs,
which would override those

00:57:16.100 --> 00:57:17.820
found in the instrument.

00:57:18.220 --> 00:57:20.100
and text information.

00:57:20.270 --> 00:57:24.850
The DLS Wave object contains
the actual sample data and the

00:57:24.860 --> 00:57:27.760
sample format for that data.

00:57:29.050 --> 00:57:32.990
And the DLS articulations object.

00:57:33.070 --> 00:57:39.900
That's what actually contains the LFOs,
the envelopes, the reverb send level,

00:57:40.000 --> 00:57:44.000
and all the other modulation
information panning.

00:57:44.470 --> 00:57:47.600
And these objects can be
attached to the DLS regions or

00:57:47.600 --> 00:57:49.890
the DLS instruments as we saw.

00:57:49.940 --> 00:57:54.300
And there's a simple set of APIs for
accessing and setting the relevant

00:57:54.380 --> 00:57:57.930
information in each one of these
objects and connecting them together.

00:57:58.020 --> 00:58:01.450
And it's really a lot easier
to use this API than to try

00:58:01.450 --> 00:58:05.930
to create a DLS collection by
doing a low-level byte munging,

00:58:05.930 --> 00:58:07.710
believe me.

00:58:09.660 --> 00:58:12.100
Backwards compatibility.

00:58:12.140 --> 00:58:14.210
I want to say that we
are supporting the old

00:58:14.220 --> 00:58:16.310
QuickTime Music Architecture components.

00:58:16.530 --> 00:58:17.590
They have been reimplemented.

00:58:17.660 --> 00:58:19.600
The Node Allocator and
the Music component,

00:58:19.700 --> 00:58:22.460
the old software synth,
have been reimplemented on top

00:58:22.520 --> 00:58:25.400
of all of this new technology,
but we are deprecating the

00:58:25.400 --> 00:58:26.980
APIs for these components.

00:58:27.150 --> 00:58:31.480
They continue to work,
but we are really encouraging

00:58:31.480 --> 00:58:36.830
developers to move over to the new
Audio Unit Music Device APIs and

00:58:36.840 --> 00:58:39.440
the Sequencing Service APIs.

00:58:41.940 --> 00:58:44.650
I would want to wrap up
my presentation here,

00:58:44.660 --> 00:58:49.820
and I guess we can move on
to Q&A session with Bill.

00:58:50.160 --> 00:58:54.460
Thanks for giving me time,
and I hope you'll all find

00:58:54.460 --> 00:58:56.810
a use for Music Services.

00:59:02.000 --> 00:59:04.600
Just before we actually
get started on the Q&A,

00:59:04.600 --> 00:59:10.470
I'd like to say that the synthesizer
that's in QuickTime on the

00:59:10.470 --> 00:59:15.150
OS X disks that you have on DP4
is actually the new synthesizer.

00:59:15.160 --> 00:59:19.350
We haven't publicized the APIs yet
as we're still actually going

00:59:19.350 --> 00:59:21.890
through review stages for those APIs.

00:59:22.000 --> 00:59:27.150
But the sample set that's on the
DP4 CD is an 8-bit sample set.

00:59:27.150 --> 00:59:32.000
It's the same sample set that
the current QTMA engine uses.

00:59:32.000 --> 00:59:35.510
But the actual synthesizer,
synthesis engine itself is the

00:59:35.510 --> 00:59:40.970
new MusicDevice components that
Chris has been talking about today.

00:59:41.240 --> 00:59:45.810
And that'll be available for
seeding a little bit later as

00:59:45.810 --> 00:59:48.990
Doug discussed in his talk as well.

00:59:49.000 --> 00:59:52.000
If you send email to audio@apple.com,
you'll get a link to the audio.

00:59:52.000 --> 00:59:54.990
You'll be able to get access
to seeding information.

00:59:54.990 --> 00:59:57.000
We'll be setting up seeding lists.