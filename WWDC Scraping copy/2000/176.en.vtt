WEBVTT

00:00:15.690 --> 00:00:20.590
So we'd like to get this session started,
so I'd like to thank you all for coming.

00:00:20.600 --> 00:00:25.800
It looks like we have a full house,
which is great to see.

00:00:25.800 --> 00:00:32.520
This is the second of three sessions on
audio focusing specifically on Mac OS X.

00:00:32.600 --> 00:00:38.450
The first session we did on Wednesday was
to do with the new audio unit and music

00:00:38.560 --> 00:00:44.180
services that are being provided on OS X,
as well as the MIDI services.

00:00:44.180 --> 00:00:48.700
The MIDI services are obviously at a
lower level to deal with MIDI devices.

00:00:48.710 --> 00:00:52.660
The music and audio units
sort of sit up above those,

00:00:52.660 --> 00:00:56.630
and so application services
that programs like QuickTime and

00:00:56.630 --> 00:00:58.620
other applications would use.

00:00:58.620 --> 00:01:03.510
In this session we're going to be
covering the Sound Manager as it

00:01:03.570 --> 00:01:08.550
exists on OS X and some revisions
of what's been going on with the

00:01:08.550 --> 00:01:11.160
Sound Manager over the last few months.

00:01:11.160 --> 00:01:11.880
And then we'll be getting into
the background of the sound

00:01:11.880 --> 00:01:12.340
management and the sound processing.

00:01:12.560 --> 00:01:15.020
So we've got the sound manager,
which is the lower level interface

00:01:15.020 --> 00:01:18.870
that expresses the interface
between your application or the

00:01:19.040 --> 00:01:23.860
higher levels of the audio engine
to the particular audio devices,

00:01:23.860 --> 00:01:26.130
from an application point of view.

00:01:26.130 --> 00:01:29.120
And then in the session following
this we'll be talking about what is

00:01:29.140 --> 00:01:34.160
going on underneath that API in the
kernel and in the I/O Kit world.

00:01:34.160 --> 00:01:37.470
So without any further ado,
I'll get Jeff Moore to come on,

00:01:37.470 --> 00:01:39.420
who's been doing most of
the work in this area,

00:01:39.480 --> 00:01:40.630
and make him welcome.

00:01:40.630 --> 00:01:41.310
Thank you.

00:01:48.360 --> 00:01:50.040
Hi, everybody.

00:01:50.140 --> 00:01:52.630
Wow, it is full.

00:01:52.690 --> 00:01:55.300
It's good to see there are
this many people who are as

00:01:55.300 --> 00:01:57.700
interested in audio as I am.

00:01:58.620 --> 00:02:00.400
So let's get started.

00:02:00.610 --> 00:02:03.390
So like Bill said,
today we're going to talk a little

00:02:03.400 --> 00:02:05.600
bit about the Sound Manager first.

00:02:05.600 --> 00:02:09.180
We're going to talk a little bit about
what the Sound Manager can do for you.

00:02:09.530 --> 00:02:11.050
Specifically,
we're going to talk a little bit about

00:02:11.050 --> 00:02:12.600
how the Sound Manager is implemented.

00:02:12.600 --> 00:02:16.350
We're also going to talk a little
bit about what's changed in the

00:02:16.350 --> 00:02:18.600
Sound Manager over the past year.

00:02:18.600 --> 00:02:20.600
Been a bunch of different versions.

00:02:20.600 --> 00:02:22.600
It's been a little bit
of confusion about it.

00:02:22.850 --> 00:02:26.440
From that point,
we're going to go into what's new

00:02:26.440 --> 00:02:31.600
with OS X and the Core Audio services
that we've been working on.

00:02:31.600 --> 00:02:36.600
Then, by the end of this,
hopefully you'll understand where we're

00:02:36.600 --> 00:02:41.830
going with all this stuff and you'll be
able to move your own audio data to and

00:02:41.830 --> 00:02:44.550
from devices and all over the system.

00:02:46.160 --> 00:02:49.860
So first we're going to talk a
little bit about the Sound Manager.

00:02:49.880 --> 00:02:54.100
The Sound Manager is available
now on three platforms:

00:02:54.460 --> 00:02:59.900
Mac OS X, Mac OS 9, and Win32.

00:03:00.100 --> 00:03:06.100
The Mac OS X and Win32 versions are
very similar in terms of feature set.

00:03:06.100 --> 00:03:08.970
Mac OS 9 has a few more
features than the other ones.

00:03:09.100 --> 00:03:12.970
Then I'll talk a little bit
about things we've changed.

00:03:13.090 --> 00:03:16.270
Most notably,
we added variable bitrate decoding,

00:03:16.270 --> 00:03:20.590
and we've fixed a bunch of
synchronization bugs recently.

00:03:20.840 --> 00:03:25.720
Then I'm going to talk a little
bit more about what we did to bring

00:03:25.720 --> 00:03:29.140
it up on Carbon and what's there,
what isn't there.

00:03:30.480 --> 00:03:34.800
So, to start off with,
the Sound Manager itself,

00:03:34.800 --> 00:03:40.620
we're all pretty familiar with sound
channels and the general procedural

00:03:40.690 --> 00:03:45.400
API for playing sound and for
bringing sound into your application.

00:03:45.400 --> 00:03:49.640
I thought I'd do a quick review a little
bit about how all that is implemented

00:03:49.640 --> 00:03:54.320
under the hood because it's not exactly
clear because a lot of this stuff is

00:03:54.340 --> 00:03:57.200
not exposed through the procedural API.

00:03:57.400 --> 00:04:01.590
So, under the hood,
all sound channels are implemented

00:04:01.590 --> 00:04:04.400
via a network of sound components.

00:04:04.400 --> 00:04:08.430
They do all the actual processing
work and moving the data and

00:04:08.500 --> 00:04:12.480
massaging it into a format that
you can play or encoding it,

00:04:12.480 --> 00:04:14.400
decoding it, whatever.

00:04:14.400 --> 00:04:20.390
There are a bunch of different areas
that the Sound Manager focuses on.

00:04:20.400 --> 00:04:25.170
Rate conversion is arguably the
most time-expensive thing the

00:04:25.170 --> 00:04:27.400
Sound Manager will do for you.

00:04:27.400 --> 00:04:30.490
Now,
all these little components are linked

00:04:30.490 --> 00:04:33.380
together in pretty much linear chains.

00:04:33.400 --> 00:04:38.210
That is, you won't have two inputs
feeding one sound component.

00:04:38.690 --> 00:04:41.090
They just have one in, one out.

00:04:41.400 --> 00:04:45.270
And then,
all the semantics of the procedural

00:04:45.380 --> 00:04:51.400
API are really handled at the low
level by the system mixer component.

00:04:51.400 --> 00:04:56.670
It's a pretty monolithic
architecture that...

00:04:56.930 --> 00:05:00.980
"It does the job,
but it's beginning to show

00:05:00.980 --> 00:05:03.290
its age a little bit."

00:05:03.960 --> 00:05:09.780
So this diagram kind of represents
a runtime view of a system using the

00:05:09.800 --> 00:05:12.900
Sound Manager and the connections
of the various componentry.

00:05:12.900 --> 00:05:20.640
The first two chains you see show what
is typically the full chain you get.

00:05:20.930 --> 00:05:25.550
That is, you start with a sound source
component whose job is to be the

00:05:25.560 --> 00:05:29.900
traffic cop with the buffer of data
that's being pulled through the chain.

00:05:29.950 --> 00:05:35.690
And then you have a decoder component,
which is the component that will

00:05:35.690 --> 00:05:39.740
take a stream in one format and
convert it into a format that

00:05:40.080 --> 00:05:41.750
the rest of the system can use.

00:05:42.090 --> 00:05:44.790
Typically that's a linear PCM format.

00:05:44.900 --> 00:05:46.900
And then you have an equalizer component.

00:05:46.900 --> 00:05:48.890
The equalizer component
actually has two jobs.

00:05:48.900 --> 00:05:52.730
Its first job is to provide the
implementation of the base and trouble

00:05:52.730 --> 00:05:54.900
controls that you see in QuickTime.

00:05:55.060 --> 00:05:57.510
The second job it has is to
implement the spectrum analyzer

00:05:57.510 --> 00:05:58.900
that you see in QuickTime.

00:05:58.900 --> 00:06:05.040
To do that, it siphons off the stream and
puts things into a buffer and then

00:06:05.040 --> 00:06:07.900
performs the FFT at event time.

00:06:07.900 --> 00:06:11.890
This thing will never perform
its FFT at interrupt time.

00:06:11.900 --> 00:06:16.240
So it's not too much of a performance
drain if you're not using the equalizer

00:06:16.240 --> 00:06:18.890
for actual base and trouble adjustments.

00:06:18.900 --> 00:06:21.900
And then finally,
you have a rate converter component.

00:06:21.900 --> 00:06:23.880
And the rate converter
component does two roles.

00:06:23.920 --> 00:06:26.020
First,
it will take the raw data and convert it

00:06:26.150 --> 00:06:27.900
to the rate that you see in QuickTime.

00:06:27.900 --> 00:06:30.900
It will convert it to the
rate that the hardware wants.

00:06:30.900 --> 00:06:35.810
But it also does the work of
handling the rate multiplier command.

00:06:35.930 --> 00:06:37.830
So if you say,
"Play this sound twice as fast,"

00:06:37.900 --> 00:06:42.180
it upsamples and then downsamples
to get everything back into the

00:06:42.280 --> 00:06:44.870
format that the hardware wants.

00:06:44.970 --> 00:06:47.900
And then finally,
everything is junctioned at

00:06:47.900 --> 00:06:49.900
the system mixer component.

00:06:49.900 --> 00:06:53.710
On Mac OS 9,
you only ever have one mixer component

00:06:53.710 --> 00:06:56.900
talking to one output device component.

00:06:56.900 --> 00:06:59.900
On Windows and on Mac OS X,
you have one of those.

00:06:59.900 --> 00:07:02.900
You have a mixer and an
output device per process.

00:07:03.040 --> 00:07:06.900
Everything is handled in user
space for the sound manager.

00:07:08.800 --> 00:08:50.300
[Transcript missing]

00:08:51.590 --> 00:08:53.700
Go figure.

00:08:53.700 --> 00:08:56.990
You'd see a whole bunch
of unstable connections.

00:08:57.350 --> 00:09:02.300
Specifically, if you were using tunneling
IP for encryption,

00:09:02.300 --> 00:09:06.790
you would have a lot of
trouble trying to get through.

00:09:08.330 --> 00:09:11.780
So, variable bitrate decoding was
a very interesting problem

00:09:11.950 --> 00:09:14.590
and a very interesting feature
to add to the Sound Manager.

00:09:14.940 --> 00:09:18.540
Now, as most of you know,
variable bitrate encoding is a

00:09:18.540 --> 00:09:23.200
technique that varies the number
of bits used to encode a sample

00:09:23.200 --> 00:09:25.520
or a block of samples over time.

00:09:26.450 --> 00:09:30.780
Typically,
this yields better quality encoding

00:09:30.870 --> 00:09:33.740
for a lower overall bitrate.

00:09:33.740 --> 00:09:41.570
We added that specifically to support
QuickTime's MP3 decoder in QuickTime 4.1.

00:09:42.870 --> 00:09:48.330
So to do it,
we had to basically grapple with the

00:09:48.450 --> 00:09:53.240
fundamental issue that you don't,
with variable bitrate situations,

00:09:53.430 --> 00:09:56.800
quite often you just don't know
the relationship between a buffer's

00:09:56.800 --> 00:09:59.600
size in bytes and the number
of samples you're going to get

00:09:59.600 --> 00:10:01.180
out of that when you decode it.

00:10:01.590 --> 00:10:05.150
And as the Sound Manager is
pretty reliant on the fact

00:10:05.310 --> 00:10:09.160
that you know ahead of time the
number of samples in a buffer.

00:10:09.160 --> 00:10:17.270
So we had to go and mess things up a
little bit so that we could express

00:10:17.270 --> 00:10:21.000
that notion of a buffer size in
terms of the number of bytes it had

00:10:21.000 --> 00:10:22.600
rather than the number of samples.

00:10:22.600 --> 00:10:26.310
To do this, we ended up extending
three data structures.

00:10:26.550 --> 00:10:29.980
The scheduled sound header
needed to be extended so that

00:10:29.980 --> 00:10:34.000
you had access to schedule a
block of variable bitrate data.

00:10:34.200 --> 00:10:39.360
The sound component data structure
had to be extended so that the

00:10:39.440 --> 00:10:44.240
component chains could talk about
variable bitrate data with each other.

00:10:44.380 --> 00:10:47.520
And then the sound param block had
to be changed so that the mixer

00:10:47.520 --> 00:10:49.790
could keep track of it as well.

00:10:49.940 --> 00:10:52.120
In all cases,
what we did to extend these data

00:10:52.120 --> 00:10:55.520
structures were we added a new flag
to their flags field that said,

00:10:55.520 --> 00:11:00.250
"I'm extended." And then you could cast
that to one of these extended structures

00:11:00.270 --> 00:11:05.160
and then you would access to extended
fields that included another flags field.

00:11:05.280 --> 00:11:09.440
And the flags field for that was used
to indicate whether the structures was

00:11:09.440 --> 00:11:12.800
counting by sample frames or by bytes.

00:11:12.850 --> 00:11:16.870
And then the field to actually
hold the byte count as well.

00:11:19.450 --> 00:11:24.950
So, in addition to that,
we also revved the Sound Converter

00:11:25.360 --> 00:11:28.230
API to better support variable
bitrate situations and,

00:11:28.230 --> 00:11:30.810
in fact,
be a better and easier to use system in

00:11:30.810 --> 00:11:33.400
general just for any sort of conversion.

00:11:33.400 --> 00:11:37.400
So, we added a new routine called
Sound Converter Fill Buffer.

00:11:37.400 --> 00:11:41.130
It's a direct replacement for
the functionality you got from

00:11:41.130 --> 00:11:43.400
Sound Converter Convert Buffer.

00:11:43.400 --> 00:11:46.240
In particular,
the difference is that the mechanism

00:11:46.240 --> 00:11:50.400
for moving data through Sound Converter
Fill Buffer is a callback mechanism

00:11:50.400 --> 00:11:55.430
where you specify a routine that
the Sound Converter can call to

00:11:55.530 --> 00:11:57.400
get more data to decode or encode.

00:11:57.400 --> 00:12:02.270
This gives you complete control
over the buffering in the system.

00:12:02.410 --> 00:12:05.500
You specify the output buffering when
you call Sound Converter Fill Buffer,

00:12:05.610 --> 00:12:09.340
and you have complete control of the
input buffering because you have a

00:12:09.340 --> 00:12:11.320
function to feed it into the system.

00:12:11.400 --> 00:12:13.820
Finally,
this obviated the need for calling

00:12:13.820 --> 00:12:17.730
Sound Converter to get buffer sizes
for really any reason if you're using

00:12:17.730 --> 00:12:21.870
Sound Converter Fill Buffer since
you are already in control of all the

00:12:21.910 --> 00:12:24.360
information both on input and output.

00:12:24.530 --> 00:12:28.390
There's no need to ask the
Sound Converter about it anymore.

00:12:29.270 --> 00:12:33.160
So the best place to find more
about this stuff is the recent

00:12:33.160 --> 00:12:37.250
QuickTime 4.1 developer update note.

00:12:37.440 --> 00:12:41.090
And that's a URL where you
can get the PDF version.

00:12:46.700 --> 00:13:58.700
[Transcript missing]

00:13:59.780 --> 00:14:04.320
So the other big thing we did
was the Carbon Sound Manager.

00:14:04.320 --> 00:14:09.660
And we brought that up for,
I think it was DP2.

00:14:09.670 --> 00:14:17.510
Yeah, it first showed up in
Developer Preview 2 of OS X.

00:14:17.780 --> 00:14:23.300
So the Sound Manager for Carbon is
pretty much full featured,

00:14:23.320 --> 00:14:25.320
with a few exceptions.

00:14:25.320 --> 00:14:28.690
Now, in most cases,
the features that we did not choose

00:14:28.740 --> 00:14:35.750
to port were primarily because the
functionality was either duplicated by

00:14:35.760 --> 00:14:42.570
other services in the Sound Manager,
or indeed on other services in the OS.

00:14:42.990 --> 00:14:47.870
But also because the services
that they were providing were

00:14:47.870 --> 00:14:49.490
obsolete in a lot of cases.

00:14:50.040 --> 00:14:53.590
So, among other things,
these included the wavetable

00:14:53.590 --> 00:14:56.220
synthesizer and related commands.

00:14:56.220 --> 00:14:59.870
This is things like freak command and,
let's see,

00:15:00.180 --> 00:15:05.080
there are a bunch of other ones
that escape me at the moment.

00:15:05.100 --> 00:15:08.370
The other big one that we didn't include,
choose to port,

00:15:08.370 --> 00:15:10.320
was sound play double buffer.

00:15:10.320 --> 00:15:13.280
Sound play double buffer is
probably the most inefficient

00:15:13.280 --> 00:15:18.240
mechanism for feeding sound into
the Sound Manager at the moment.

00:15:18.240 --> 00:15:22.590
You are much better off to use buffer
commands and callback commands,

00:15:22.630 --> 00:15:25.720
or even better yet,
you should be using scheduled sound.

00:15:25.720 --> 00:15:30.200
Scheduled sound is far and away the
best way to get sound in and out of

00:15:30.200 --> 00:15:33.610
the Sound Manager at a specific time.

00:15:33.970 --> 00:15:40.900
We also don't have any support for
recording to or playing from disk.

00:15:40.900 --> 00:15:44.870
Again, we feel that these features are
best accomplished through QuickTime.

00:15:44.900 --> 00:15:51.170
Then there were a bunch of other sound
commands whose services that we just

00:15:51.170 --> 00:15:54.900
don't need anymore or didn't work right.

00:15:54.900 --> 00:15:57.840
Some examples of these
are the amp command.

00:15:57.920 --> 00:16:01.900
It's pretty much exactly
what the volume command does.

00:16:01.900 --> 00:16:04.910
The rate command,
which does sort of what the

00:16:04.910 --> 00:16:09.330
rate multiplier command does,
only it has an interesting problem

00:16:09.340 --> 00:16:14.040
in that it treats all the rates as if
you were scaling 22 kilohertz sound,

00:16:14.160 --> 00:16:18.600
which can give you unpredictable
results if you're not sure

00:16:18.600 --> 00:16:20.870
ahead of time what you're doing.

00:16:20.900 --> 00:16:23.320
Then there's commands
like the load command,

00:16:23.320 --> 00:16:26.900
which were about querying registers
on the old Apple sound chip.

00:16:26.900 --> 00:16:29.900
I don't think anybody was using those.

00:16:29.900 --> 00:16:29.900
At least I hope not.

00:16:32.210 --> 00:16:36.960
So, ultimately,
where this leaves us right now is that

00:16:36.960 --> 00:16:42.550
we have a system that's pretty well
optimized for handling 16-bit words,

00:16:43.040 --> 00:16:49.060
stereo channel formats,
with a 44.1 kilohertz sampling rate.

00:16:49.100 --> 00:16:53.180
Now, we support constant and
variable bitrate formats,

00:16:53.180 --> 00:16:56.100
usually the simpler variety of those.

00:16:56.300 --> 00:16:59.460
And we're using a lot,
we're seeing native

00:16:59.460 --> 00:17:03.090
processing coming along,
we're using a lot of it ourselves.

00:17:03.180 --> 00:17:07.120
And we've also got a fairly
loose synchronization model,

00:17:07.120 --> 00:17:09.100
all things said and done.

00:17:09.380 --> 00:17:14.080
But it's doing the job pretty
much for what we have to do today.

00:17:14.530 --> 00:17:18.160
So going forward,
the sorts of things that we see

00:17:18.200 --> 00:17:23.300
coming down the road are at the
hardware we see 24-bit integer

00:17:23.300 --> 00:17:25.500
formats coming straight at us.

00:17:25.500 --> 00:17:29.280
And then in the software realm,
you see a very heavy reliance on

00:17:29.280 --> 00:17:33.500
32-bit floating point to support
all the bandwidth you need.

00:17:33.500 --> 00:17:36.680
And then in channel formats,
surround sound is just

00:17:36.680 --> 00:17:38.500
beginning to take off now.

00:17:38.500 --> 00:17:40.350
You're seeing it more and
more at the consumer level.

00:17:40.500 --> 00:17:43.480
You're seeing it more and
more at the authoring level.

00:17:43.520 --> 00:17:46.390
A lot of games are being authored in 5.1.

00:17:46.500 --> 00:17:49.030
And in the pro market,
you're starting -- in

00:17:49.090 --> 00:17:52.450
the authoring market,
you're seeing much higher sampling rates

00:17:52.510 --> 00:17:54.490
than what we've been used to in the past.

00:17:54.500 --> 00:17:58.350
96 kilohertz certainly looks
like it's going to be the

00:17:58.350 --> 00:18:00.500
next bump up in the standard.

00:18:00.500 --> 00:18:05.310
And then we're also beginning to
see even more complex encoding

00:18:05.310 --> 00:18:10.370
schemes than the variable bitrate
stuff that you see in MP3.

00:18:10.600 --> 00:18:13.970
Specifically, you see codecs that you,
that,

00:18:13.970 --> 00:18:19.600
encodings that are going to be doing
different techniques for data resiliency

00:18:19.610 --> 00:18:21.500
when you transport it over the network.

00:18:21.500 --> 00:18:27.470
New kinds of perceptual encoding
techniques that result in better,

00:18:27.470 --> 00:18:30.350
smaller, faster, whatever.

00:18:30.490 --> 00:18:35.490
But they're coming and we
need to be ready for them.

00:18:35.620 --> 00:18:38.840
And then we also see, you know,
native processing is

00:18:38.900 --> 00:18:40.500
just going to explode.

00:18:40.520 --> 00:18:42.000
We're talking multi-processors.

00:18:42.000 --> 00:18:43.490
I mean, it's like 4G, 4G.

00:18:43.500 --> 00:18:48.250
You've got plenty of bandwidth
to burn for signal processing.

00:18:48.640 --> 00:18:51.920
In addition to that,
we also see hardware acceleration

00:18:51.920 --> 00:18:53.370
starting to take off.

00:18:53.680 --> 00:18:56.560
In fact, on the PC platform,
it's already taken off

00:18:56.560 --> 00:19:00.390
like crazy for games,
for doing 3D rendering.

00:19:00.500 --> 00:19:06.190
We also see an increasing requirement for
tight synchronization with other media,

00:19:06.490 --> 00:19:09.430
both internal media and
external to the box.

00:19:09.500 --> 00:19:12.890
Increasingly,
we need to run machines in sync over a

00:19:12.890 --> 00:19:15.490
network or in sync with a SMPTE deck.

00:19:15.500 --> 00:19:22.500
We feel that all those features need to
be encompassed in any audio architecture

00:19:22.500 --> 00:19:26.890
at the operating system level.

00:19:27.200 --> 00:19:28.060
What is Core Audio?

00:19:28.230 --> 00:19:30.540
What are we going to do
about all that stuff?

00:19:30.620 --> 00:19:35.900
First, we're going to provide a new
low-level audio device API.

00:19:36.040 --> 00:19:38.500
Specifically,
it's geared to allow you to read

00:19:38.510 --> 00:19:44.130
and write data from a given device,
and to do that in a way that can

00:19:44.220 --> 00:19:47.770
be shared across many processes.

00:19:47.920 --> 00:19:51.590
Then, we're also going to do some
inter-application communication stuff,

00:19:51.750 --> 00:19:55.810
so that you can have audio being
generated in one application and sent

00:19:55.950 --> 00:19:58.710
to be processed in another application.

00:19:59.270 --> 00:20:03.190
And then on Wednesday,
Chris Rogers went in-depth about

00:20:03.270 --> 00:20:07.980
the new component model that
we're going to be supporting on

00:20:07.980 --> 00:20:12.540
OS X and in other places as well,
the audio unit architecture.

00:20:12.770 --> 00:20:16.350
And I hope you saw his talk because
you're going to hear a lot more about

00:20:16.350 --> 00:20:18.750
that stuff as the year progresses.

00:20:18.850 --> 00:20:23.990
And then probably the best news about
all this is that we're going to open

00:20:23.990 --> 00:20:28.150
source all the low-level sources,
or the services.

00:20:29.790 --> 00:20:33.410
We feel pretty strongly that
while we think we're pretty good,

00:20:33.410 --> 00:20:37.360
we know what we're doing,
you guys have more knowledge

00:20:37.360 --> 00:20:41.450
about the specific areas and more
knowledge about your specific

00:20:41.520 --> 00:20:44.840
hardware so that you could tell us,
"Hey, you're not doing that right," or,

00:20:44.840 --> 00:20:48.640
"You're not going to be able to support
my hardware." We really want to encourage

00:20:48.990 --> 00:20:54.140
you to participate in what we're
trying to do and to make it better.

00:20:55.770 --> 00:21:00.110
So this diagram kind of lays
out the way Core Audio is

00:21:00.110 --> 00:21:03.030
being spread about the system.

00:21:03.100 --> 00:21:04.630
Now down in the kernel you have I/O Kit.

00:21:04.710 --> 00:21:06.200
That's where all the drivers live.

00:21:06.310 --> 00:21:08.100
That's where all the hardware lives.

00:21:08.160 --> 00:21:10.990
So the big problem with a
protected mode system like

00:21:11.000 --> 00:21:13.650
this is how do you manage the,
how do you get the communication out

00:21:13.770 --> 00:21:18.600
to the application so that you can move
the data fast enough and enough of it so

00:21:18.600 --> 00:21:20.990
that you can do something useful with it.

00:21:21.250 --> 00:21:24.760
So we're going to tackle that
with the Audio Device API.

00:21:24.800 --> 00:21:30.960
It specifically manages moving data
to and over the kernel user boundary.

00:21:31.000 --> 00:21:35.640
And it sits entirely itself,
entirely in user space.

00:21:35.660 --> 00:21:40.020
It doesn't have a single part piece
of it that lives in the kernel.

00:21:40.330 --> 00:21:44.120
And then it lives individually
within each client process

00:21:44.120 --> 00:21:45.860
that's using it as well.

00:21:45.960 --> 00:21:49.970
So each, so you link,
it's just another shared library.

00:21:50.110 --> 00:21:52.730
And then on top of that,
while clients can directly

00:21:52.730 --> 00:21:55.960
access the Audio Device API,
we only really expect to see that

00:21:55.960 --> 00:22:01.640
done with clients that have a really
high degree of need for low level

00:22:01.730 --> 00:22:04.360
control and management and whatnot.

00:22:04.360 --> 00:22:08.600
Mostly we hope to see you using
audio units to deal with the hardware

00:22:08.600 --> 00:22:12.540
as we'll be providing audio units
that completely wrap up the use of

00:22:12.540 --> 00:22:18.780
the Audio Device API as well as the
audio IPC mechanisms that will allow

00:22:18.780 --> 00:22:21.000
you to move data across processes.

00:22:21.000 --> 00:22:23.370
completely in user space again.

00:22:24.370 --> 00:22:27.360
So, what are the goals of
the Audio Device API?

00:22:27.450 --> 00:22:31.390
So, like I said,
it's designed to be multi-client.

00:22:31.800 --> 00:22:37.790
The Macintosh, since its inception,
has been able to play sound in

00:22:37.790 --> 00:22:41.830
two applications simultaneously.

00:22:42.190 --> 00:22:45.400
If we weren't able to do that in OS X,
that would be a huge step back.

00:22:45.570 --> 00:22:51.030
So it's very important that multi-client
features be carried forward.

00:22:51.120 --> 00:22:53.790
Further, we're supporting multi-channel.

00:22:54.080 --> 00:22:56.190
Obviously,
we think surround sound is important.

00:22:56.220 --> 00:23:00.740
We've got to be able to talk to devices
that have more than two channels.

00:23:00.810 --> 00:23:04.460
And to be able to do that in
a way that has low latency,

00:23:04.600 --> 00:23:07.670
so that when you say,
"Play this buffer," it gets out there on

00:23:07.670 --> 00:23:10.210
the wire as fast as we can get it there.

00:23:10.310 --> 00:23:15.040
The Audio Device API is designed
to have very low overhead.

00:23:15.250 --> 00:23:20.360
Specifically, for the latency,
we depend very heavily on the

00:23:20.440 --> 00:23:24.920
Core OS Scheduler to make sure that
the threads that have our code in

00:23:24.920 --> 00:23:27.340
them run when they're supposed to run.

00:23:27.440 --> 00:23:31.520
Then the latency will obviously also
depend on the transport layer you're

00:23:31.520 --> 00:23:33.960
using to send the audio to the hardware.

00:23:34.090 --> 00:23:36.800
It's like PCI has one kind of latency.

00:23:37.120 --> 00:23:40.090
USB has a totally
different kind of latency.

00:23:40.250 --> 00:23:44.220
Then another primary goal
is synchronization with

00:23:44.220 --> 00:23:46.030
the Audio Device API.

00:23:46.210 --> 00:23:50.760
It's very important that a device be able
to be synchronized with both internal

00:23:50.760 --> 00:23:53.080
hardware and with external hardware.

00:23:53.190 --> 00:24:01.160
Both SMPTE signals, digital clocks,
word clocks, Studio Sync, everything.

00:24:01.160 --> 00:24:04.890
We're bringing that into the system.

00:24:05.060 --> 00:24:08.620
Then finally,
we hope it sucks a little bit less.

00:24:13.620 --> 00:24:14.600
Jim here.

00:24:14.600 --> 00:24:21.800
So the data formats for
the Audio Device API,

00:24:21.810 --> 00:24:24.880
well, it's pretty much format agnostic.

00:24:24.880 --> 00:24:27.460
Now,
we do treat PCM data a little bit better

00:24:27.460 --> 00:24:31.020
than we treat other kinds of data,
but by and large,

00:24:31.020 --> 00:24:34.730
whatever your hardware wants to take,
the Audio Device API is prepared

00:24:34.730 --> 00:24:37.330
to ask for it from the client code.

00:24:37.330 --> 00:24:41.990
We pass every, all the data is passed
around in void stars,

00:24:42.060 --> 00:24:46.740
and we don't make any
requirements about the data.

00:24:46.740 --> 00:24:52.040
Now, if you do choose to use PCM,
the PCM format that we use

00:24:52.040 --> 00:24:55.730
internally is 32-bit floats,
and we support both interleaved and

00:24:55.730 --> 00:24:59.930
non-interleaved streams for PCM data.

00:25:00.100 --> 00:25:01.940
And further,
we'll do all the mixing for you

00:25:01.940 --> 00:25:05.200
internally if you're using PCM data.

00:25:05.200 --> 00:25:07.350
Otherwise,
you pretty much have to rely on

00:25:07.360 --> 00:25:11.060
the driver supporting mixing in
some fashion for other formats,

00:25:11.060 --> 00:25:15.120
because we don't know how to do that,
and we're willing to let

00:25:15.210 --> 00:25:19.190
the driver figure it out.

00:25:20.830 --> 00:25:25.880
And further, with the 32-bit floats,
to actually convert into

00:25:25.880 --> 00:25:28.640
the hardware format,
we also rely on the driver,

00:25:28.720 --> 00:25:32.140
providing us a routine to do that.

00:25:33.950 --> 00:25:39.070
So, conceptually,
a device in this model encapsulates

00:25:39.340 --> 00:25:43.270
an I/O cycle to read and write
the data to the device and a

00:25:43.390 --> 00:25:45.900
clock to keep track of that I/O.

00:25:45.970 --> 00:25:50.290
And the clock generates timestamps
that specifically map out the

00:25:50.320 --> 00:25:54.370
relationship between the host clock,
which in our case is

00:25:54.370 --> 00:25:57.900
the CPU time register,
as specified by uptime,

00:25:57.900 --> 00:26:04.080
and there are other system services on
OS X for retrieving that clock value,

00:26:04.080 --> 00:26:08.640
and the sample clock of the hardware,
that is the counter that's

00:26:08.640 --> 00:26:10.900
counting the samples going by.

00:26:10.900 --> 00:26:15.080
It's really important to
know to a high degree,

00:26:15.080 --> 00:26:18.820
as accurately as you can,
the relationship between when

00:26:18.820 --> 00:26:22.900
a sample is played and the host
clock time that it was played at.

00:26:23.010 --> 00:26:25.900
You'll see why in a few minutes.

00:26:25.900 --> 00:26:29.900
Devices also have a set of properties.

00:26:29.900 --> 00:26:33.890
Properties are used to describe the
state and configuration of a device.

00:26:33.900 --> 00:26:38.440
They have getters and setters,
and they are specified

00:26:38.530 --> 00:26:40.900
as selector value pairs.

00:26:40.900 --> 00:26:47.180
The selector is an integer ID,
and the value is any format that

00:26:47.180 --> 00:26:50.900
the property wishes to express.

00:26:50.900 --> 00:26:54.730
Again,
it's expressed as a void star in the API.

00:26:55.620 --> 00:27:00.510
Another big feature of properties
is that you can schedule the

00:27:00.510 --> 00:27:06.890
driver to make the change to the
property for you ahead of time.

00:27:07.020 --> 00:27:10.250
That way, if the driver supports it,
that gives you a way to do

00:27:10.250 --> 00:27:13.900
sample accurate scheduling
with hardware changes.

00:27:13.900 --> 00:27:20.260
This will be much more important as
FireWire devices start coming online.

00:27:21.540 --> 00:27:25.850
Finally,
clients can register for notifications

00:27:25.850 --> 00:27:28.500
in the changing of properties.

00:27:29.500 --> 00:27:32.780
The notification mechanism
specifically will,

00:27:32.840 --> 00:27:38.320
you will get a notification only
if the value changes and if it

00:27:38.340 --> 00:27:40.560
changes anywhere on the system.

00:27:40.560 --> 00:27:45.160
So if process A makes a change and
process B is looking for that change,

00:27:45.270 --> 00:27:49.750
process B will get a notification
that process A changed that value.

00:27:51.280 --> 00:27:56.060
So, on the inside,
what we have is a single ring

00:27:56.060 --> 00:28:01.310
buffer that gets mapped by
I/O Kit into each client process.

00:28:01.730 --> 00:28:05.820
Now, IO-Kit reads and writes to
this buffer asynchronously

00:28:05.820 --> 00:28:08.080
from what the client's up to.

00:28:08.080 --> 00:28:11.600
In fact,
it's usually done via DMA program.

00:28:11.600 --> 00:28:14.840
We're trying to avoid having to
have any kernel threads actually

00:28:14.840 --> 00:28:17.880
executing to do any audio
processing if we can at all help it.

00:28:18.000 --> 00:28:23.760
And then the interrupts that
we get in this system are only

00:28:23.830 --> 00:28:28.440
generated when the device driver
wraps back around the ring buffer.

00:28:29.190 --> 00:28:33.810
Now, the size of the ring buffer
is typically fairly large.

00:28:33.980 --> 00:28:37.760
In the current system,
it's about three-quarters of a second.

00:28:37.800 --> 00:28:41.680
So you're going to only see the interrupt
rate for this system is extremely low.

00:28:41.800 --> 00:28:45.700
Now, you all have a lot of questions,
I can tell.

00:28:48.400 --> 00:28:54.030
So, every time the buffer wraps around,
we get a new timestamp for

00:28:54.250 --> 00:28:56.400
when that wraparound happened.

00:28:56.400 --> 00:29:01.300
And that timestamp comes out of the
sample clock and from the host clock.

00:29:01.300 --> 00:29:05.040
And you can also get a
timestamp whenever you want.

00:29:05.040 --> 00:29:07.480
So you can generate
more as you need them.

00:29:07.480 --> 00:29:11.040
And sometimes those timestamps are
going to be interpolated because

00:29:11.040 --> 00:29:16.140
we may not have specific data on
the time that you're asking for.

00:29:16.140 --> 00:29:20.990
So we have code that keeps track
of the history of time and can

00:29:20.990 --> 00:29:24.740
predict one value given the other.

00:29:25.730 --> 00:29:28.780
In this diagram,
you kind of see what's going on

00:29:28.900 --> 00:29:31.400
from the device's point of view.

00:29:31.440 --> 00:29:34.440
So there's an input ring buffer
and an output ring buffer.

00:29:34.640 --> 00:29:39.100
And the client is spinning around,
reading and writing to those buffers.

00:29:39.330 --> 00:29:43.210
And the DMA heads chase those
heads around and clean up or

00:29:43.210 --> 00:29:45.280
write new data after them.

00:29:45.500 --> 00:29:48.900
And then, as you can see,
when the head gets back

00:29:48.980 --> 00:29:52.540
to the interrupt point,
we'll raise an interrupt.

00:29:52.700 --> 00:29:55.160
And at that point,
we'll generate some new timestamps.

00:29:55.400 --> 00:29:58.250
And we'll do a few other
housecleaning chores.

00:29:58.620 --> 00:30:01.440
But other than that,
we don't actually call out

00:30:01.510 --> 00:30:03.460
to applications to get data.

00:30:03.610 --> 00:30:08.090
That's a big difference to the way
systems have worked in the past.

00:30:08.620 --> 00:30:11.520
So how do we get data to the system?

00:30:11.630 --> 00:30:15.920
So from the client's point of view,
implicitly,

00:30:15.920 --> 00:30:21.420
you're going to be doing a lot of
multi-threaded programming with audio.

00:30:21.620 --> 00:30:23.800
Just be clear on that because
that's pretty complicated

00:30:23.800 --> 00:30:29.940
and it's a lot different than
the Mac OS 9 implementation.

00:30:30.040 --> 00:30:32.510
There are a lot of new issues
you're going to have to face

00:30:32.510 --> 00:30:35.980
about atomic operations on data,
making sure that you're not

00:30:36.390 --> 00:30:39.560
waiting on a semaphore that's not
ever going to get signaled on,

00:30:39.660 --> 00:30:43.100
or the usual things that you have
to deal with when you're dealing

00:30:43.160 --> 00:30:45.410
with a multi-threaded environment.

00:30:45.410 --> 00:30:49.400
It's now coming to bear right on
you when you're dealing with audio.

00:30:50.050 --> 00:30:54.520
So the client has one high
priority run to completion

00:30:54.640 --> 00:30:58.640
thread per device in the process.

00:30:58.640 --> 00:31:03.390
Now the client can give us the thread
and you can configure it however you want

00:31:03.390 --> 00:31:08.710
with your own priorities or we will do
it on your behalf and we will set things

00:31:08.810 --> 00:31:13.450
up so that we give you the appropriate
priority as well for the type of latency

00:31:13.530 --> 00:31:17.770
that you're looking for in your I/O.

00:31:18.600 --> 00:31:24.800
[Transcript missing]

00:31:24.950 --> 00:31:28.990
Okay, so the code that's running
in this thread needs to,

00:31:29.080 --> 00:31:33.020
obviously, because it tends to be a
very high priority thread,

00:31:33.020 --> 00:31:35.190
in fact,
audio tends to be one of the highest

00:31:35.190 --> 00:31:40.890
priority threads in the system,
there's a high degree of,

00:31:40.950 --> 00:31:44.220
there's a high chance that you
will lock the system up if you take

00:31:44.260 --> 00:31:46.880
too much time in the I/O thread.

00:31:47.010 --> 00:31:49.130
Consequently,
there are a number of things

00:31:49.130 --> 00:31:50.780
you can do to alleviate that.

00:31:50.840 --> 00:31:52.360
You can just not take so long.

00:31:52.470 --> 00:31:55.370
That's a fine idea.

00:31:55.460 --> 00:31:58.690
Optimize your code.

00:31:58.900 --> 00:32:06.040
Perhaps the better strategy is if your
situation allows it to have a secondary

00:32:06.040 --> 00:32:10.200
thread available that runs at a lower
priority than the audio thread that

00:32:10.320 --> 00:32:14.440
you can use to generate or render your
audio or read it off the disk or get

00:32:14.440 --> 00:32:18.390
it from the network or do whatever
you need to do to get your data and

00:32:18.460 --> 00:32:21.800
process it and get it into a form that's
ready to be handed to the hardware.

00:32:36.230 --> 00:32:36.300
Perhaps the better strategy is if
your situation allows it to have a

00:32:36.300 --> 00:32:36.300
secondary thread available that runs
at a lower priority than the audio

00:32:36.300 --> 00:32:36.300
thread that you can use to get your
data and process it and get it from

00:32:36.300 --> 00:32:36.300
the network or do whatever you need
to do to get your data and process

00:32:36.300 --> 00:32:36.300
it and get it into a form that's
ready to be handed to the hardware.

00:32:36.380 --> 00:32:42.300
Now, in fact, if you do take up too much
time in your I/O thread,

00:32:42.300 --> 00:32:44.490
first we're going to tell you about it.

00:32:44.580 --> 00:32:49.140
We will send you a notification that
says your thread is taking too much time.

00:32:49.210 --> 00:32:52.230
And you will also hear
glitches in your stream,

00:32:52.230 --> 00:32:55.230
obviously,
because you're not generating the

00:32:55.330 --> 00:33:00.550
audio at the right time to be played
so that the stream can be continuous.

00:33:01.010 --> 00:33:06.080
The other sorts of issues
that you're going to see is...

00:33:06.810 --> 00:33:10.700
You can lock the system up,
but still not panic the system.

00:33:10.700 --> 00:33:15.500
The machine will just freeze,
and the only thing you

00:33:15.550 --> 00:33:16.700
can do is a cold reboot.

00:33:16.700 --> 00:33:23.870
Given that, the implementation of the
Audio Device API goes to great

00:33:23.930 --> 00:33:27.630
lengths to keep that from happening,
to the point where it won't reschedule

00:33:27.640 --> 00:33:32.660
your thread for you if it sees
that you're taking too much time.

00:33:32.700 --> 00:33:35.640
So if you eat too much processor time,
we're going to scale

00:33:35.640 --> 00:33:39.860
you back a little bit,
so that you're not starving

00:33:39.860 --> 00:33:42.600
the rest of the system.

00:33:50.280 --> 00:33:52.320
So what do you do in this thread?

00:33:52.320 --> 00:33:53.380
Right?

00:33:53.430 --> 00:33:56.000
And that's kind of an important question.

00:33:56.000 --> 00:34:02.050
So the I/O thread is basically scheduled
to wake up periodically in a way,

00:34:02.270 --> 00:34:05.760
and when it wakes up,
the idea is that you read your

00:34:05.760 --> 00:34:08.180
input and you write to the output.

00:34:08.200 --> 00:34:12.870
Now, the way we schedule the thread
to wake up is so that it kind of

00:34:12.900 --> 00:34:16.200
simulates a double buffering situation.

00:34:16.200 --> 00:34:20.900
That is, specifically, by default,
your thread will be scheduled to wake

00:34:21.050 --> 00:34:26.200
up about a buffer ahead of the buffer
you're supposed to render for output.

00:34:26.490 --> 00:34:30.610
This gives you roughly 100% of
CPU to do whatever it is you need

00:34:30.650 --> 00:34:35.070
to do and still be able to deal,
to deliver the audio to the hardware on

00:34:35.120 --> 00:34:37.980
time so that you have glitch-free audio.

00:34:39.520 --> 00:34:44.320
Like I said, the input and output are
presented to you synchronously.

00:34:44.460 --> 00:34:48.830
So when your I/O routine gets called,
you get both the input

00:34:48.830 --> 00:34:50.370
data and the output data.

00:34:50.820 --> 00:34:55.140
Further, you get timestamps that talk
about when that data was acquired

00:34:55.140 --> 00:34:59.400
or when that data is going to
be inserted in the output stream.

00:34:59.500 --> 00:35:04.260
And you get a third timestamp that tells
you what the current time is as well,

00:35:04.500 --> 00:35:07.730
so you don't have to ask
and can potentially save a

00:35:07.780 --> 00:35:10.680
little time in the I/O thread.

00:35:11.130 --> 00:35:17.280
Now, the buffer size that you're using is
completely configurable by the client.

00:35:17.280 --> 00:35:20.520
We can do this because we're just
writing into one shared ring buffer.

00:35:20.610 --> 00:35:24.240
We just know that we need to
write that data at a certain space

00:35:24.240 --> 00:35:29.000
ahead of the DMA read head so that
we keep the stream continuous.

00:35:29.170 --> 00:35:32.320
The buffer size that you
use is entirely up to you.

00:35:32.320 --> 00:35:35.230
You can make it as big
or as small as you want.

00:35:35.230 --> 00:35:39.500
Obviously, you have trade-offs with
overhead in those terms.

00:35:39.640 --> 00:35:42.910
The smaller the buffer size,
the more frequently we

00:35:42.920 --> 00:35:45.000
need your thread to run.

00:35:45.000 --> 00:35:50.240
Also, the more effect the jitter in the
thread wakeup is going to affect you.

00:35:50.490 --> 00:35:57.340
For instance, if you want to render at,
say, 64 sample buffers,

00:35:57.390 --> 00:36:03.610
that's roughly three or
four milliseconds of data.

00:36:03.920 --> 00:36:08.640
Now, if the thread that you're using
to render that data has a jitter

00:36:09.100 --> 00:36:13.650
of some number of microseconds,
obviously the smaller your buffer is,

00:36:13.650 --> 00:36:16.620
the more that jitter number
is going to matter to you.

00:36:19.120 --> 00:36:22.430
So another thing that's
configurable about this whole

00:36:22.860 --> 00:36:24.600
process is the wake-up time.

00:36:24.600 --> 00:36:28.980
So you can say how much time in
advance you want the thread to wake up.

00:36:28.990 --> 00:36:34.760
You can make it as close to the
actual delivery time as you want,

00:36:34.940 --> 00:36:39.460
provided you know,
have a good idea about how much time

00:36:39.460 --> 00:36:41.560
about you take to deliver your data.

00:36:41.690 --> 00:36:44.710
The general rule,
there are a lot of interesting

00:36:44.710 --> 00:36:46.430
applications for that.

00:36:46.440 --> 00:36:50.080
One reason you might want to go as
close to the delivery point as possible

00:36:50.080 --> 00:36:55.230
is to be as responsive as possible
to interactive events like MIDI keys or

00:36:55.230 --> 00:37:03.620
user interface events or user experience,
user interface devices and whatnot.

00:37:03.640 --> 00:37:07.590
So given that those parameters
that you've told us how big

00:37:07.590 --> 00:37:11.480
your buffer is and how far in
advance you want us to wake up,

00:37:11.480 --> 00:37:11.480
we're going to have to
wait a little bit longer.

00:37:13.480 --> 00:37:18.470
So the actual wake-up time that we
schedule the thread to wake up for

00:37:18.550 --> 00:37:23.440
is then calculated using the previous
timestamps and the relationship

00:37:23.480 --> 00:37:27.830
between the number of samples played
and how much host time has passed

00:37:28.260 --> 00:37:33.420
in order to generate the appropriate
time to set to wake the thread up at.

00:37:35.100 --> 00:38:36.300
[Transcript missing]

00:38:38.780 --> 00:38:41.860
So,
I thought I'd finish this up by showing

00:38:42.050 --> 00:38:48.190
you exactly how easy it is to write
a real live client with this stuff.

00:38:48.380 --> 00:38:52.490
This code was adapted specifically
from the SDEV component that's used in

00:38:52.500 --> 00:38:55.570
the Sound Manager to talk to this API.

00:38:55.680 --> 00:38:59.600
So, first thing you got to do is you
got to find a device to talk to.

00:38:59.720 --> 00:39:04.360
So, to do that,
you use a property of the entire system,

00:39:04.370 --> 00:39:07.500
which is a little different than
a property for a specific device.

00:39:07.500 --> 00:39:10.470
You can tell system routines
versus device routines by

00:39:10.780 --> 00:39:11.890
the name of the routine.

00:39:12.180 --> 00:39:14.340
System routines start audio hardware.

00:39:14.380 --> 00:39:17.040
Device routines start audio device.

00:39:17.100 --> 00:39:19.820
So, the first property you're
interested in is trying to find

00:39:20.120 --> 00:39:21.620
out the default output device.

00:39:21.790 --> 00:39:25.920
So, you call, get the system property for
the default output device.

00:39:25.960 --> 00:39:27.590
Pretty simple.

00:39:28.140 --> 00:39:32.800
Then you need to figure out what kind
of data you need to send to this device.

00:39:32.800 --> 00:39:39.810
So to do that, you get the devices
stream format property.

00:39:40.000 --> 00:39:46.160
Now the stream format in this API is
encompassed by the AudioStream

00:39:46.160 --> 00:39:49.000
basic description struct.

00:39:49.000 --> 00:39:55.250
It contains enough information
to describe any constant

00:39:55.250 --> 00:39:59.990
bitrate format where all the
channels are the same width.

00:40:00.230 --> 00:40:03.420
This applies to most of the general
compression techniques that you

00:40:03.420 --> 00:40:11.500
see in the Sound Manager today,
like IMA, Linear PCM, MuLaw, A-Law,

00:40:11.500 --> 00:40:12.920
all that stuff.

00:40:13.000 --> 00:40:18.000
Now the struct will supply
you with the sample rate,

00:40:18.110 --> 00:40:22.480
the number of bytes in a frame,
the number of bytes in the channel,

00:40:23.010 --> 00:40:28.830
and the number of bytes in a packet if
the format has another grouping above the

00:40:28.830 --> 00:40:32.990
sample frame and the channel structures.

00:40:33.390 --> 00:40:39.580
More complicated formats obviously have
more information to talk about than just

00:40:39.580 --> 00:40:42.000
how big their individual fields are.

00:40:42.000 --> 00:40:44.960
For variable bitrate data,
you need to know where do the

00:40:44.970 --> 00:40:47.000
frames actually start in the stream.

00:40:47.000 --> 00:40:53.600
More complicated formats also provide
an extended description which is format

00:40:53.600 --> 00:40:58.000
specific and is defined by that format.

00:40:58.060 --> 00:41:04.000
That's also available via
another property on the device.

00:41:04.970 --> 00:41:07.850
In order to do I/O,
you also need to know how

00:41:07.850 --> 00:41:09.700
big your I/O buffer is.

00:41:09.700 --> 00:41:12.680
In this case,
we don't really care what the I/O buffer

00:41:12.740 --> 00:41:14.860
is because this is just a simple client.

00:41:14.970 --> 00:41:19.850
He's just going to take whatever the
default buffer size is for this device.

00:41:19.950 --> 00:41:23.960
Again, you just call audio device
get property to do that.

00:41:24.910 --> 00:41:29.620
And one thing I should mention that
sizes in this API are almost always

00:41:30.400 --> 00:41:32.800
passed around in terms of bytes.

00:41:32.890 --> 00:41:36.020
And you can calculate
the number of frames,

00:41:36.020 --> 00:41:38.800
if you can calculate the number
of frames for that format,

00:41:38.860 --> 00:41:42.430
by getting the stream format
and doing the appropriate

00:41:42.430 --> 00:41:47.730
math using the description in
the stream format descriptor.

00:41:47.800 --> 00:41:50.970
So to start playback,
you need to tell the device

00:41:50.970 --> 00:41:52.610
about your I/O routine.

00:41:52.800 --> 00:41:56.800
So you install it by calling
audio device at I/O proc.

00:41:56.970 --> 00:41:59.140
Now you also are given a
place to pass in a pointer to

00:41:59.220 --> 00:42:02.790
whatever kind of data you want,
passed back to your I/O routine,

00:42:02.800 --> 00:42:05.450
so you can, you know,
that's really useful for keeping

00:42:05.710 --> 00:42:08.120
track of context on multiple,
well,

00:42:08.210 --> 00:42:11.800
you all know how to use those things,
been around forever.

00:42:13.750 --> 00:42:17.030
So, and then you start the, you see,
then you just start the

00:42:17.030 --> 00:42:18.400
device by starting it.

00:42:18.400 --> 00:42:20.860
There's a routine to start it.

00:42:21.660 --> 00:42:25.590
And stopping a device is pretty
much the same but in reverse.

00:42:25.900 --> 00:42:30.500
You call audio device stop,
and then if you're done with I/O,

00:42:30.500 --> 00:42:33.500
you can remove the I/O proc as well.

00:42:33.650 --> 00:42:37.040
Now, one thing I should point out that
you'll notice that with start

00:42:37.290 --> 00:42:40.420
and stop that you also need to
pass in the I/O routine again.

00:42:40.530 --> 00:42:43.390
Now,
the reason why is that you can install

00:42:43.550 --> 00:42:46.500
multiple I/O routines on a given device.

00:42:46.500 --> 00:42:49.520
I'm sure that there are a
lot of reasons to do that,

00:42:49.520 --> 00:42:52.400
but it's just useful
for a number of things.

00:42:52.910 --> 00:43:00.200
So here's the prototype
for the I/O routine.

00:43:00.200 --> 00:43:02.410
When it's called,
you get the ID of the device

00:43:02.640 --> 00:43:04.540
that the I/O is happening on.

00:43:04.540 --> 00:43:06.510
You get a timestamp that represents now.

00:43:06.870 --> 00:43:10.250
Like I said,
timestamps represent the mapping between

00:43:10.250 --> 00:43:12.630
the sample time and the host clock.

00:43:12.870 --> 00:43:19.560
You also get a pointer to the input
data and a timestamp for when the first

00:43:20.020 --> 00:43:22.700
frame of that input data was acquired.

00:43:22.850 --> 00:43:26.770
And then you get a pointer to the output
buffer and a timestamp for when the

00:43:26.770 --> 00:43:31.520
first frame of the output buffer is going
to be inserted into the output stream.

00:43:31.700 --> 00:43:35.730
And then you get back your
client data pointer as well.

00:43:36.500 --> 00:43:40.300
And here's the entire
implementation of the I/O routine.

00:43:40.500 --> 00:43:47.920
In my case, I'm using a My Nifty file
object in my client data field

00:43:48.110 --> 00:43:51.430
so that I can get my data back,
so I can get some data to play.

00:43:51.510 --> 00:43:56.430
And I cast that back, and then I use it,
and I put that data right

00:43:56.500 --> 00:43:58.220
in the output buffer.

00:43:58.380 --> 00:44:03.500
And then if I find out that I'm done,
I can just turn off that I/O routine.

00:44:03.680 --> 00:44:08.210
And the semantics there is that
when you turn off an I/O routine

00:44:08.210 --> 00:44:12.500
from during an I/O process,
that current I/O will complete,

00:44:12.500 --> 00:44:16.490
and then no more I/O for
that routine will happen.

00:44:18.440 --> 00:44:21.340
So,
when are we going to give this to you?

00:44:21.460 --> 00:44:24.420
So, like I said,
the Sound Manager is in DP4 now.

00:44:24.700 --> 00:44:27.330
It's been there since DP2.

00:44:27.530 --> 00:44:30.900
The Audio Device API is also in DP4.

00:44:31.680 --> 00:44:36.270
The IPC mechanism is going to be,
we're going to start seeding that prior

00:44:36.340 --> 00:44:41.600
to the public beta and we'll hopefully
have it GMed for the public beta.

00:44:41.710 --> 00:44:43.750
And then with the audio
units architecture,

00:44:43.810 --> 00:44:46.000
we're looking at this fall for releasing.

00:44:46.100 --> 00:44:48.810
We don't really have
anything specific there.

00:44:49.330 --> 00:44:52.580
So next,
I'm just going to show you that it all

00:44:52.580 --> 00:44:55.380
actually works and it's really alive.

00:44:55.520 --> 00:44:57.350
First, I want to point out that the
music that you heard when you

00:44:57.360 --> 00:45:01.920
came in was coming live off my
PowerBook using QuickTime Player on

00:45:01.920 --> 00:45:05.580
OS X on top of the Audio Device API.

00:45:25.280 --> 00:45:28.240
I think you can hear me.

00:45:28.240 --> 00:45:34.370
So first up, I want to say in DP4,
by default, the Sound Manager is not set

00:45:34.370 --> 00:45:37.240
up to use the Audio Device API.

00:45:37.240 --> 00:45:42.140
You can add a magic
cookie to the framework,

00:45:42.140 --> 00:45:47.080
and it's documented
on DP4 how to do that,

00:45:47.080 --> 00:45:47.080
to make that actually happen.

00:45:49.170 --> 00:45:51.980
But other than that,
there's one reason why all the demos

00:45:51.980 --> 00:45:57.030
you've seen previous to this haven't been
running through the Audio Device API.

00:46:17.860 --> 00:46:17.860
So first up,
I'd like to show just a reasonably

00:46:17.860 --> 00:46:17.860
high frame rate QuickTime movie.

00:46:17.860 --> 00:46:17.860
And the thing to watch for in
this one is synchronization.

00:46:17.860 --> 00:46:17.860
The synchronization
is still pretty solid.

00:46:17.860 --> 00:46:17.860
It's not 100% perfect yet,
but it's pretty good already.

00:46:28.550 --> 00:46:29.490
"Gotta like that.

00:46:29.580 --> 00:46:35.440
Alright.

00:46:35.480 --> 00:46:36.450
And how about that?

00:46:36.530 --> 00:46:39.380
The sound stopped right
when the movie did.

00:46:39.380 --> 00:46:41.590
Thanks, Mark.

00:46:41.590 --> 00:46:43.740
Right in sync.

00:46:43.850 --> 00:46:45.450
Let's try that again."

00:46:48.020 --> 00:46:49.000
Don't you love it?

00:46:49.000 --> 00:46:50.000
You don't have to reboot.

00:46:51.120 --> 00:46:52.550
Hey, and you don't have to reboot.

00:46:52.550 --> 00:46:53.000
How about that?

00:46:54.140 --> 00:46:58.290
Oh, another interesting thing about
the Audio Device API is you

00:46:58.290 --> 00:47:01.560
don't have to reboot to reinstall
a new version of it either.

00:47:01.560 --> 00:47:05.590
As long as you're not playing sound,
you can just put in a new version and go.

00:47:05.730 --> 00:47:06.880
That's kind of neat.

00:47:07.000 --> 00:47:08.810
It cuts down on the development time.

00:47:09.740 --> 00:47:13.590
So, let's, you know,
once more from the top.

00:47:36.300 --> 00:47:38.950
delivers 10 times out of 10.

00:47:38.960 --> 00:47:40.480
Who's the cat that won't cop out?

00:47:40.500 --> 00:47:44.200
Shea Ray.

00:47:44.200 --> 00:47:46.920
They say I'm a complicated man.

00:47:48.200 --> 00:47:52.200
I might take you down,
but I'll never let you down.

00:47:52.200 --> 00:47:55.200
Who's the man who'd risk his
neck for his brother man?

00:47:55.200 --> 00:48:00.100
Now, what's my name?

00:48:20.800 --> 00:48:30.300
[Transcript missing]

00:48:32.150 --> 00:48:35.490
Another thing I wanted to show you a
little bit was some of the benefits

00:48:35.570 --> 00:48:37.230
of variable bitrate encoding.

00:48:37.340 --> 00:48:43.650
You see some examples of some MP3 files
I've encoded using different kinds

00:48:43.650 --> 00:48:46.000
of different data rates and whatnot.

00:48:46.000 --> 00:48:49.850
First, I'd like to play the original.

00:48:50.900 --> 00:48:56.130
I'm going to give you a feel for
what this sounds like before I give

00:48:56.250 --> 00:49:01.450
you all the compressed versions.

00:49:26.700 --> 00:49:28.800
Okay, of particular note,
you should hear the cymbal

00:49:28.800 --> 00:49:30.090
sounds and what they sound like.

00:49:30.180 --> 00:49:32.850
Kind of remember that
and try to figure out,

00:49:33.090 --> 00:49:35.020
you know, which of these sound the best.

00:49:35.100 --> 00:49:37.290
First,
let's start with the high data rate

00:49:37.290 --> 00:49:39.460
version since those are the easiest.

00:49:39.460 --> 00:49:42.820
Obviously,
they're going to sound relatively good.

00:49:42.820 --> 00:49:45.040
And the relative difference
between variable bit rate and

00:49:45.040 --> 00:49:49.860
constant bit rate starts to go
away when you use larger bit rates.

00:49:49.860 --> 00:49:50.550
But they're still there.

00:49:50.550 --> 00:49:53.740
I mean, if nothing else,
you get smaller files.

00:49:53.740 --> 00:49:57.070
So here's the variable bit rate,
or here's the 128K

00:49:57.070 --> 00:49:58.460
constant bit rate kind.

00:49:58.460 --> 00:50:01.510
This is typically the format
you find on the Internet.

00:50:34.000 --> 00:50:36.500
It sounds pretty close to the original.

00:50:36.500 --> 00:50:38.850
And here's the variable bit,
the high-quality version

00:50:38.900 --> 00:50:40.780
for this encoder of--

00:50:43.600 --> 00:50:50.060
As you can see, the file size is a bit
smaller than the 128K size.

00:51:24.500 --> 00:51:28.340
And again, you can hear it's still pretty
close to the original mix.

00:51:28.340 --> 00:51:30.410
And in this case,
the savings are obvious.

00:51:30.410 --> 00:51:33.260
Variable bitrate wins because
it's a smaller file size.

00:51:33.730 --> 00:51:37.770
Just kind of an interesting aside,
the normal encoded version,

00:51:37.930 --> 00:51:46.710
at least for this encoder,
wait for the variable bitrate to parse.

00:52:22.500 --> 00:52:40.300
[Transcript missing]

00:52:53.800 --> 00:52:59.810
This almost doesn't sound like a hi-hat.

00:53:13.600 --> 00:53:20.420
Let's take a look at the variable bitrate
version in the lowest quality setting

00:53:20.430 --> 00:53:23.760
that squeezes the most bits out of it.

00:53:23.810 --> 00:53:28.470
In this case, again,
you see the file size is roughly

00:53:28.470 --> 00:53:31.710
the same as the 64K version.

00:53:49.600 --> 00:53:53.240
It depends on the, uh,
the actual content.

00:53:53.240 --> 00:53:57.400
Hold that question.

00:54:06.740 --> 00:54:09.380
So you can see the
low-quality version is much,

00:54:09.390 --> 00:54:12.880
much better than the 64K version,
and you get the same file size.

00:54:12.890 --> 00:54:15.780
So, you know, use VBR, I guess.

00:54:20.800 --> 00:54:28.790
So to finish things up,
you need to contact Dan Brown to--

00:54:28.970 --> 00:54:30.640
We're going to finish
things up and participate,

00:54:30.690 --> 00:54:34.680
particularly if you want to
participate in the seeding program.

00:54:34.680 --> 00:54:41.900
We're going to be seeding the core
audio services a lot quicker than

00:54:41.900 --> 00:54:43.400
we've been running in the past.

00:54:43.400 --> 00:54:47.570
We're hoping to keep things moving,
get things out into the open,

00:54:47.570 --> 00:54:53.380
get you guys working with the stuff,
and working with us to make it

00:54:53.450 --> 00:54:54.880
better so that we can actually
meet your needs for a change.

00:54:55.300 --> 00:54:59.540
So contact Dan and he
can get that working.

00:54:59.630 --> 00:55:02.260
So next we're going to finish
things up with a little Q&A,

00:55:02.270 --> 00:55:05.500
but Bill has something to say first.