WEBVTT

00:00:11.420 --> 00:00:13.060
Good afternoon, everyone.

00:00:13.080 --> 00:00:19.900
Welcome to session 182, which is Java:
Getting the Best Performance.

00:00:19.900 --> 00:00:22.600
And here to begin our
presentation this afternoon,

00:00:22.600 --> 00:00:26.560
please welcome Jim Laskey.

00:00:30.840 --> 00:00:31.720
Good afternoon.

00:00:31.800 --> 00:00:33.800
We're going to talk a little
bit about performance.

00:00:33.800 --> 00:00:38.560
Yesterday we talked a little bit
about some of the new technology that

00:00:38.560 --> 00:00:42.400
was being introduced on Mac OS X,
in particular,

00:00:42.400 --> 00:00:47.540
Hotspot and some of Java 2.

00:00:47.540 --> 00:00:51.540
We're going to focus on different
aspects of these two technologies,

00:00:51.540 --> 00:00:53.940
but we're trying to
zero in on performance,

00:00:53.940 --> 00:00:57.040
and we're going to talk about
how we've made improvements

00:00:57.040 --> 00:01:00.040
to performance in the Java VM.

00:01:00.070 --> 00:01:05.160
We're going to give you some hints
or ideas about how you can modify

00:01:05.160 --> 00:01:07.850
your code to gain performance.

00:01:08.700 --> 00:01:11.330
We've broken this talk up into
three parts because we wanted to

00:01:11.450 --> 00:01:14.870
get three different categories
of things to talk about.

00:01:14.870 --> 00:01:18.660
The first part will be done
by Ivan Posva of the VM team,

00:01:18.670 --> 00:01:22.500
who's going to talk about the VM,
the new memory management,

00:01:22.700 --> 00:01:24.440
thread synchronization.

00:01:24.580 --> 00:01:28.930
Then I'll come back for part two
and talk about code execution and

00:01:28.930 --> 00:01:32.940
how we boosted performance of code.

00:01:33.040 --> 00:01:36.580
Finally,
John Berkey will come up and discuss how

00:01:36.640 --> 00:01:42.860
we can get some performance out of the
AWT and swing in the new Java classes.

00:01:42.860 --> 00:01:46.800
I guess I'll get Ivan to come up now.

00:01:53.570 --> 00:01:57.880
Well, thanks Jim for the introduction.

00:01:57.880 --> 00:02:02.500
So let's look at first what are
the factors of Java performance.

00:02:02.580 --> 00:02:05.900
First of all,
it's the design of your application

00:02:05.900 --> 00:02:08.500
that's the most important part of that.

00:02:08.500 --> 00:02:12.890
Second is the speed of the
bytecode execution inside the VM,

00:02:12.890 --> 00:02:17.500
the speed at which we execute
the bytecodes of your program.

00:02:17.500 --> 00:02:22.330
Next, and that's what I will focus about,
is speed of VM operations,

00:02:22.330 --> 00:02:27.630
be it class loading, garbage collection,
threading, synchronization,

00:02:27.630 --> 00:02:29.500
and its associated libraries.

00:02:29.500 --> 00:02:33.380
And last, it's the speed of the hardware
you're running on and the

00:02:33.380 --> 00:02:35.500
underlying operating system.

00:02:35.500 --> 00:02:39.910
That's not much we can do about it,
but we are pressing on the

00:02:39.910 --> 00:02:41.560
kernel team in that area.

00:02:43.700 --> 00:02:52.800
[Transcript missing]

00:02:53.210 --> 00:02:57.170
Automatic memory management
and language level support for

00:02:57.470 --> 00:02:59.860
threading and synchronization.

00:03:00.010 --> 00:03:04.990
Unfortunately,
these two points are also associated with

00:03:05.470 --> 00:03:10.240
having the most impact on performance,
most negative impact on performance.

00:03:10.340 --> 00:03:16.810
And we are here to clean up some of those
misconceptions and show you what you can

00:03:16.810 --> 00:03:20.810
do to give the VM hints in that area.

00:03:21.100 --> 00:04:39.900
[Transcript missing]

00:04:40.220 --> 00:04:58.040
: The less memory you use,
it can be a considerable gain.

00:04:58.040 --> 00:04:58.040
So I will -- the hotspot VM uses a
generational copying garbage collector,

00:04:58.040 --> 00:04:58.050
and on the next slide I will
dive into a bit more detail.

00:04:58.620 --> 00:05:00.770
: So, it is accurate.

00:05:00.780 --> 00:05:05.730
The garbage collector is accurate,
which means we know at all times

00:05:05.850 --> 00:05:09.240
in the executing Java program,
we know at all times where we

00:05:09.240 --> 00:05:12.040
have life references to objects.

00:05:12.100 --> 00:05:17.790
We are not conservative when
we walk through the stack,

00:05:17.790 --> 00:05:17.790
so we know

00:05:17.930 --> 00:05:32.490
: What words on the stack are
actual integers even though they

00:05:32.490 --> 00:05:32.490
look like object references?

00:05:32.490 --> 00:05:32.490
We can collect those objects.

00:05:32.490 --> 00:05:32.490
In contrast to the

00:05:33.630 --> 00:05:47.190
: Conservative collectors,
this can really make a big difference

00:05:47.190 --> 00:05:47.190
in memory usage in the sense that
you're not keeping live objects that

00:05:47.190 --> 00:05:47.190
are apparent objects but you actually

00:05:47.300 --> 00:05:55.620
: It is generational,
which means the majority of

00:05:55.690 --> 00:05:59.300
objects that you allocate in
Java when you actually study it,

00:05:59.490 --> 00:06:02.290
they are very short lived.

00:06:02.290 --> 00:06:05.380
So what we do,
we allocate all the objects

00:06:05.520 --> 00:06:10.290
in this new object heap,
what we call the nursery.

00:06:10.300 --> 00:06:16.200
And once we exhausted this nursery,
we copy the surviving objects out

00:06:16.250 --> 00:06:22.300
of there into an old space and can
start from scratch in that nursery.

00:06:22.660 --> 00:06:29.790
Which means we have a fast
allocation in this nursery.

00:06:29.790 --> 00:06:29.790
We don't have to deal with the garbage we

00:06:30.830 --> 00:06:35.340
We have in there,
we don't do anything with the garbage.

00:06:35.340 --> 00:06:39.500
We just copy out the objects
that survive from this new

00:06:39.590 --> 00:06:42.390
generation to the old generation.

00:06:42.620 --> 00:06:47.670
You could say that garbage collection
is actually the wrong term.

00:06:47.780 --> 00:06:52.680
It's more like a search and rescue
operation where you rescue the

00:06:52.680 --> 00:06:58.890
few survivors out of the new heap,
out of the nursery, into the old space.

00:06:59.900 --> 00:07:03.470
It is also a search where we,
due to the accurate nature,

00:07:03.540 --> 00:07:05.900
we know exactly who's surviving.

00:07:05.900 --> 00:07:08.900
So the search is pretty fast.

00:07:08.900 --> 00:07:12.330
I say, I mentioned copying.

00:07:12.330 --> 00:07:21.160
We actually move those objects out of
the nursery into separate memory area.

00:07:21.160 --> 00:07:21.160
Studies have shown that

00:07:21.370 --> 00:07:27.830
: The first thing I want to say is
that 5-10% of allocated objects survive

00:07:28.080 --> 00:07:34.450
from the new allocation space from
the nursery into the old generation.

00:07:34.550 --> 00:07:38.680
So if you say this nursery
is half a megabyte big,

00:07:38.680 --> 00:07:45.680
we copy 25-50 kilobytes worth of objects,
which is not very much.

00:07:46.500 --> 00:07:49.650
Also, since we have this copying
infrastructure already in place

00:07:50.070 --> 00:07:55.320
for this rather big object space,
we compact it regularly.

00:07:55.320 --> 00:07:59.090
Every time we run the
old space collector,

00:07:59.090 --> 00:08:08.470
we can compact the heap and keep
the memory usage to a minimum.

00:08:08.710 --> 00:08:17.550
: The old space collector also,
since it has to deal with much

00:08:17.550 --> 00:08:17.550
bigger memory allocations,
is incremental, so it's...

00:08:18.210 --> 00:08:22.410
It works on a chunk at a time,
every time it's invoked,

00:08:22.570 --> 00:08:27.560
so it reduces user perceivable
pauses so you don't have this

00:08:27.580 --> 00:08:32.400
big stop in the middle where
you're collecting the whole heap,

00:08:32.400 --> 00:08:39.230
but you have these many,
many small pauses which makes

00:08:39.300 --> 00:08:42.030
your UI applications or server
applications respond much faster.

00:08:43.140 --> 00:08:47.640
So the benefits to you as a
programmer are very fast allocation.

00:08:47.640 --> 00:08:52.220
Since we always allocate
out of this nursery,

00:08:52.230 --> 00:08:58.950
and we allocate in a stack-like fashion,
so all we do is increment the

00:08:58.950 --> 00:09:01.490
pointer and this is our new object.

00:09:01.570 --> 00:09:06.250
All we have to do after we increment
the pointer is check if we exhausted

00:09:06.250 --> 00:09:11.400
the nursery space and then we have
to trigger this new space collection.

00:09:11.400 --> 00:09:13.010
This...

00:09:13.370 --> 00:09:19.240
This allocation code is actually
inlined in the compiled code.

00:09:19.240 --> 00:09:24.140
All we have to do is to allocate a new
object when you have the new operation.

00:09:24.170 --> 00:09:27.280
This is equivalent to 11 instructions.

00:09:27.300 --> 00:09:31.160
When you compare that to, say,
a C-style malloc function call,

00:09:31.160 --> 00:09:34.480
you have to go through
cross-library function glue,

00:09:34.480 --> 00:09:38.290
and then you have to do
the C prolog to allocate.

00:09:38.320 --> 00:09:41.540
After those 11 instructions,
you're not even allocating

00:09:41.650 --> 00:09:45.470
objects in the C language yet,
versus in Java, you have your object

00:09:45.470 --> 00:09:46.960
and you're ready to go.

00:09:47.140 --> 00:09:51.030
As I mentioned before,
we have an accurate collector,

00:09:51.040 --> 00:09:56.160
so we do aggressive reclamation
of this nursery as well

00:09:56.160 --> 00:09:59.770
as of the old generations.

00:10:00.430 --> 00:10:04.800
So, for example,
you don't leave objects around

00:10:04.800 --> 00:10:07.300
that are not accessible anymore.

00:10:07.530 --> 00:10:11.810
And due to these two factors,
you have essentially

00:10:11.810 --> 00:10:14.290
free temporary objects.

00:10:14.300 --> 00:10:17.300
Temporary objects are by
definition short-lived,

00:10:17.300 --> 00:10:19.300
which we don't have to deal with.

00:10:19.300 --> 00:10:22.860
You have no overhead for
short-lived objects because all

00:10:22.860 --> 00:10:26.300
we have to do is basically copy
the few survivors out of there.

00:10:26.440 --> 00:10:32.400
So, I claim you have essentially
free temporary objects,

00:10:32.400 --> 00:10:34.290
especially since Java is multithreaded.

00:10:34.300 --> 00:10:38.790
If you have an allocation cache,
you would have to lock that

00:10:39.180 --> 00:10:41.300
allocation cache first.

00:10:41.300 --> 00:10:47.290
You would have to take the object out
of the allocation cache and then unlock.

00:10:47.340 --> 00:10:53.480
By that time, you already allocated the
object out of the nursery.

00:10:53.980 --> 00:10:55.890
What do you need to do?

00:10:55.970 --> 00:10:58.840
Well, as I mentioned,
you do not build allocation caches,

00:10:58.840 --> 00:11:03.490
but what you also have to do
is you have to tell the VM that

00:11:03.490 --> 00:11:06.760
you're done with an object.

00:11:07.040 --> 00:11:09.920
So what you have to do is,
if you have an object here

00:11:10.300 --> 00:11:13.050
that you don't need anymore,
or even worse,

00:11:13.050 --> 00:11:16.460
if you have an object hierarchy,
especially if you have

00:11:16.600 --> 00:11:19.900
it in a static field,
you have to set that value,

00:11:19.900 --> 00:11:24.430
that reference to null,
so we know this object hierarchy or

00:11:24.430 --> 00:11:29.350
this object is not reachable anymore,
so we don't copy it out

00:11:29.350 --> 00:11:32.170
into the old generation.

00:11:32.340 --> 00:11:34.310
Or even worse,
if it's in the old generation,

00:11:34.380 --> 00:11:36.700
we keep compacting it and so on.

00:11:36.700 --> 00:11:38.650
We keep that memory alive.

00:11:38.660 --> 00:11:41.490
So what you have to make sure
that you null out your objects

00:11:41.530 --> 00:11:43.220
that you're not using anymore.

00:11:43.220 --> 00:11:47.910
And while I mentioned allocation caches,
what you also have to do

00:11:47.910 --> 00:11:52.970
is if you have native code,
be it stub-based libraries

00:11:52.970 --> 00:11:59.800
based on the old native call
conventions from Java 1.0,

00:11:59.800 --> 00:12:03.140
JDK 1.0.2, or Java 1.1.

00:12:03.160 --> 00:12:11.650
Or if you have in your project,
if you have JDIRECT2 code, you have to...

00:12:12.010 --> 00:12:17.400
- Basically,
convert those projects to use JDIRECT3.

00:12:17.400 --> 00:12:20.900
There is a talk tomorrow
about Mac OS X Java in depth,

00:12:20.940 --> 00:12:24.900
and that talks about how to
convert projects to JDIRECT3.

00:12:24.900 --> 00:12:32.900
Or you have to convert your native
stub libraries to use JNI native calls.

00:12:32.900 --> 00:12:38.200
And if you're using the
Objective-C to Java bridge for

00:12:38.500 --> 00:12:43.040
: When you're wrapping
Objective-C frameworks to Java,

00:12:43.070 --> 00:12:47.100
then you have to make sure that
you recompile your wrapper projects

00:12:47.100 --> 00:12:52.510
with the new JNI-based bridge
that comes with DP4 on the CD.

00:12:54.340 --> 00:12:58.260
Before I go into synchronization,
I wanted to mention that

00:12:58.370 --> 00:13:03.970
Java threads are one-to-one mapped
to P threads and therefore are also

00:13:03.970 --> 00:13:07.190
one-to-one mapped to kernel threads.

00:13:07.580 --> 00:13:11.730
They are fully preemptive.

00:13:13.120 --> 00:13:20.100
which means we inside the VM or you,
we do not have to deal with scheduling.

00:13:20.100 --> 00:13:23.290
The kernel does that for us.

00:13:23.340 --> 00:13:26.250
We are multi-processing
ready as you've seen in the

00:13:26.250 --> 00:13:28.580
hardware keynote this morning.

00:13:28.580 --> 00:13:28.580
If

00:13:29.070 --> 00:13:33.360
If we have something that the
kernel can use to schedule

00:13:33.360 --> 00:13:38.990
more threads onto a second CPU,
we in the VM will make use of that.

00:13:39.020 --> 00:13:44.000
Therefore, your applications will run
faster naturally as well.

00:13:44.000 --> 00:13:51.130
We integrated native and Java stacks
for invocation stacks into one memory

00:13:51.130 --> 00:13:55.970
area so you have better locality
of references as VM internals.

00:13:56.000 --> 00:14:00.330
This is not something you will notice,
but this is one of the reasons

00:14:00.330 --> 00:14:02.990
why you have to go to JNI,
for example,

00:14:03.000 --> 00:14:08.420
as well as the Accurate Garbage Collector
makes it necessary to go to JNI.

00:14:08.670 --> 00:14:12.600
So, my last slide is on synchronization.

00:14:12.640 --> 00:14:17.990
First I wanted to explain,
when we talk about synchronization

00:14:17.990 --> 00:14:19.700
what is a contended case.

00:14:19.720 --> 00:14:26.360
A contended case is when you're executing
in one thread a synchronized block,

00:14:26.370 --> 00:14:31.040
be it a synchronized method or,
if you do execute a synchronized

00:14:31.040 --> 00:14:35.900
statement on a particular object,
you're, inside that call,

00:14:35.900 --> 00:14:39.070
inside that block,
and a different thread comes in

00:14:39.070 --> 00:14:44.800
and tries to do a synchronized
block on the same object.

00:14:44.850 --> 00:14:48.550
So, at that time,
it is called that you have

00:14:48.560 --> 00:14:52.110
contention on that object,
and

00:14:52.490 --> 00:14:57.630
The uncontended case, therefore,
is if you have one thread go in,

00:14:57.680 --> 00:15:02.230
synchronize in a particular object,
execute the whole block,

00:15:02.260 --> 00:15:05.980
and exit the synchronized block
without any other thread trying

00:15:06.010 --> 00:15:08.140
to synchronize in the same object.

00:15:08.410 --> 00:15:11.740
The studies have shown that the
contended case is very rare.

00:15:11.750 --> 00:15:17.490
In those instances we use Pthread
primitives and kernel primitives to

00:15:17.520 --> 00:15:23.070
make sure that we do the right thing
of blocking the thread so it doesn't

00:15:23.070 --> 00:15:25.300
use any CPU cycles from then on.

00:15:25.300 --> 00:15:29.590
But what is much more important is
that we have very fast synchronization

00:15:29.590 --> 00:15:31.300
in the uncontended case.

00:15:31.560 --> 00:15:34.260
What you have basically is
a constant time overhead.

00:15:34.290 --> 00:15:38.070
You have a couple of instructions
to make sure that you set

00:15:38.070 --> 00:15:40.300
up that synchronized block.

00:15:40.300 --> 00:15:43.290
We do not allocate memory
on the heap or anything.

00:15:43.350 --> 00:15:49.870
It's all stack allocated for that memory,
basically for that synchronized

00:15:49.870 --> 00:15:56.100
block we allocated inside
the invocation of that stack.

00:15:57.420 --> 00:16:00.800
Most important of all,
we don't use any OS resources,

00:16:00.800 --> 00:16:03.930
so we don't use any kernel resources,
we don't make any pthread calls,

00:16:03.990 --> 00:16:08.740
we don't make any kernel calls,
which is important since we want

00:16:08.840 --> 00:16:11.300
this constant time overhead.

00:16:11.300 --> 00:16:17.520
So, I will hand the podium back to Jim to
talk about Java code execution.

00:16:24.400 --> 00:16:30.160
I want to try to focus on basically
what we've done to improve

00:16:30.170 --> 00:16:33.560
performance in code generation.

00:16:33.560 --> 00:16:34.320
But I want to give you some background.

00:16:34.430 --> 00:16:36.230
So we're going to go into a
little bit of history first and

00:16:36.230 --> 00:16:37.390
then talk about optimization.

00:16:37.400 --> 00:16:41.010
And then at the end I have
ten slides which are what I'm

00:16:41.080 --> 00:16:44.690
calling code generator hints,
which are things that you can

00:16:44.690 --> 00:16:48.690
do in your code which the code
generator can look at and say,

00:16:48.720 --> 00:16:50.400
"Oh,
this means I can do this optimization."

00:16:50.400 --> 00:16:54.470
And hopefully you can pick up a few of
those things and put it in your own code.

00:16:55.700 --> 00:18:52.000
[Transcript missing]

00:18:52.300 --> 00:19:08.190
: Now interpretation can carry us so far,
but it's really not the end game.

00:19:08.190 --> 00:19:08.190
And what we need to be able to do at
that point is to get into some kind

00:19:08.190 --> 00:19:08.190
of native code generation so that
we get down to the point where we're

00:19:08.190 --> 00:19:08.190
actually running at the same sorts
of speed that you would see in C++.

00:19:08.360 --> 00:19:12.540
So we get into code generators and
we had the first JITs that came out

00:19:12.540 --> 00:19:19.150
use the Java lane compiler interface
as a plug-in to the classic VM.

00:19:19.190 --> 00:19:22.900
And what they did was intercept
the execution of methods and go

00:19:23.030 --> 00:19:27.300
through and convert the bytecodes
into native machine code.

00:19:27.300 --> 00:19:31.640
And a lot of the earlier JITs
that came out basically all they

00:19:31.640 --> 00:19:36.370
really did was take the bytecode
sequence and convert them one to

00:19:36.380 --> 00:19:39.300
one into sequences of native code.

00:19:39.300 --> 00:19:42.280
That worked and again you got
another boost of performance.

00:19:42.290 --> 00:19:45.300
You probably got about a five
times boost in performance.

00:19:45.300 --> 00:19:49.300
But the problem is it wasn't
really utilizing the true

00:19:49.300 --> 00:19:50.300
performance of the machine.

00:19:50.300 --> 00:19:52.290
It wasn't scheduling the instructions.

00:19:52.290 --> 00:19:55.380
It wasn't looking at the sequences
of instructions and whether you

00:19:55.380 --> 00:19:57.300
could get any kind of optimization.

00:19:58.560 --> 00:20:01.740
: So then we got a round of static
code generators where people

00:20:01.740 --> 00:20:06.230
would take the back end off of a
C compiler and put a Java front end

00:20:06.250 --> 00:20:09.490
in and produce static executed code.

00:20:09.500 --> 00:20:13.500
And that sort of works for
some types of applications,

00:20:13.500 --> 00:20:17.630
but the problem with that is
that Java is a very rich and

00:20:17.630 --> 00:20:21.920
dynamic environment to work in,
and static applications don't really fit

00:20:21.920 --> 00:20:24.500
into what the spirit of Java is about.

00:20:24.500 --> 00:20:30.500
You need that dynamic
environment to run in.

00:20:31.670 --> 00:20:34.650
So then we get into the high
performance JITs and you're

00:20:34.880 --> 00:20:40.350
familiar with say the semantic
JIT that we've been using in MRJ.

00:20:40.400 --> 00:20:45.810
There's a couple of other high
performance JITs like the C2 or the

00:20:45.810 --> 00:20:48.980
server compiler that's on hotspot.

00:20:49.040 --> 00:20:55.320
And what these JITs would do is basically
optimize the heck out of the bytecode

00:20:55.320 --> 00:20:59.370
and try to reduce it down to something
that would be quite close to what you

00:20:59.370 --> 00:21:01.440
would expect from a C and a C++ compiler.

00:21:03.530 --> 00:21:06.400
The problem with these
high performance JITs,

00:21:06.400 --> 00:21:10.490
well the positive part is that we're
getting the really good performance,

00:21:10.490 --> 00:21:12.060
really, really good performance.

00:21:12.160 --> 00:21:15.170
But the problem with these was
that there was a lot of competition

00:21:15.300 --> 00:21:18.730
between the JIT companies and they
were trying to squeeze out as much

00:21:18.730 --> 00:21:23.010
performance as they possibly could,
try to get the best caffeine mark

00:21:23.120 --> 00:21:28.200
they could working with the semantic
JIT on the semantic JIT team.

00:21:28.210 --> 00:21:33.070
We were getting infinite scores and
caffeine marks because we were optimizing

00:21:33.070 --> 00:21:38.210
methods right down to the point where
they were just simple return statements.

00:21:38.710 --> 00:22:02.890
: But the problem is,
that's the positive side,

00:22:02.890 --> 00:22:02.890
the negative side is that it was
taking more and more time to compile

00:22:02.890 --> 00:22:02.890
these things and more and more memory.

00:22:02.890 --> 00:22:02.890
And that's where the cost was.

00:22:02.890 --> 00:22:02.890
So we have to find some kind of balance
between getting the optimization done

00:22:02.890 --> 00:22:02.890
and keeping the compile time and the
memory requirements down to a minimum.

00:22:02.890 --> 00:22:02.890
And that's where we are
with hotspot client version.

00:22:04.520 --> 00:22:07.850
The traditional type of
optimizations you would expect

00:22:07.850 --> 00:22:11.380
would be expression reduction,
CSEs,

00:22:11.730 --> 00:22:16.860
loop unfolding or loop optimizations,
data flow analysis.

00:22:16.940 --> 00:22:20.630
These are all the standard sort of
things you would see in the Dragon book,

00:22:20.690 --> 00:22:22.400
I guess the standard compiler book.

00:22:22.400 --> 00:22:27.720
And typically what would happen would
be that you would go through a round of

00:22:27.720 --> 00:22:33.370
these optimizations and then they would
reduce the application down to a little

00:22:33.370 --> 00:22:37.330
bit smaller and then you would have to
go back and repeat them again because

00:22:37.340 --> 00:22:38.400
then it would bring it down again.

00:22:38.400 --> 00:22:41.400
So we have a Heuristic type algorithm
to actually reduce the optimization.

00:22:41.400 --> 00:22:45.400
And this is really where a lot of these
high end optimizers got into trouble

00:22:45.400 --> 00:22:48.070
was because they would loop and loop and
loop and they could loop several hundred

00:22:48.070 --> 00:22:50.460
times before they give up and say,
well this is the best

00:22:50.460 --> 00:22:51.400
I can do with this method.

00:22:51.400 --> 00:22:52.400
And then actually actually
get the best performance.

00:22:52.400 --> 00:22:52.740
Executed.

00:22:57.830 --> 00:23:03.500
One of the great things about the
Just-In-Time compiler is that you can

00:23:03.500 --> 00:23:07.790
do optimizations that you wouldn't
be able to do in a static situation.

00:23:07.800 --> 00:23:11.990
One of the most important ones is
to be able to determine whether a

00:23:11.990 --> 00:23:14.790
virtual method is monomorphic or not.

00:23:14.800 --> 00:23:17.800
What this means,
what monomorphism is about,

00:23:17.800 --> 00:23:24.800
is that we have in Java the ability to
create subclasses of a particular method,

00:23:24.800 --> 00:23:27.800
and then the ability to be
able to override those methods.

00:23:27.800 --> 00:23:31.870
So in order to call a particular
method with a particular object,

00:23:31.870 --> 00:23:34.960
there's a dispatch that goes
on that says which method is

00:23:34.960 --> 00:23:36.800
associated with this object.

00:23:36.800 --> 00:23:41.760
So there's basically virtual objects,
a virtual dispatch.

00:23:42.110 --> 00:23:44.600
But most of the time,
and it turns out that

00:23:44.600 --> 00:23:47.610
in most applications,
a little bit over 80% of the

00:23:47.760 --> 00:23:51.820
classes that you have in your
environment are not overridden.

00:23:51.840 --> 00:23:54.640
They're leaf classes.

00:23:54.720 --> 00:23:57.500
So there's really no need to go
through this dispatch mechanism.

00:23:57.520 --> 00:24:02.040
You can make a direct call to that
method and not worry about having to,

00:24:02.060 --> 00:24:04.900
you know, hitting the wrong method.

00:24:05.010 --> 00:24:07.820
So the just-in-time compilers
try to exploit this,

00:24:07.820 --> 00:24:10.620
and one of the things that
you can do besides just,

00:24:10.620 --> 00:24:13.000
you know,
simplifying the actual call to the

00:24:13.000 --> 00:24:18.090
method is inline the code for the
method that you're dispatching to.

00:24:18.410 --> 00:24:21.030
If you take a look at some of
the examples of your own code,

00:24:21.030 --> 00:24:24.680
you'll probably find lots of places
where you're calling this method.

00:24:24.750 --> 00:24:27.230
It's rather simple,
maybe a few lines of code.

00:24:27.300 --> 00:24:30.150
A good example of those
would be the accessors,

00:24:30.150 --> 00:24:32.730
the getters and setters for your class.

00:24:33.000 --> 00:24:40.200
Isn't it a shame I have to go
off and call this method because

00:24:40.200 --> 00:24:40.820
this code could fit in right in
line and be very inexpensive?

00:24:40.910 --> 00:24:42.440
This is what the JIT does for you.

00:24:42.440 --> 00:24:45.340
It goes off and says, "Oh,
that's a really simple method.

00:24:45.380 --> 00:24:47.300
I can determine that it's monomorphic.

00:24:47.390 --> 00:24:50.240
There's nobody overriding this,
so I'm just going to inline

00:24:50.240 --> 00:24:53.470
this code in." And the code
becomes very inexpensive.

00:24:54.870 --> 00:24:59.310
Now, there's always the possibility
that that method may get

00:24:59.310 --> 00:25:03.030
overloaded or not overloaded but
overridden at some other time.

00:25:03.030 --> 00:25:08.030
So what will happen in the
Just-In-Time compiler environment

00:25:08.280 --> 00:25:11.800
is that it may create several
flavors of the same method.

00:25:11.800 --> 00:25:14.200
So you may have several different
versions of that method,

00:25:14.200 --> 00:25:19.240
one where maybe a call is overloaded
or overridden and another case

00:25:19.240 --> 00:25:21.560
where it isn't overridden.

00:25:21.570 --> 00:25:25.980
So what happens is that the method that
needs to be executed will be chosen at

00:25:25.980 --> 00:25:31.470
runtime and will go off and do that code
or whichever one suits the situation.

00:25:33.670 --> 00:25:37.560
: Also, I should make a point,
and this is another great advantage

00:25:37.650 --> 00:25:41.610
of just-in-time compilers,
is that we can do processor-specific

00:25:41.610 --> 00:25:43.740
optimizations at runtime.

00:25:43.870 --> 00:25:49.410
The great beauty of Java is that I can
take this bytecode and port it to any

00:25:49.410 --> 00:25:52.400
machine and have it run on that machine.

00:25:52.450 --> 00:25:56.370
While even on the same general
architecture like the PowerPC,

00:25:56.370 --> 00:26:02.690
I can get different kind of optimization
on a G3 than I would on a G4 because

00:26:02.700 --> 00:26:04.790
of scheduling and so on and so forth.

00:26:04.880 --> 00:26:08.400
So the types of things that you could
do would be to say on the PowerPC I can

00:26:08.400 --> 00:26:11.670
use mask and shift operations,
or if I'm running on a G4 I can

00:26:11.670 --> 00:26:14.040
use a velocity engine instructions.

00:26:14.040 --> 00:26:16.640
So I can do that on the fly.

00:26:16.700 --> 00:26:18.930
And then I can do instruction scheduling.

00:26:19.000 --> 00:26:20.430
And once I've done that,
on a particular machine,

00:26:20.430 --> 00:26:20.910
I can run it on the G4.

00:26:20.940 --> 00:26:20.940
: And then I can do
instruction scheduling.

00:26:20.940 --> 00:26:20.940
And once I've done that,
on a particular machine,

00:26:20.940 --> 00:26:20.940
I can run it on the G4.

00:26:20.940 --> 00:26:20.940
: And then I can do
instruction scheduling.

00:26:20.940 --> 00:26:20.940
And once I've done that,
on a particular machine,

00:26:20.940 --> 00:26:20.940
I can run it on the G4.

00:26:20.940 --> 00:26:23.690
or a particular machine
that I'm running on,

00:26:23.700 --> 00:26:26.440
I can cache the code that I've generated.

00:26:26.440 --> 00:26:30.540
The next time I go and execute it,
I'm going to use that cache code.

00:26:30.540 --> 00:26:31.610
I don't have to recompile it.

00:26:32.010 --> 00:26:36.310
That code has already been tailored
for the machine it's running on.

00:26:39.830 --> 00:26:41.700
What gets compiled?

00:26:41.890 --> 00:26:46.460
There are probably all kinds of
myths out there about all these

00:26:46.460 --> 00:26:52.700
magic heuristics that we use to
figure out what method gets executed.

00:26:52.930 --> 00:26:58.780
There are some truths and rumors,
but generally the things that do get

00:26:58.780 --> 00:27:06.250
compiled into native machine code are
primarily methods that have loops.

00:27:07.270 --> 00:27:10.440
and methods that have been
interpreted n number of times.

00:27:10.490 --> 00:27:14.340
Those are the two primary triggers
that trigger whether something gets

00:27:14.340 --> 00:27:16.140
converted to native code or not.

00:27:16.240 --> 00:27:20.310
A method that has a loop may get
executed once in the interpreter,

00:27:20.310 --> 00:27:23.930
but then each successive time that
it may get converted to native

00:27:23.940 --> 00:27:27.200
code and get run as native code.

00:27:27.200 --> 00:27:30.280
The triggers for n number of times,
I said n number of times

00:27:30.280 --> 00:27:33.200
because different JITs
trigger at different levels.

00:27:33.200 --> 00:27:37.430
Like hotspot triggers at around
1500 executions before it actually

00:27:37.430 --> 00:27:39.200
goes and converts to native.

00:27:39.200 --> 00:27:43.800
But that fluctuates depending
on different kinds of criteria.

00:27:43.950 --> 00:27:46.400
What doesn't get compiled
are typically things that,

00:27:46.490 --> 00:27:49.240
say, if you have a method
that's currently running,

00:27:49.240 --> 00:27:51.470
and it's looping and
calling other things,

00:27:51.510 --> 00:27:53.810
and it seems to be
looping for a long time.

00:27:53.820 --> 00:27:59.220
If it didn't meet the original loop
criteria and didn't get compiled,

00:27:59.220 --> 00:28:02.810
it may sit there and
continue as an interpreter.

00:28:02.820 --> 00:28:05.640
This is something that would
require on-stack replacement.

00:28:05.640 --> 00:28:08.560
In the current version of Hotspot,
we don't have that in place yet.

00:28:08.560 --> 00:28:11.710
But you should be able to replace,
eventually we'll be able

00:28:11.790 --> 00:28:15.250
to replace something that's
currently being interpreted with

00:28:15.260 --> 00:28:17.180
something that's been compiled.

00:28:17.180 --> 00:28:20.980
But that doesn't prevent that
method from being compiled.

00:28:20.980 --> 00:28:23.680
What happens is that if that
method is being called by any

00:28:23.680 --> 00:28:27.000
other point in your application,
then it will use the compiled

00:28:27.150 --> 00:28:30.800
version of it because it's already
been triggered to be compiled.

00:28:32.170 --> 00:28:36.070
Class initializers typically don't
get compiled because of the fact

00:28:36.070 --> 00:28:37.590
that they're only executed once.

00:28:37.590 --> 00:28:40.500
They usually do the initialization
of their statics and create

00:28:40.510 --> 00:28:41.840
whatever things that they need.

00:28:41.860 --> 00:28:44.100
They don't need to go beyond that.

00:28:44.100 --> 00:28:49.100
So they typically don't get compiled.

00:28:49.400 --> 00:28:52.080
Finally,
things that are written in Java assembler

00:28:52.080 --> 00:28:56.660
that are very convoluted in their go-to
structures and whatnot where it's really

00:28:56.660 --> 00:28:59.100
hard to do the analysis of that code.

00:28:59.100 --> 00:29:04.090
We can't generate native code for them.

00:29:04.100 --> 00:29:07.100
We may try,
but it's typically not worth the trouble.

00:29:07.100 --> 00:29:11.760
The only time I've ever run into
that really is with the JCK.

00:29:11.760 --> 00:29:15.940
There's a lot of the JCK tests
that try to see what you can

00:29:15.940 --> 00:29:18.100
do to trip up on the JIT.

00:29:18.100 --> 00:29:19.100
So, that's the first thing.

00:29:19.100 --> 00:29:22.080
So you don't typically
have to worry about that.

00:29:23.040 --> 00:29:28.670
Okay, so now I have ten hints or
things that you can do that you

00:29:28.690 --> 00:29:32.170
can provide the code generator,
provide to the code generator that

00:29:32.170 --> 00:29:34.690
will actually help your performance.

00:29:34.700 --> 00:29:37.450
There's various degrees of
performance improvement here.

00:29:37.460 --> 00:29:41.540
Some may be a little bit
more dramatic than others,

00:29:41.540 --> 00:29:46.720
but you don't need to use them all and
you don't necessarily have to go out

00:29:46.760 --> 00:29:48.620
and feel that you have to use them all.

00:29:48.620 --> 00:29:51.240
They're just ideas that you can
keep in the back of your mind

00:29:51.240 --> 00:29:56.420
when you're trying to tune your
application at the end of your cycle.

00:29:56.420 --> 00:29:59.930
The first and probably the most
important thing is to write

00:29:59.930 --> 00:30:01.820
small and concise methods.

00:30:01.820 --> 00:30:06.230
Try to avoid methods that have
2,000 lines of code in them because

00:30:06.230 --> 00:30:10.870
what happens is that when the
just-in-time compiler kicks in,

00:30:10.880 --> 00:30:12.480
it's got to compile the whole thing.

00:30:12.480 --> 00:30:16.440
And maybe you're only going to use
a couple of lines of it because

00:30:16.440 --> 00:30:21.210
you've got this big case statement
and there's maybe two lines in it.

00:30:21.240 --> 00:30:24.250
One of them is going to get executed
most of the time and the other ones

00:30:24.250 --> 00:30:26.240
may get compiled or run very rarely.

00:30:26.240 --> 00:30:32.340
So what you should try to do
is to try to keep them small so

00:30:32.340 --> 00:30:33.880
that they'll compile quickly.

00:30:33.880 --> 00:30:39.300
And then if you've got some code
that's not going to be used very often,

00:30:39.300 --> 00:30:43.580
move that code into separate
routines so that if it's necessary,

00:30:43.580 --> 00:30:45.880
they'll be compiled then
but otherwise move them off.

00:30:45.880 --> 00:30:49.230
Don't worry about your method
being too small because method

00:30:49.230 --> 00:30:50.460
inlining will take care of that.

00:30:51.240 --> 00:30:54.520
The JIT will figure out the nice load
balance to get a nice size routine

00:30:54.520 --> 00:30:57.720
and what inlines nicely and so on.

00:30:57.720 --> 00:30:59.540
So don't worry about
the size of the thing.

00:30:59.540 --> 00:31:00.230
It's not important.

00:31:00.240 --> 00:31:01.560
That's not crucial.

00:31:01.560 --> 00:31:02.740
All right.

00:31:04.100 --> 00:31:06.380
: The method is too simple.

00:31:06.380 --> 00:31:10.770
Finally,
you should always remember that accessor

00:31:10.770 --> 00:31:13.860
methods are almost always in line.

00:31:13.860 --> 00:31:20.330
Even in the classic interpreter,
the accessor methods were often in line.

00:31:20.330 --> 00:31:27.030
It's good to use accessor methods
instead of accessing the fields directly.

00:31:27.320 --> 00:31:30.300
The next hint is to
trust the supply classes.

00:31:30.320 --> 00:31:34.160
What we try to do is look
for hot spots in your code,

00:31:34.230 --> 00:31:37.800
things that take a long time
and try to gain performance.

00:31:37.800 --> 00:31:41.830
One of the things we do in our
analysis is identify methods

00:31:41.840 --> 00:31:43.300
that get executed a lot.

00:31:43.300 --> 00:31:47.300
We want to try to tune those so
that they execute very quickly.

00:31:47.300 --> 00:31:49.650
Often we tune them
directly into assembler.

00:31:49.660 --> 00:31:53.670
Methods in the class string
and string buffer vector,

00:31:53.680 --> 00:31:58.020
which are used a lot,
we actually have intrinsic or built-in

00:31:58.060 --> 00:32:02.890
methods to deal with a lot of those
situations so you get better performance.

00:32:02.900 --> 00:32:08.270
If you think you can write
it better than Sun did,

00:32:08.270 --> 00:32:13.240
just remember that maybe we're
going to do it a little bit

00:32:13.370 --> 00:32:15.160
better for you in the background.

00:32:15.160 --> 00:32:17.890
That's just something to keep in mind.

00:32:17.970 --> 00:32:19.640
Array copy is something
we gain performance on.

00:32:19.710 --> 00:32:23.660
If you're running on a G4,
hopefully we'll be able to get this.

00:32:23.660 --> 00:32:24.890
This is not currently in place.

00:32:24.900 --> 00:32:29.780
But if you're array copy running on a G4,
use the velocity engine

00:32:29.780 --> 00:32:31.660
to help with the copy.

00:32:31.660 --> 00:32:33.260
And sine and cosine and tan.

00:32:33.260 --> 00:32:38.300
On Intel, you would call the hardware
directly to do those.

00:32:38.300 --> 00:32:41.900
We call the library directly so you
don't actually go through the glue.

00:32:41.900 --> 00:32:44.540
So it's a bit of a
performance improvement.

00:32:46.520 --> 00:32:52.600
We don't have 64-bit architecture,
so there is a cost in using lawns.

00:32:53.150 --> 00:32:57.420
If you don't really need longs,
you're just doing it because you think

00:32:57.440 --> 00:33:01.030
you might need the precision later,
then we'll maybe rethink it a little bit

00:33:01.030 --> 00:33:03.300
and go back to using straight integers.

00:33:03.760 --> 00:33:08.500
Long multiply takes five instructions.

00:33:08.670 --> 00:33:11.500
Long divide has to call a subroutine.

00:33:11.500 --> 00:33:14.100
A shift operation may
take several instructions.

00:33:14.100 --> 00:33:19.800
So it's not as simple as just saying
long and things are going to work well.

00:33:19.800 --> 00:33:22.810
There's lots of techniques to
get around some of the problems

00:33:22.880 --> 00:33:24.400
you might have with longs.

00:33:24.400 --> 00:33:29.190
I did a class library that handles
the situation where you're trying

00:33:29.190 --> 00:33:32.040
to use longs that deal with the
unsigned integer problem when

00:33:32.040 --> 00:33:33.100
you want to do unsigned compares.

00:33:33.250 --> 00:33:37.130
There's a way of actually doing that
without having to resort to long.

00:33:37.770 --> 00:33:40.120
Floats vs.

00:33:40.120 --> 00:33:46.110
Doubles Floats are smaller
and take up less memory.

00:33:46.130 --> 00:33:49.790
In most circumstances,
floats and doubles have

00:33:49.790 --> 00:33:51.700
equivalents in execution.

00:33:51.800 --> 00:33:53.910
But there are some
circumstances like divide,

00:33:53.910 --> 00:33:56.660
where divide of a double
is actually twice as long,

00:33:56.670 --> 00:33:59.700
or almost twice as long, as a float.

00:33:59.760 --> 00:34:02.380
So if you don't really
need the precision,

00:34:02.380 --> 00:34:03.700
stick with float.

00:34:03.700 --> 00:34:06.940
Another reason why I'm recommending
using float is that as we

00:34:06.940 --> 00:34:09.700
progress to the velocity engine,
the velocity engine

00:34:09.700 --> 00:34:13.280
doesn't support doubles,
it only supports floats.

00:34:13.530 --> 00:34:16.810
So if you're thinking about
declaring an array of doubles,

00:34:16.810 --> 00:34:19.320
see if you can use an
array of floats instead,

00:34:19.320 --> 00:34:23.570
because then that's most likely
what we'll be able to use to,

00:34:23.570 --> 00:34:26.840
or what we'll apply
the velocity engine to.

00:34:26.860 --> 00:34:30.870
There's no commitment to that,
but just keep that in mind.

00:34:31.660 --> 00:34:35.280
Try to avoid the use of generic types.

00:34:35.280 --> 00:34:38.200
It costs actually to
use these generic types,

00:34:38.200 --> 00:34:42.010
especially when you're doing
assignments between the generic

00:34:42.080 --> 00:34:43.770
type to a specific type.

00:34:43.820 --> 00:34:47.600
Because the VM has to do a type check
to make sure that it's valid to do that.

00:34:47.600 --> 00:34:49.600
And it does that type check at runtime.

00:34:49.600 --> 00:34:52.860
And it may have to search up the
class hierarchy in order to determine

00:34:52.860 --> 00:34:54.600
whether that's a member or not.

00:34:55.650 --> 00:34:57.600
Okay, so that's just something
that you should keep in mind.

00:34:57.630 --> 00:34:59.880
Especially when you're doing
assignments from a generic

00:35:00.020 --> 00:35:01.600
type array to a specific array.

00:35:01.600 --> 00:35:04.600
Because that means that even
if you're doing an array copy,

00:35:04.640 --> 00:35:07.590
it has to validate everything that's
being moved from that array over.

00:35:07.590 --> 00:35:11.600
Okay, it has to go through a class check.

00:35:12.220 --> 00:35:16.440
So, try to use subclassing and method
overloading as much as you can.

00:35:16.440 --> 00:35:20.730
Because that will actually be better
in the long run than actually -- and

00:35:20.740 --> 00:35:24.600
then using one -- let's say writing
one routine that has a generic type.

00:35:24.990 --> 00:35:27.230
And then doing an instance
of check inside of it.

00:35:27.230 --> 00:35:29.900
It's better to do the
overloading of the methods.

00:35:31.870 --> 00:35:34.600
Copy the local values.

00:35:34.600 --> 00:35:38.690
Some of the optimizers in the JITs will
actually do this optimization for you,

00:35:38.700 --> 00:35:46.530
but it's better for the interpreter,
it's better for the lower-end JITs and

00:35:46.550 --> 00:35:52.380
so on and so forth to have you move
it out and work with that copy and

00:35:52.380 --> 00:35:55.180
then step it back in if you need to.

00:35:55.810 --> 00:36:02.000
: In this particular example,
the example on the left-hand side,

00:36:02.000 --> 00:36:06.400
we have the increment of
the index -- or sorry,

00:36:06.400 --> 00:36:11.320
extracting the value from the table,
increment the value, check to see if the

00:36:11.320 --> 00:36:15.290
value has exceeded 100,
and then reset it to zero if it has.

00:36:15.320 --> 00:36:20.110
Every time you have that index,
it's going to have to do

00:36:20.110 --> 00:36:20.110
an array balance check.

00:36:20.680 --> 00:36:33.840
: Again, the higher level optimizers will
take care of that and move that out,

00:36:33.840 --> 00:36:33.840
but you can't rely on that.

00:36:33.840 --> 00:36:33.840
So it's probably a good idea to
move it into a separate entity.

00:36:35.570 --> 00:36:42.180
The other thing is there's a semantic
issue there where if there's another

00:36:42.180 --> 00:36:46.590
thread that goes and changes that
field or changes that array entry,

00:36:46.600 --> 00:36:48.490
you don't know which
copy you're going to get.

00:36:48.500 --> 00:36:50.830
So if you extract the copy,
work with that copy and

00:36:50.830 --> 00:36:52.930
step it back in again,
you know exactly which

00:36:52.940 --> 00:36:54.500
value you're dealing with.

00:36:55.600 --> 00:36:59.020
In a situation where you have multiple
threads that are accessing something,

00:36:59.020 --> 00:37:02.440
you should use volatile if
you're not using synchronization.

00:37:02.440 --> 00:37:06.590
Volatile is a little bit cheaper
than synchronization because what

00:37:06.590 --> 00:37:11.760
it says is you need to reload that
value every time you access it.

00:37:11.760 --> 00:37:15.760
And if volatile is not there,
then what will happen is a

00:37:15.830 --> 00:37:19.360
highly optimizing JIT will say,
"Oh, well I've got this value.

00:37:19.360 --> 00:37:22.530
I don't have to reload it." But then
meanwhile another thread changes

00:37:22.530 --> 00:37:25.180
the value and you're sitting
there in your loop waiting for

00:37:25.180 --> 00:37:26.890
it to change and it won't change.

00:37:27.670 --> 00:37:33.740
So use the word volatile when you've got
global values that are being changed.

00:37:37.300 --> 00:37:43.400
: Final, we've had a lot of discussion
internally about these at final,

00:37:43.400 --> 00:37:43.400
but

00:37:43.770 --> 00:37:49.180
It's one of my favorite words as far
as just-in-time compiling is concerned

00:37:49.210 --> 00:37:53.260
because it gives me a lot of hints about
what the class can be or what kinds of

00:37:53.260 --> 00:37:55.700
optimizations I can do on the class.

00:37:55.700 --> 00:37:57.690
But it's something that
you don't need to overuse.

00:37:57.700 --> 00:38:03.080
Write your applications and if you
feel that the class is not going to

00:38:03.080 --> 00:38:08.680
ever be overridden for any reason,
for instance, your application class,

00:38:08.700 --> 00:38:12.700
if it's not going to be overridden,
declare it as being final.

00:38:12.810 --> 00:38:15.780
And what this wins you is the fact
that it says all of the methods in

00:38:15.780 --> 00:38:17.700
this class can now be monomorphic.

00:38:17.700 --> 00:38:18.700
They'll never be overridden.

00:38:18.700 --> 00:38:22.700
I can make direct calls so it
improves the performance of the calls.

00:38:22.700 --> 00:38:25.360
It also says that if I do an
instance of on that class,

00:38:25.530 --> 00:38:28.700
all I have to do is compare it to
see if it's equal to the class.

00:38:28.700 --> 00:38:32.700
So if I have the class string,
which is declared as final,

00:38:32.830 --> 00:38:36.190
then the check to see if that
is a string is a simple compare

00:38:36.270 --> 00:38:37.700
to see if the classes are equal.

00:38:37.700 --> 00:38:40.120
I don't have to search up the
class hierarchy in order to

00:38:40.120 --> 00:38:41.700
find out what's going on there.

00:38:41.700 --> 00:38:41.940
Amen.

00:38:45.960 --> 00:38:51.390
The other use of FINAL, of course,
is on statics.

00:38:51.420 --> 00:38:53.900
And this says, this value is constant.

00:38:53.910 --> 00:38:55.340
It's not going to change.

00:38:55.440 --> 00:38:57.890
So once the Just-In-Time compiler
knows that it's constant,

00:38:57.900 --> 00:39:00.290
it'll just grab it and say, okay,
I've got this.

00:39:00.290 --> 00:39:03.860
I can apply it to
optimizations in the code.

00:39:03.890 --> 00:39:06.420
And in this code sequence,
what I can gain here is that

00:39:06.420 --> 00:39:10.020
I know that the allocation of that
character array is a fixed size.

00:39:10.050 --> 00:39:15.300
So all I have to do is increment the
allocation pointer by a fixed size.

00:39:15.360 --> 00:39:19.490
Boom, I've got my array declared.

00:39:19.510 --> 00:39:22.070
And in the loop,
I know that it's a fixed size loop,

00:39:22.080 --> 00:39:24.960
or the loop is going to go
iterate a fixed number of times,

00:39:24.960 --> 00:39:26.120
in this case 32.

00:39:26.170 --> 00:39:28.920
So I can actually get rid of
the loop and maybe do a blanket

00:39:28.920 --> 00:39:33.400
initialization of that array to spaces.

00:39:33.400 --> 00:39:34.840
So, Thank you.

00:39:36.670 --> 00:39:40.930
If you have a choice between
declaring virtual class

00:39:41.090 --> 00:39:46.540
hierarchy or using interfaces,
you'll get better performance

00:39:46.600 --> 00:39:49.600
from virtual calls than you
will through interface calls.

00:39:49.600 --> 00:39:54.700
That's because a virtual call
requires a simple index into an

00:39:54.700 --> 00:39:59.600
array to get the address of the
method that you want to call.

00:39:59.650 --> 00:40:02.950
Where an interface requires an actual
search of the class to make sure we

00:40:03.010 --> 00:40:06.600
find the implementer of the class and
then it does an index into an array.

00:40:06.600 --> 00:40:08.600
So there's a little bit of overhead.

00:40:08.600 --> 00:40:11.200
Now in Hotspot they're very
clever where they've actually

00:40:11.200 --> 00:40:13.600
cached the last instruction or
the last method that you've called

00:40:13.710 --> 00:40:15.600
from a particular call point.

00:40:15.600 --> 00:40:18.910
So it's a little bit better in Hotspot,
but it still has to go through a

00:40:18.910 --> 00:40:22.280
verification to make sure that,
well, it really is an instance of that

00:40:22.280 --> 00:40:26.000
class that is being passed through.

00:40:29.910 --> 00:40:32.260
Limit the use of JNI and JDirect.

00:40:32.410 --> 00:40:37.950
Initially when I wanted to do this talk,
I wanted to convey to you that

00:40:37.950 --> 00:40:42.560
if you feel you can do it,
get better performance out of C,

00:40:42.560 --> 00:40:46.710
you should probably really rethink it
a little bit and think that Java is

00:40:46.710 --> 00:40:48.900
the way to actually write your code.

00:40:48.910 --> 00:40:52.900
Try to avoid going off and
doing things if you can.

00:40:52.900 --> 00:40:55.470
Because the optimization levels
that you're going to get in

00:40:55.470 --> 00:40:57.890
Java will be pretty close to C,
if not better,

00:40:57.900 --> 00:41:01.900
depending on which kind of level
of optimization that you're doing.

00:41:01.900 --> 00:41:05.040
When you're using JNI,
there's a translation layer

00:41:05.050 --> 00:41:06.550
that has to take place.

00:41:06.550 --> 00:41:08.900
You can translate into the system.

00:41:08.900 --> 00:41:12.130
And then coming back again,
you have to actually do a lookup of

00:41:12.130 --> 00:41:15.890
the method in order to find out which
method to call back into the VM.

00:41:15.900 --> 00:41:17.900
So use Java as much as possible.

00:41:17.900 --> 00:41:18.900
Subtitles by the Amara.org community

00:41:20.900 --> 00:41:24.790
In conclusion, I just want to repeat
what Yvonne said earlier.

00:41:24.790 --> 00:41:28.210
The best thing, first of all,
is to make sure that your

00:41:28.230 --> 00:41:30.070
application has a good design.

00:41:31.140 --> 00:41:34.680
Once you have a good design,
go back and look at places where

00:41:34.680 --> 00:41:36.890
you need to improve performance.

00:41:36.940 --> 00:41:40.310
We didn't talk about performance
tools here yet because we're not

00:41:40.340 --> 00:41:42.100
really finished with them yet.

00:41:42.100 --> 00:41:48.880
Hotspot is actually released with -XPROF,
which will give you a profile of the

00:41:48.880 --> 00:41:56.090
methods that have been executed and what
percentage of time you spent in there.

00:41:56.100 --> 00:41:58.100
As we go along,
we're going to have better tools.

00:41:58.100 --> 00:42:01.380
There's H-PROF tools that will
give you much more detailed

00:42:01.390 --> 00:42:03.100
reports on performance.

00:42:03.100 --> 00:42:07.300
Get a good design of your application,
then go back and start tweaking it and

00:42:07.300 --> 00:42:11.100
maybe apply a few of these hints so that
you can get the Just-In-Time compiler

00:42:11.100 --> 00:42:13.100
to produce better code for you.

00:42:13.100 --> 00:42:17.880
Okay?

00:42:17.880 --> 00:42:17.880
John.

00:42:17.880 --> 00:42:17.880
Oops.

00:42:29.170 --> 00:42:32.680
So hi,
I'm John Burkey and I'm on the AWT team.

00:42:32.690 --> 00:42:36.380
And I'm gonna talk about a little
bit different side of performance

00:42:36.380 --> 00:42:40.260
and that's how we can work
together to improve performance,

00:42:40.260 --> 00:42:42.540
specifically from the framework level.

00:42:42.540 --> 00:42:46.140
A lot of what you just heard is
about how to write good methods

00:42:46.180 --> 00:42:50.340
and good classes yourselves,
but from the AWT's perspective,

00:42:50.360 --> 00:42:53.470
we're more concerned with just
highlighting a few things that

00:42:53.480 --> 00:42:56.800
will help you use our frameworks,
actually JavaScript's frameworks,

00:42:56.800 --> 00:43:00.060
our implementation of Sun's frameworks.

00:43:00.100 --> 00:43:05.510
So anyway, the five major areas I'm
gonna cover are here.

00:43:05.700 --> 00:43:12.610
: I'm going to go ahead
and start the presentation.

00:43:12.610 --> 00:43:12.610
I don't need to read them.

00:43:12.610 --> 00:43:12.610
Basically, you'll see it's a lot of
usage pattern kind of stuff.

00:43:14.020 --> 00:43:17.660
So for image creation,
the main thing is there's a new call

00:43:17.660 --> 00:43:19.640
in Java 2 called get compatible image.

00:43:19.640 --> 00:43:24.120
And for all the 1.1 usage,
specifically swing and the kinds of

00:43:24.120 --> 00:43:27.700
things that are in the toolkit class,
we'll automatically take

00:43:27.700 --> 00:43:28.570
care of this for you.

00:43:28.580 --> 00:43:32.550
And what this will do is,
depending on the decisions we make

00:43:32.550 --> 00:43:36.130
based on device depth and stuff,
we'll make sure that

00:43:36.130 --> 00:43:37.880
it's the optimum image.

00:43:37.880 --> 00:43:42.260
Most users of image don't need
to do anything other than this.

00:43:43.260 --> 00:43:48.420
And if you do need to dive into bit
style manipulation of the image,

00:43:48.420 --> 00:43:52.020
then number one, check,
make sure you really want to do that,

00:43:52.040 --> 00:43:55.900
and then go into the imaging classes and
be real careful about the ones you pick.

00:43:55.900 --> 00:43:59.430
One of the cases here is that there
are some image types on Windows,

00:43:59.430 --> 00:44:02.670
for example,
that aren't as common on Mac,

00:44:02.670 --> 00:44:06.320
and so they may not
perform as you expected.

00:44:06.320 --> 00:44:07.780
So that's the kind of
thing you want to notice.

00:44:07.780 --> 00:44:11.220
And again, so get compatible image is
definitely your first choice.

00:44:14.350 --> 00:44:17.020
The next thing is this thing
called rendering hints.

00:44:17.020 --> 00:44:22.490
And if you have a graphics object,
you can both get the list of

00:44:22.490 --> 00:44:25.040
default hints and also set your own.

00:44:25.040 --> 00:44:30.260
And the basic idea is that with Java 2,
there's an ability to do a

00:44:30.260 --> 00:44:32.830
lot of really nice graphics.

00:44:32.840 --> 00:44:36.150
You can do anti-aliased text,
anti-aliased permutants.

00:44:36.160 --> 00:44:40.810
You can do image splitting with
different kinds of convolutions

00:44:40.810 --> 00:44:43.460
and do really high-quality work.

00:44:43.640 --> 00:44:45.890
But the fact is,
for a lot of what we do today,

00:44:45.890 --> 00:44:49.060
including most of our normal
GUI framework operations,

00:44:49.060 --> 00:44:51.330
we don't need quite the quality.

00:44:51.340 --> 00:44:55.900
So that's why, for example,
a lot of these will be defaulted to

00:44:55.900 --> 00:45:01.010
lower quality on the implementations
for like when Swing uses it.

00:45:02.330 --> 00:45:05.840
And that's also because a lot of times,
in GUI framework building,

00:45:05.840 --> 00:45:09.410
anti-aliasing can get in
the way and cause fuzziness.

00:45:09.420 --> 00:45:12.280
And I'm sure you have examples
of that in your own experience.

00:45:12.280 --> 00:45:17.760
So the key here is that we haven't
actually fine-tuned all this

00:45:17.760 --> 00:45:19.480
stuff yet in our implementation.

00:45:19.480 --> 00:45:23.490
But what I recommend is that
first you get familiar with

00:45:23.490 --> 00:45:27.680
these and experiment with them,
understand the different ones.

00:45:27.680 --> 00:45:28.860
I'll explain them in a sec here.

00:45:29.270 --> 00:45:34.000
And then as we moved towards shipping,
you try these again with our

00:45:34.050 --> 00:45:37.800
final candidates because you will
start to experience differences.

00:45:37.800 --> 00:45:41.910
This is really important for us
because we can make serious changes

00:45:41.920 --> 00:45:44.980
in the way implementation does
things when we're in fast mode.

00:45:44.980 --> 00:45:47.580
So rendering is a key.

00:45:47.580 --> 00:45:51.680
I actually deleted off the front
of these int key underscore,

00:45:51.680 --> 00:45:55.220
but they're in this
sun.awt.sunhands class.

00:45:55.220 --> 00:45:58.980
And so rendering is a key
that you can pass into.

00:45:59.120 --> 00:46:02.080
There's a little hash map,
and you can say basically

00:46:02.080 --> 00:46:03.280
quality or fast.

00:46:03.280 --> 00:46:07.520
Anti-aliasing you can turn on and
off for both primitives and text.

00:46:07.520 --> 00:46:08.640
That's why there's two of them there.

00:46:08.640 --> 00:46:11.140
Fractional metrics is the ability to...

00:46:11.500 --> 00:46:15.500
specified sub-pixel positions for text.

00:46:15.500 --> 00:46:18.540
One thing I'll note here, too,
is there's a new way

00:46:18.540 --> 00:46:21.010
to do text in Java 2,
which is glyph vectors.

00:46:21.010 --> 00:46:23.730
And it is the highest
performance way to do text.

00:46:23.730 --> 00:46:28.050
Don't assume that you can make the
-- do those kinds of things yourself.

00:46:28.050 --> 00:46:32.050
And if you really are doing text stuff,
you want to see high quality and speed,

00:46:32.050 --> 00:46:34.620
look at the way the swing
examples do this stuff before

00:46:34.620 --> 00:46:35.860
you go and write your own.

00:46:35.860 --> 00:46:39.800
Because they're using glyph vectors and
that is the fastest way to do the text.

00:46:39.800 --> 00:46:43.000
So fractional metrics comes up
because there's an additional cost

00:46:43.000 --> 00:46:48.480
with doing sub-pixel positioning of
your letters on your text rendering.

00:46:48.480 --> 00:46:52.040
So dithering,
there's a couple different choices there.

00:46:52.040 --> 00:46:54.130
Again, it's quality versus speed.

00:46:54.130 --> 00:46:57.000
Interpolation,
there's bilinear bicubic and

00:46:57.000 --> 00:46:59.180
that's for your image blitting.

00:46:59.180 --> 00:47:01.860
Basically your image will look a little
better when it's scaled to different

00:47:01.860 --> 00:47:04.590
sizes if you use the higher quality ones.

00:47:04.600 --> 00:47:05.520
But it's slower.

00:47:05.520 --> 00:47:10.020
So keep that in mind.

00:47:10.020 --> 00:47:13.320
Alpha And color rendering, same thing,
color speed.

00:47:17.270 --> 00:47:22.200
So bitmap manipulation,
this is pretty key for our platform.

00:47:22.200 --> 00:47:27.000
We're optimizing first to make swing
apps and normal usage apps fast.

00:47:27.000 --> 00:47:33.070
What that means is we actually are
not gonna use the same data buffer

00:47:33.070 --> 00:47:36.300
types internally that are used
on the Windows implementation,

00:47:36.300 --> 00:47:39.360
and that's because then we can do
hardware accelerated blitz between

00:47:39.360 --> 00:47:41.220
our off screens and our screen.

00:47:41.220 --> 00:47:44.410
And so in order for us to do that,
we have a different

00:47:44.410 --> 00:47:47.880
implementation class under there,
so don't assume that you can type unsaved

00:47:47.910 --> 00:47:50.380
cast down to specific data buffer types.

00:47:50.380 --> 00:47:51.520
They won't be down there.

00:47:51.560 --> 00:47:55.430
So at least look, do an instance of,
you can create the other

00:47:55.430 --> 00:47:58.270
types and they will work,
but they will be slower.

00:47:58.280 --> 00:47:59.280
So just be careful.

00:47:59.380 --> 00:48:05.020
Again, this next one's kind of obvious,
but there's a whole bunch of

00:48:05.020 --> 00:48:09.880
APIs that are in the new imaging
stuff that are useful for some

00:48:09.880 --> 00:48:14.040
high quality advanced imaging stuff
and they were developed with some,

00:48:14.040 --> 00:48:16.920
developers with that name,
but they're not as

00:48:16.920 --> 00:48:19.020
useful for typical case.

00:48:19.060 --> 00:48:21.860
So I'm calling it low
call frequency methods.

00:48:21.890 --> 00:48:24.980
The point is that unreadable
raster and buffered image,

00:48:24.980 --> 00:48:30.960
there's two basic ways to do things,
and that's, this is the ones that I think

00:48:30.960 --> 00:48:33.340
are good for a typical case,
and it's basically,

00:48:33.340 --> 00:48:36.940
you can pass in a whole rect of
whatever size you want of pixels

00:48:36.940 --> 00:48:41.700
to push between your bitmaps,
your rasters.

00:48:41.700 --> 00:48:45.020
And this is better because you can
control the frequency of call operation,

00:48:45.020 --> 00:48:48.060
for the number of pixels you want to do.

00:48:48.060 --> 00:48:51.300
So for example, if you want to drink
RAM and you've got it,

00:48:51.300 --> 00:48:54.400
you can basically have a full
copy that gets pushed across

00:48:54.400 --> 00:48:56.260
and make one call to copy.

00:48:56.260 --> 00:49:00.150
Or if you want to go
all the way their way,

00:49:00.170 --> 00:49:03.840
I would recommend at
the lowest per scanline,

00:49:03.840 --> 00:49:06.040
you can say make a
method call per scanline.

00:49:06.040 --> 00:49:10.200
That's much better than these for speed,
which exist in the API,

00:49:10.240 --> 00:49:12.660
but you want to be aware,
and that's where you basically

00:49:12.660 --> 00:49:14.380
make a method call per pixel.

00:49:14.380 --> 00:49:16.520
shooting yourself in
the foot in most cases.

00:49:19.000 --> 00:49:38.710
: So double buffering,
this is kind of interesting.

00:49:38.710 --> 00:49:38.710
So on Mac OS X we double buffer all
Carbon windows right now in most cases.

00:49:38.710 --> 00:49:38.710
So what that means is since your
app's already double buffered,

00:49:38.710 --> 00:49:38.710
and we'll take care of
flushing that up efficiently,

00:49:38.710 --> 00:49:38.710
if you want to have the
effect of double buffering,

00:49:38.750 --> 00:49:41.160
use the swing stuff because
on Windows that stuff

00:49:41.160 --> 00:49:44.450
will be double buffered,
and on our platform you'll

00:49:44.460 --> 00:49:46.720
still just be double buffered.

00:49:46.720 --> 00:49:49.690
Whereas, if you create your own
Java image and draw into that,

00:49:49.690 --> 00:49:52.670
which is really easy to do in
Java as most of you know I'm sure,

00:49:52.670 --> 00:49:56.770
and then blip that image you'll
actually be triple buffered on Mac OS X,

00:49:56.770 --> 00:49:58.830
which is kind of wasteful.

00:49:58.860 --> 00:50:02.870
So again, we take care of the swing.

00:50:02.870 --> 00:50:07.520
So what I do in the case where I want
to implement this is I just go ahead

00:50:07.540 --> 00:50:11.000
and use the swing stuff and use Jpanel,
etc.

00:50:11.000 --> 00:50:13.000
and then it'll all just be taken care of.

00:50:16.250 --> 00:50:18.300
: So this is another
issue from ECOWAS 10.

00:50:18.300 --> 00:50:19.290
It's performance related.

00:50:19.320 --> 00:50:23.640
Basically, the hard work of doing live
window resizing is yours,

00:50:23.640 --> 00:50:25.200
the app developers.

00:50:25.200 --> 00:50:29.700
We'll make, of course,
all the primitive stuff fast, but

00:50:29.800 --> 00:50:33.100
When you do live window sizing,
suddenly component.paint

00:50:33.100 --> 00:50:34.180
actually has to be fast,
right?

00:50:34.180 --> 00:50:36.270
Because it's going to be
called every time you move the

00:50:36.270 --> 00:50:37.580
little bottom of the window.

00:50:37.670 --> 00:50:42.100
So if your code can't handle that,
you'll either have real

00:50:42.100 --> 00:50:45.120
chunky performance when
you're moving the window.

00:50:45.120 --> 00:50:48.350
It'll act funny, like you have a low
frame rate or something,

00:50:48.350 --> 00:50:50.690
and the mouse will become unresponsive.

00:50:50.780 --> 00:50:54.020
Or what you can do is do some
kind of threaded rendering.

00:50:54.020 --> 00:50:59.020
So the case,
some cases are like JPEG loading,

00:50:59.020 --> 00:51:01.420
where you show what you got, don't wait.

00:51:01.420 --> 00:51:03.810
Or if you've got just a
real complicated image,

00:51:03.820 --> 00:51:07.720
maybe you just pass over and do
some kind of simple version of that.

00:51:07.720 --> 00:51:10.320
And queue up a thread that
does the rest of the work.

00:51:10.320 --> 00:51:12.720
And then at that point,
if it's real complicated like that,

00:51:12.720 --> 00:51:15.020
maybe you would use double,
another buffer.

00:51:15.020 --> 00:51:17.730
In fact, be triple buffered,
but draw to an image

00:51:17.730 --> 00:51:19.010
and then do it later.

00:51:19.020 --> 00:51:22.020
That gets a little more complicated
because then as the size changes,

00:51:22.020 --> 00:51:24.020
maybe you want to sample and draw images.

00:51:24.020 --> 00:51:25.020
And then scale.

00:51:25.020 --> 00:51:28.020
I can talk to people afterwards if
there's specific examples for that.

00:51:28.020 --> 00:51:31.010
The main thing I want to point
out is that with live resizing,

00:51:31.020 --> 00:51:33.020
your paint is going to be called a lot.

00:51:33.020 --> 00:51:40.010
And so it needs to be as
fast as you can make it.

00:51:40.020 --> 00:51:43.040
So I guess that's all for my slides.

00:51:50.420 --> 00:51:53.400
I guess I really don't have much to
say as far as the summary is concerned,

00:51:53.400 --> 00:51:56.240
but the environment that
you'll be working in is a

00:51:56.240 --> 00:51:59.810
little bit different than MRJ.

00:51:59.810 --> 00:52:02.650
And there are going to be some different
things that you may have spent a lot of

00:52:02.730 --> 00:52:04.900
time trying to get performance on in MRJ.

00:52:04.900 --> 00:52:07.400
They're going to be a little bit
different when you get to Mac OS X.

00:52:07.400 --> 00:52:11.500
So hopefully we've sort of covered a
broad enough area that you keep these

00:52:11.500 --> 00:52:13.750
things in mind when you're working.

00:52:13.760 --> 00:52:18.940
So I guess I'm going to get Alan to
come up and coordinate a Q&A session.

00:52:18.940 --> 00:52:20.400
We've got time.