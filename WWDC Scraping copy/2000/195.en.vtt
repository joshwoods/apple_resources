WEBVTT

00:00:08.210 --> 00:00:10.900
Hi, my name is Robert Bowdidge,
and I'm a member of Apple's

00:00:10.970 --> 00:00:12.140
Developer Tools Group.

00:00:12.140 --> 00:00:15.330
And today I'm going to talk about
Murphy's Law Revisited or Apple's

00:00:15.330 --> 00:00:19.510
Performance Tools for Mac OS X,
and hopefully we're actually

00:00:19.650 --> 00:00:21.210
going to have a demo.

00:00:21.630 --> 00:00:25.510
So the first question,
or the way I'd like to start this,

00:00:25.510 --> 00:00:26.890
is why are you all here?

00:00:27.040 --> 00:00:29.430
Why should you actually care
about performance tools,

00:00:29.430 --> 00:00:30.430
especially today?

00:00:30.540 --> 00:00:32.820
Well, it turns out that with
the changes in Mac OS X,

00:00:32.820 --> 00:00:35.360
this is a perfect time to
be very concerned about the

00:00:35.440 --> 00:00:37.020
performance of your applications.

00:00:37.130 --> 00:00:40.820
We're all working on a new operating
system where the libraries that we're

00:00:41.000 --> 00:00:44.260
used to are now working on different
system routines and may not behave the

00:00:44.260 --> 00:00:46.280
way that we're used to them behaving.

00:00:46.290 --> 00:00:48.880
As a result,
we need to actually take a look at

00:00:48.880 --> 00:00:52.620
our apps and decide whether the system
calls and the library calls that we

00:00:52.620 --> 00:00:56.010
used to be doing actually have the
same performance that they used to,

00:00:56.010 --> 00:00:59.390
whether there's any changes in their
semantics and what operations they do,

00:00:59.700 --> 00:01:02.540
whether there's changes
in how they behave.

00:01:02.540 --> 00:01:04.510
In addition,
some of the algorithms we may

00:01:04.510 --> 00:01:08.970
have chosen in the past may no
longer work as well in Mac OS X.

00:01:09.050 --> 00:01:10.820
And here's three examples
that actually come out of some

00:01:10.820 --> 00:01:12.850
of my experience at Apple.

00:01:12.970 --> 00:01:15.750
The first one is the difference
in how the heap is done.

00:01:15.750 --> 00:01:18.760
For example, in Mac OS X,
we no longer have fixed-sized heaps.

00:01:18.760 --> 00:01:21.490
Instead, the heap will expand
as far as it needs to,

00:01:21.490 --> 00:01:24.080
as long as you keep allocating memory.

00:01:24.120 --> 00:01:26.310
As a result,
the idea of allocating memory and

00:01:26.310 --> 00:01:29.600
then setting the purgeable bit
doesn't make sense anymore because

00:01:29.600 --> 00:01:32.710
the operating system is never going
to bother to purge this memory.

00:01:32.960 --> 00:01:37.580
There was one case in the Finder,
actually, where they were loading

00:01:37.580 --> 00:01:39.330
in the background image.

00:01:39.400 --> 00:01:42.110
They would load the compressed
image into a buffer,

00:01:42.110 --> 00:01:44.960
then they would uncompress
it into another buffer,

00:01:44.960 --> 00:01:47.560
and then they had a third
copy for--as a working copy

00:01:47.560 --> 00:01:49.340
that was marked as purgeable.

00:01:49.340 --> 00:01:51.100
And the idea was that if
the memory was ever needed,

00:01:51.200 --> 00:01:54.460
that copy would get blown away
and it could be recreated easily.

00:01:54.560 --> 00:01:56.770
On Mac OS X,
at least two of these buffers

00:01:56.770 --> 00:01:58.580
weren't really necessary.

00:01:58.590 --> 00:02:01.320
The idea of the purgeable
case didn't really make sense

00:02:01.320 --> 00:02:03.340
because it was never going away.

00:02:03.440 --> 00:02:06.860
And the copy that was on the
version of the file on disk wasn't

00:02:06.860 --> 00:02:11.580
needed to be copied into memory
because we have memory mapped files.

00:02:11.590 --> 00:02:13.780
And so with cases like that,
you need to worry about exactly

00:02:13.780 --> 00:02:17.440
what your app's doing with memory.

00:02:17.440 --> 00:02:17.440
Similarly, the case of pulling--.

00:02:17.440 --> 00:02:21.810
The case of pulling is much more
expensive on a multitask operating

00:02:21.900 --> 00:02:26.270
system than when you're only expecting
one application to really be taking

00:02:26.270 --> 00:02:28.050
control of the CPU at a time.

00:02:28.110 --> 00:02:30.090
If you're sitting around
looking out on the network,

00:02:30.120 --> 00:02:32.100
looking on the file system
for a file to appear,

00:02:32.100 --> 00:02:35.010
waiting for the mouse to move,
those are cycles that are being

00:02:35.010 --> 00:02:38.170
used by the CPU that can't be
used for other applications.

00:02:38.230 --> 00:02:40.800
And so you don't want to pull
on Mac OS X because you're going

00:02:40.800 --> 00:02:42.710
to drag down the performance of
all the other things that might

00:02:42.710 --> 00:02:44.200
be running in the background.

00:02:44.200 --> 00:02:46.780
And finally, because we're no longer
operating in a single disk,

00:02:46.780 --> 00:02:48.400
we're operating in a
single address space,

00:02:48.420 --> 00:02:51.850
the idea of inter-process communication
becomes a bit more difficult.

00:02:51.960 --> 00:02:55.690
We can't just sort of pass a
pointer and provide another app a

00:02:55.830 --> 00:02:57.780
sneaky way to look into our memory.

00:02:57.800 --> 00:03:00.670
Instead,
we need to explicitly use one of the real

00:03:00.740 --> 00:03:04.750
IPC mechanisms such as Mach messaging or
TCP/IP or we need to use shared memory.

00:03:04.750 --> 00:03:10.600
Or we actually need to map memory into
both processes using the Mach underlying

00:03:10.600 --> 00:03:13.110
virtual memory mechanisms.

00:03:13.840 --> 00:03:17.470
In addition, many of the tools that we're
used to using may no longer work

00:03:17.480 --> 00:03:19.820
or may not make sense anymore.

00:03:19.840 --> 00:03:22.980
A good example of this
is Even Better Bus Error.

00:03:23.010 --> 00:03:26.600
This is a quick and dirty tool that
will basically make sure that your app

00:03:26.600 --> 00:03:31.420
is not writing or reading from address
zero by putting a bogus value there.

00:03:31.420 --> 00:03:34.370
On Mac OS X,
this is not necessary anymore because

00:03:34.370 --> 00:03:37.350
the operating system by default
makes sure that for every task,

00:03:37.370 --> 00:03:40.220
the first page of memory
ends up being non-readable,

00:03:40.220 --> 00:03:41.170
non-writable.

00:03:41.170 --> 00:03:43.770
If your application tries
to read or write to it,

00:03:43.960 --> 00:03:45.090
boom, it crashes.

00:03:45.090 --> 00:03:48.990
You get an immediate feedback
that you're doing something badly.

00:03:49.620 --> 00:03:52.000
Isn't that nice?

00:03:52.000 --> 00:03:55.120
In other cases,
there's new tasks that may be necessary.

00:03:55.120 --> 00:03:57.710
There's other cases such as
understanding about purgeable and

00:03:57.710 --> 00:03:59.460
non-purgeable that no longer matter.

00:03:59.500 --> 00:04:02.500
You need to understand
different sets of tools.

00:04:02.620 --> 00:04:06.490
Hopefully,
what you'll learn today are some

00:04:06.930 --> 00:04:12.540
ideas about what tools are out
there and perhaps what tools are

00:04:12.540 --> 00:04:12.790
necessary as some ideas about third
party things that can be filled in.

00:04:13.870 --> 00:04:16.870
So, as an overview,
I'm going to start out by talking

00:04:17.090 --> 00:04:19.590
about two classes of tools.

00:04:19.590 --> 00:04:22.630
The first set of tools are a set
of Unix-like command line tools

00:04:22.770 --> 00:04:27.360
that give you information about
the low level state of the system.

00:04:27.390 --> 00:04:29.740
The second set of tools are some
graphical and exploratory tools

00:04:29.740 --> 00:04:32.290
that actually give you a higher
level understanding about how

00:04:32.410 --> 00:04:33.770
your application is running.

00:04:33.950 --> 00:04:37.880
Some of these may be familiar to you,
such as MallocDebug or Sampler.

00:04:37.880 --> 00:04:39.690
For each tool,
I'm going to try to give you a little

00:04:39.700 --> 00:04:43.510
bit of background about how it's used,
what its purpose is,

00:04:43.510 --> 00:04:48.670
and also hopefully give you enough
excitement to make you want to go off and

00:04:48.670 --> 00:04:51.120
try these on your own and explore them.

00:04:51.120 --> 00:04:52.880
For each of them,
I'll also try to give some of the

00:04:52.880 --> 00:04:55.340
details about how you interpret
its data and how to use it to

00:04:55.340 --> 00:04:56.920
actually analyze your system.

00:04:56.960 --> 00:04:58.300
However, this is going to be a survey.

00:04:58.440 --> 00:05:02.300
There's just not enough time to really
go into depth about what's going on.

00:05:02.480 --> 00:05:09.340
And so, hopefully this will at least spur
you to explore and ask questions.

00:05:09.340 --> 00:05:10.380
Finally, there's two other themes
I'm going to try to cover.

00:05:10.380 --> 00:05:10.380
One is the use of the
Unix-like command line tool.

00:05:10.380 --> 00:05:10.380
The other is the use of the
Unix-like command line tool.

00:05:10.380 --> 00:05:10.380
The other is the use of the
Unix-like command line tool.

00:05:10.480 --> 00:05:10.480
The other is the use of the
Unix-like command line tool.

00:05:10.480 --> 00:05:10.780
The other is the use of the
Unix-like command line try to

00:05:10.940 --> 00:05:12.990
keep going through as I talk.

00:05:13.050 --> 00:05:16.990
The first one is that I want to tell
you a little about how you might try

00:05:16.990 --> 00:05:18.230
to approach performance problems.

00:05:18.390 --> 00:05:21.700
These won't be at a very high level,
but hopefully these will be some tricks.

00:05:21.710 --> 00:05:24.400
The second issue I'm going to try to
do is give you some little hints about

00:05:24.480 --> 00:05:27.340
performance problems that I've seen,
such as what I talked

00:05:27.340 --> 00:05:29.220
about on the last slide.

00:05:29.350 --> 00:05:32.480
Once again, I'm not going to be able
to go into detail on these.

00:05:32.540 --> 00:05:37.140
If you're looking for specific details
about how to make your calls to,

00:05:37.140 --> 00:05:39.500
let's say, core foundation more
efficient or the Carbon,

00:05:39.990 --> 00:05:42.470
talking to the people who are
responsible for those libraries,

00:05:42.820 --> 00:05:45.070
going to those sessions,
such as the core graphics session

00:05:45.070 --> 00:05:48.340
yesterday or some of the Carbon sessions
or the core foundation sessions,

00:05:48.470 --> 00:05:51.480
will give you more ideas about some
of the obvious things you should be

00:05:51.480 --> 00:05:54.020
doing to make your app more efficient.

00:05:55.440 --> 00:05:56.340
So Scott, how are we doing?

00:05:56.400 --> 00:06:06.140
OK, so let's start off with
command line performance tools.

00:06:06.320 --> 00:06:11.650
How many of you have experience--
have actually used Unix?

00:06:12.320 --> 00:06:13.380
Good.

00:06:13.380 --> 00:06:15.720
Most of you actually will have a leg up.

00:06:15.750 --> 00:06:20.160
How many of you think the command
line tools are the work of the devil?

00:06:20.160 --> 00:06:21.690
OK.

00:06:23.780 --> 00:06:25.700
Thank you, Scott.

00:06:25.780 --> 00:06:28.500
Well, actually, there's some very good
reasons to have these.

00:06:28.720 --> 00:06:30.940
The first one,
the tools that we have here are

00:06:30.940 --> 00:06:34.030
basically meant to be quick and
dirty tools to give you information

00:06:34.030 --> 00:06:35.120
about the state of your machine.

00:06:35.280 --> 00:06:38.100
And there's three really good
reasons why you want to use them.

00:06:38.100 --> 00:06:40.380
The first one is that
they're minimally invasive.

00:06:40.380 --> 00:06:42.660
That is, when you actually use these
to analyze your system,

00:06:42.660 --> 00:06:45.680
you're going to get more of an idea
about how your system or how your

00:06:45.740 --> 00:06:49.400
application is behaving on your computer,
as opposed to how the tool is

00:06:49.520 --> 00:06:52.760
actually affecting how your
app runs on the computer.

00:06:52.760 --> 00:06:55.840
The second thing is that because
all these are command line tools,

00:06:55.840 --> 00:06:58.310
that means you can
actually run them remotely.

00:06:58.460 --> 00:07:01.820
If you don't want to upset the screen,
if the machine is hung,

00:07:01.960 --> 00:07:04.680
you can log in via telnet and
you can run these commands

00:07:04.680 --> 00:07:06.200
and find out what's going on.

00:07:07.660 --> 00:07:09.950
And finally,
because all of the command line tools are

00:07:09.950 --> 00:07:13.530
basically just text-based applications,
you can use any of the

00:07:13.650 --> 00:07:17.380
UNIX filter commands to convert
the data into a format you like.

00:07:17.440 --> 00:07:19.820
If you want to see, let's say,
every 10 seconds how much memory

00:07:19.820 --> 00:07:22.620
your application is using,
you could easily write a little script

00:07:22.620 --> 00:07:25.780
that goes around and every 10 seconds
pulls one of the tools to actually

00:07:25.780 --> 00:07:27.840
find out how much memory is being used.

00:07:27.840 --> 00:07:30.990
And so in this way,
you can sort of roll your own without

00:07:30.990 --> 00:07:33.480
having to do anything too deep.

00:07:33.610 --> 00:07:35.550
The first tool that I list
here is actually PS,

00:07:35.550 --> 00:07:36.990
which is a standard UNIX tool.

00:07:36.990 --> 00:07:38.480
It stands for Process Status.

00:07:38.560 --> 00:07:41.940
It gives you information about what
processes are running on the machine.

00:07:41.940 --> 00:07:45.360
It tells you about how much
memory is used and so on.

00:07:45.380 --> 00:07:49.370
I'm not actually going to talk about that
because it ends up--because there's some

00:07:49.370 --> 00:07:51.950
other things that might be more useful.

00:07:52.400 --> 00:07:56.640
Okay, so let's take a look first at TOP.

00:07:56.700 --> 00:07:59.620
TOP is something that you can
use instead of PS to find out

00:07:59.760 --> 00:08:02.910
about the state of your system.

00:08:03.290 --> 00:08:07.400
It's actually something that
comes--there's some implementations on

00:08:07.400 --> 00:08:09.580
top on other Unix like operating systems.

00:08:09.640 --> 00:08:12.820
This one was specifically written by us.

00:08:12.820 --> 00:08:16.480
And what it does, as you can see,
is it gives you a list of the processes,

00:08:16.480 --> 00:08:19.440
ranked in basically
newest to oldest order.

00:08:19.440 --> 00:08:22.420
At the top it gives you information
about the status of the system.

00:08:22.420 --> 00:08:24.410
It starts out saying
what the load average is,

00:08:24.550 --> 00:08:27.820
what the average number of
runnable tasks happens to be.

00:08:27.940 --> 00:08:30.690
It tells you about how
many processes there are,

00:08:30.690 --> 00:08:33.490
how much memory,
the line starting with memory

00:08:33.610 --> 00:08:36.920
shows you how much memory is wired,
that is dedicated to

00:08:36.940 --> 00:08:38.640
uses of the kernel only.

00:08:38.640 --> 00:08:41.720
The second line shows you
how much memory is active,

00:08:41.720 --> 00:08:43.800
inactive, blah, blah, blah.

00:08:43.800 --> 00:08:46.100
Below that you can see how
much virtual memory there is.

00:08:46.100 --> 00:08:49.640
There's currently 688 megabytes of
memory allocated to virtual memory.

00:08:49.640 --> 00:08:52.100
Not all that may actually
have memory in it,

00:08:52.100 --> 00:08:55.780
but that's how much the virtual
memory system thinks it has.

00:08:55.780 --> 00:08:58.340
In addition,
it shows how many pages have been

00:08:58.340 --> 00:09:01.790
put out to disk and brought back
in with the page ins and page outs.

00:09:02.000 --> 00:09:03.600
And the number in parentheses--
The number in parentheses

00:09:03.660 --> 00:09:05.720
there is important because
that's actually a delta.

00:09:05.860 --> 00:09:08.590
That shows you how many pages
have changed in the last second.

00:09:08.600 --> 00:09:14.550
Why don't we run QuickTime Player so we
actually get something interesting here.

00:09:21.510 --> 00:09:23.640
And what we'll do is we'll
simply run QuickTime Player.

00:09:23.640 --> 00:09:25.800
And let's think of a
hypothetical problem.

00:09:25.800 --> 00:09:29.770
Let's assume that we're working on
the player and we're finding that the

00:09:29.890 --> 00:09:31.180
frame rate doesn't seem high enough.

00:09:31.270 --> 00:09:33.670
And we're not sure whether
we're correctly throttling

00:09:33.670 --> 00:09:36.320
it down for some reason or if
we're not getting enough CPU.

00:09:36.490 --> 00:09:38.520
This is actually not a
problem as far as I know,

00:09:38.520 --> 00:09:39.670
but it's a good story.

00:09:41.260 --> 00:09:45.080
So what we can see here is on the
second line you see LaunchCFMApp.

00:09:45.210 --> 00:09:47.130
Here's a bit of trivia.

00:09:47.370 --> 00:09:50.340
The QuickTime Player is
actually a PEF executable.

00:09:50.340 --> 00:09:53.380
It's in the same format that
you would have seen on Mac OS 9.

00:09:53.380 --> 00:09:55.840
And as a result,
whenever you try to execute

00:09:55.840 --> 00:09:59.190
one of those on Mac OS X,
the LaunchCFMApp serves as a wrapper

00:09:59.190 --> 00:10:01.420
to actually load that into memory.

00:10:01.430 --> 00:10:03.670
And so that's why you don't
actually see QuickTime Player in

00:10:03.670 --> 00:10:04.690
the list of processes.

00:10:04.730 --> 00:10:07.790
And what we can see is that the
QuickTime Player is using about

00:10:07.790 --> 00:10:09.860
25 to 30 percent of the CPU.

00:10:09.860 --> 00:10:13.150
We get the elapsed time,
the number of threads,

00:10:13.210 --> 00:10:15.940
the number of Mach ports,
which is an abstraction for

00:10:15.940 --> 00:10:19.800
communicating between the kernel
and the system and the application.

00:10:19.850 --> 00:10:24.240
Other interesting things
include the RPrivate,

00:10:24.240 --> 00:10:28.940
which is the amount of private memory,
memory that is only for this particular

00:10:29.000 --> 00:10:30.450
running version of the application.

00:10:30.450 --> 00:10:34.940
That's different from all the memory
that's needed that can be shared between

00:10:34.940 --> 00:10:38.780
multiple copies of QuickTime Player if
we had multiple ones running or other

00:10:38.830 --> 00:10:41.210
applications using the same libraries.

00:10:41.220 --> 00:10:42.880
So the RPrivate is a good
measure of how much memory your

00:10:42.880 --> 00:10:45.620
application is using right now.

00:10:47.330 --> 00:10:51.340
and our shared shows how much memory is
being used for the application itself,

00:10:51.530 --> 00:10:53.500
which can be shared,
and all the libraries

00:10:53.660 --> 00:10:56.100
which can be shared,
and memory mapped files,

00:10:56.100 --> 00:10:59.980
and all those things that aren't
only dedicated to one application.

00:11:00.000 --> 00:11:03.540
Now, we can look at this and we can say,
gee, we're only using about

00:11:03.540 --> 00:11:04.700
a third of the CPU.

00:11:04.700 --> 00:11:06.160
What's going on here?

00:11:06.160 --> 00:11:07.630
Are we spending too much time on disk?

00:11:07.690 --> 00:11:08.700
Are we throttling?

00:11:08.700 --> 00:11:10.860
Well,
one thing we can do is we can look down

00:11:10.860 --> 00:11:14.960
the list and we can understand whether
our application is doing anything bizarre

00:11:14.960 --> 00:11:17.140
that depends on the rest of the system.

00:11:17.140 --> 00:11:19.200
And in this case, there certainly is.

00:11:19.280 --> 00:11:22.690
We see at the bottom, actually,
and a lot of people probably, well,

00:11:22.770 --> 00:11:25.270
can everyone see the
line starting with 50,

00:11:25.290 --> 00:11:26.200
WindowMan?

00:11:28.130 --> 00:11:29.830
Never mind, I'll just read it out.

00:11:29.950 --> 00:11:33.240
Down towards the bottom,
there's a line that says "50,

00:11:33.240 --> 00:11:35.990
window manager." It's
using about 20% of the CPU,

00:11:35.990 --> 00:11:39.550
it's run for about 51 seconds and so on.

00:11:39.660 --> 00:11:42.150
What's happening here is that the
window manager is actually responsible

00:11:42.150 --> 00:11:44.360
for doing the drawing to the hardware.

00:11:44.360 --> 00:11:48.350
And so all the applications end
up talking to the window manager.

00:11:48.450 --> 00:11:52.660
And so it's not too surprising to see
execution divided between the two,

00:11:52.660 --> 00:11:56.090
because the QuickTime player
is spending some of its time

00:11:56.090 --> 00:11:59.470
getting all the images ready,
it's shipping them off to the window

00:11:59.550 --> 00:12:02.640
manager and then the window manager
blasts them up on the screen.

00:12:02.640 --> 00:12:05.770
So we're seeing that
we're spending about 50,

00:12:05.880 --> 00:12:11.350
60% of the CPU actually doing meaningful
computation and filling up the CPU.

00:12:11.370 --> 00:12:13.600
What's happening with
the rest of the time?

00:12:13.600 --> 00:12:15.480
Well, there's some other
tools that we could do.

00:12:15.550 --> 00:12:19.100
Let's go on the hypothesis that maybe
there's something going on with the disk.

00:12:19.100 --> 00:12:22.860
There's another tool, and before I end,
one of the nice things about

00:12:22.860 --> 00:12:22.860
Top is that it has a little bit
of a different type of tool.

00:12:22.860 --> 00:12:22.860
It's called the "Top" tool.

00:12:22.860 --> 00:12:22.860
And it's a little bit more
complex than the "Top" tool.

00:12:22.860 --> 00:12:22.860
It's a little bit more
complex than the "Top" tool.

00:12:22.860 --> 00:12:22.860
It's a little bit more
complex than the "Top" tool.

00:12:22.860 --> 00:12:23.030
It's a little bit more
complex than the "Top" tool.

00:12:23.030 --> 00:12:23.640
It's a little bit more
complex than the "Top" tool.

00:12:23.640 --> 00:12:25.200
But what it does is that it
has a huge number of extra

00:12:25.330 --> 00:12:26.360
modes and features hidden.

00:12:26.420 --> 00:12:27.580
Please check the man page.

00:12:27.580 --> 00:12:30.830
There's probably some view that's
perfect for a performance problem

00:12:30.830 --> 00:12:32.930
you're trying to track down.

00:12:32.940 --> 00:12:35.460
But I'm not going to show them all.

00:12:35.480 --> 00:12:36.830
If we're trying to go to the file system,
though,

00:12:36.830 --> 00:12:38.880
and understand how we're using that,
there's another command

00:12:38.880 --> 00:12:40.470
that might be useful.

00:12:40.640 --> 00:12:45.220
And that's called "fsusage".
With "fsusage" we name either--name

00:12:45.220 --> 00:12:47.970
an application or we name a process.

00:12:47.970 --> 00:12:49.380
And we hit return.

00:12:49.780 --> 00:12:53.540
Actually, just do--let's just do a
little quick time play,

00:12:53.560 --> 00:12:55.600
or actually do everything.

00:13:01.020 --> 00:13:02.260
Well,
what we're going to do is we're going

00:13:02.260 --> 00:13:05.220
to get a huge amount of information,
and if Scott hits the space bar,

00:13:05.220 --> 00:13:05.930
we'll start seeing it.

00:13:06.000 --> 00:13:09.840
And what you're seeing here are all the
accesses to the disk that are going on.

00:13:10.020 --> 00:13:14.120
So you're actually seeing the
file system calls being performed.

00:13:14.120 --> 00:13:19.000
And we can see what was being done,
like read or write, or page ins,

00:13:19.000 --> 00:13:23.540
page outs, doing the status of a disk,
that sort of thing.

00:13:23.540 --> 00:13:26.710
We find out how much time was
elapsed and whether it actually had

00:13:26.800 --> 00:13:30.040
to give up the CPU to another task
to let that transaction finish.

00:13:31.000 --> 00:13:33.340
And the application responsible.

00:13:33.400 --> 00:13:35.840
If Scott actually widens that window,
we'll get some more information.

00:13:35.930 --> 00:13:43.680
We'll know exactly which file handle
was accessing that and how many...

00:13:45.830 --> 00:13:46.940
There we go.

00:13:46.940 --> 00:13:50.820
It'll actually say what file handle it
was in the process and how many bytes.

00:13:50.820 --> 00:13:53.870
And what we can see here is that the
QuickTime player is getting chunks

00:13:53.980 --> 00:13:56.700
of 16,000 bytes or 32,000 bytes.

00:13:56.700 --> 00:13:58.940
And we don't see any cases where
it's having to wait too long.

00:13:58.940 --> 00:14:06.460
So that probably means we're not
having to do anything too weird

00:14:06.460 --> 00:14:06.620
with the disk and we're not waiting
for stuff to come off the disk.

00:14:06.620 --> 00:14:06.620
Maybe.

00:14:07.750 --> 00:14:11.390
Another thing that we could
check out is we could ask about

00:14:11.390 --> 00:14:12.430
how the memory is laid out.

00:14:12.530 --> 00:14:14.600
Are we using a lot of
malloc space and the like?

00:14:14.640 --> 00:14:17.820
Or just if we were curious
about how applications are

00:14:17.820 --> 00:14:21.390
laid out in memory in Mac OS X,
we might want to have some sort

00:14:21.400 --> 00:14:22.920
of a tool for visualizing that.

00:14:23.440 --> 00:14:26.790
And there's another command
line tool called VM Map.

00:14:27.150 --> 00:14:30.380
And what we can do is name VM Map.

00:14:30.400 --> 00:14:32.950
We can specify the process ID.

00:14:34.250 --> 00:14:39.950
or the name of the task,
and VM map will give us a listing of all

00:14:39.950 --> 00:14:43.240
the regions of memory where they start,
how much space they are.

00:14:43.240 --> 00:14:46.440
It'll actually start off telling us only
the readable regions or the non-writable,

00:14:46.440 --> 00:14:48.930
and then it will tell us the
writable ones at the end.

00:14:49.040 --> 00:14:54.120
And what we can see here of interest is
on the first line we see a symbolic name,

00:14:54.200 --> 00:14:54.740
page zero.

00:14:54.860 --> 00:14:57.240
We see a starting address, which is zero.

00:14:57.330 --> 00:14:58.780
It's four kilobytes.

00:14:58.830 --> 00:15:01.850
Then we see the permissions,
which is in the Unix style octal.

00:15:01.850 --> 00:15:05.300
And everybody knows how I had a redactal,
of course.

00:15:05.630 --> 00:15:06.500
Oh my.

00:15:06.740 --> 00:15:09.310
Saying that page zero is
actually zero slash zero,

00:15:09.310 --> 00:15:11.670
which means that it's non-readable,
non-writable.

00:15:11.670 --> 00:15:14.950
That's the thing that's saving
us from doing page zero accesses.

00:15:14.980 --> 00:15:21.520
If you try to dereference a pointer
which actually has the value 12,

00:15:21.520 --> 00:15:23.670
you'll know about it.

00:15:23.670 --> 00:15:25.080
You'll be able to catch
those immediately.

00:15:25.080 --> 00:15:25.080
You're not going to have to worry about
strange memory corruptions and the like.

00:15:25.390 --> 00:15:30.290
Below that we can see the
application starting at address 1000.

00:15:30.340 --> 00:15:32.740
There's a couple places that
are cut off with the RD,

00:15:32.740 --> 00:15:36.180
which are guard pages, which are, again,
non-readable,

00:15:36.180 --> 00:15:41.320
non-writable pages at the end of
stacks for the various threads,

00:15:41.600 --> 00:15:47.510
so that if you fill the stack,
it's not going to crash the system,

00:15:47.780 --> 00:15:48.590
or it's not going to trash memory,
it's simply going to

00:15:48.590 --> 00:15:48.590
crash when it hits that.

00:15:48.770 --> 00:15:53.800
and you can see all the libraries
starting at address 4130 and going down.

00:15:53.860 --> 00:15:56.770
And you can see the names of the
files that are being loaded as

00:15:56.870 --> 00:15:59.380
libraries along the right hand side.

00:16:00.220 --> 00:16:03.160
If this is too small, don't worry,
try it at home.

00:16:03.460 --> 00:16:06.320
Hopefully it will make perfect sense.

00:16:06.330 --> 00:16:09.510
If we go down a little further,
we'll actually see the writable regions.

00:16:09.590 --> 00:16:13.260
And here we can start seeing things
like the malloc allocated regions.

00:16:13.430 --> 00:16:17.660
And so we can find out which
pages malloc was placed at.

00:16:17.660 --> 00:16:20.260
And most of the malloc
buffers were actually placed

00:16:20.260 --> 00:16:21.980
right below the application.

00:16:22.590 --> 00:16:25.250
Another tool that might be useful
is we might be asking ourselves,

00:16:25.340 --> 00:16:27.640
well,
is the application running slow because

00:16:28.170 --> 00:16:32.290
we're doing some obnoxious system
call that's just hanging forever?

00:16:32.380 --> 00:16:35.290
And there's another tool called SC Usage.

00:16:37.420 --> 00:16:41.800
and what SC Usage will do is it's
going to look for all the Mach system

00:16:41.920 --> 00:16:44.040
calls going down into the kernel.

00:16:44.040 --> 00:16:47.530
It will tell us how fast or which
ones we were calling off and

00:16:47.640 --> 00:16:49.250
how much time we were spending.

00:16:49.440 --> 00:16:54.250
What you can see here is some information
about how often the app got preempted,

00:16:54.250 --> 00:17:00.490
how often the CPU gave
execution time to somebody else.

00:17:00.490 --> 00:17:01.740
We can find

00:17:03.070 --> 00:17:09.140
The second section shows us
how much time we spent idle and

00:17:09.140 --> 00:17:12.440
how much time we spent busy.

00:17:12.440 --> 00:17:16.070
What we're seeing there is that we're
spending a lot of our time in user

00:17:16.070 --> 00:17:19.960
mode running the application and a
fair amount of time waiting in the app,

00:17:19.960 --> 00:17:22.790
probably because we're doing
a lot of disk accesses.

00:17:22.890 --> 00:17:26.130
Below that, we find the most popular
system calls being done,

00:17:26.320 --> 00:17:29.630
and we find we're actually spending
a lot of time on semaphore wait

00:17:29.630 --> 00:17:31.850
and Mach message overwrite trap.

00:17:32.900 --> 00:17:36.320
Okay, you might say, gee, that's weird.

00:17:36.320 --> 00:17:39.140
Maybe we're locked on a semaphore.

00:17:39.170 --> 00:17:40.310
That's a very good guess.

00:17:40.310 --> 00:17:42.630
Unfortunately,
it's not completely true because on

00:17:42.630 --> 00:17:46.170
a lot of applications on Mac OS X,
there will be usually one or two

00:17:46.170 --> 00:17:50.100
threads that are basically waiting
for something really bad to happen.

00:17:50.100 --> 00:17:51.930
They've sent a message
off to the system saying,

00:17:51.930 --> 00:17:53.580
let me know when something bad happens.

00:17:53.580 --> 00:17:56.610
And they just sit there on
Mach message overwrite trap,

00:17:56.640 --> 00:18:00.410
which is send off a message,
overwrite the buffer when it comes back,

00:18:00.410 --> 00:18:02.240
wait until we get a message back.

00:18:02.900 --> 00:18:05.940
And nothing ever comes back,
and so they're constantly waiting.

00:18:05.940 --> 00:18:08.710
So understanding that those are
having huge wait times doesn't

00:18:08.710 --> 00:18:10.200
necessarily buy us anything.

00:18:10.200 --> 00:18:13.310
However, in some cases,
understanding we're spending lots of

00:18:13.310 --> 00:18:17.390
time doing semaphore signals may tell us
something about how our app's running,

00:18:17.390 --> 00:18:19.710
that we're spending too much
time actually waiting on

00:18:20.150 --> 00:18:21.780
critical sections or something.

00:18:21.780 --> 00:18:25.960
Let's see, what else do we want to show?

00:18:25.960 --> 00:18:28.970
I guess that's about it.

00:18:31.760 --> 00:18:33.760
Okay,
so those are the command line tools.

00:18:33.760 --> 00:18:37.050
Everyone who had covered their heads
because they were afraid of them can now

00:18:37.250 --> 00:18:42.170
come back up because we're actually going
to look at things that look nice and

00:18:42.180 --> 00:18:45.070
that don't use any nasty technologies.

00:18:45.370 --> 00:18:47.370
So the next thing I'm going
to show you are some graphical

00:18:47.430 --> 00:18:50.600
tools that tend to give you a
little higher level information.

00:18:50.600 --> 00:18:53.740
They don't give you quite the immediacy,
but hopefully will help you

00:18:53.890 --> 00:18:55.500
understand what's going on.

00:18:55.500 --> 00:18:59.270
The first of these is called MallocDebug,
and the point of MallocDebug is

00:18:59.330 --> 00:19:03.340
to help you understand how your
application is using heap memory.

00:19:03.960 --> 00:19:07.840
So what it does is for every
allocation that your app is doing,

00:19:07.990 --> 00:19:10.820
it will keep track of how
much memory was created,

00:19:10.820 --> 00:19:14.840
where that memory was created,
and will give you a way of seeing what's

00:19:14.840 --> 00:19:17.050
currently allocated in the system.

00:19:17.120 --> 00:19:18.720
It's really good for
answering questions like,

00:19:18.870 --> 00:19:21.650
how much heap memory is
my application using?

00:19:21.750 --> 00:19:23.130
Am I using 500K?

00:19:23.130 --> 00:19:25.020
Am I using 10 megabytes?

00:19:25.020 --> 00:19:27.460
Are there any places where I'm
using large chunks of memory?

00:19:27.580 --> 00:19:31.040
Am I allocating 3 megabyte chunks
for some array that I don't realize?

00:19:31.160 --> 00:19:33.890
Are there places where I'm
overrunning or underrunning buffers,

00:19:33.890 --> 00:19:36.440
preparing,
trying to trash somebody else's memory,

00:19:36.440 --> 00:19:39.320
which is a great way to
make subtle memory bugs?

00:19:39.320 --> 00:19:41.180
Are there cases where
I might be leaking memory,

00:19:41.380 --> 00:19:43.210
where I'm allocating things
but forgetting to free them?

00:19:43.220 --> 00:19:43.220
So the first thing I'm going to
show you is a little bit of a graph.

00:19:43.220 --> 00:19:43.220
So I'm going to show you a graph
of the memory that I'm allocating.

00:19:43.220 --> 00:19:43.220
And then I'm going to show you a graph
of the memory that I'm allocating.

00:19:43.220 --> 00:19:43.260
And then I'm going to show
you a graph of the memory

00:19:44.600 --> 00:19:46.750
In all cases,
what Malloc is going to try to do is give

00:19:46.750 --> 00:19:52.890
you information about how you're creating
memory using Malloc as the core idea.

00:19:52.900 --> 00:19:57.280
And unlike some other
tools you might be using,

00:19:57.280 --> 00:20:00.030
what it does is it tries to
give you a snapshot of how much

00:20:00.030 --> 00:20:03.490
memory you're using right now,
as opposed to showing you memory that

00:20:03.810 --> 00:20:06.870
you'd allocated before that's been freed,
for example.

00:20:06.940 --> 00:20:08.160
So it's only a snapshot.

00:20:08.160 --> 00:20:11.300
The way MallocDebug does
this is kind of cool.

00:20:11.700 --> 00:20:14.960
What it does is it has its own version
of Malloc that's been instrumented,

00:20:14.960 --> 00:20:17.830
and it slides that version of
Malloc under your application

00:20:17.830 --> 00:20:18.860
when you launch it.

00:20:18.940 --> 00:20:22.910
And as a result,
it makes it very easy to use.

00:20:23.000 --> 00:20:25.770
You don't have to worry about
recompiling your code to make sure

00:20:25.770 --> 00:20:27.260
that this new library is used.

00:20:27.720 --> 00:20:31.650
You don't have to change any source.

00:20:31.780 --> 00:20:32.620
You don't have to do anything.

00:20:32.620 --> 00:20:33.450
It just works.

00:20:33.460 --> 00:20:35.530
And that's one of the
advantages of these tools.

00:20:35.560 --> 00:20:38.940
In addition, because we have our
own version of Malloc,

00:20:39.230 --> 00:20:42.460
what we can do is when Malloc is called,
we can actually keep track of

00:20:42.480 --> 00:20:45.930
the call stack and find out
how you actually got there.

00:20:46.140 --> 00:20:49.040
And because every other
allocator in the system,

00:20:49.040 --> 00:20:52.530
whether that's in core foundation,
whether that's in Carbon,

00:20:52.710 --> 00:20:55.070
whether that's in Objective C,
all of those eventually

00:20:55.130 --> 00:20:56.090
go through Malloc.

00:20:56.200 --> 00:20:59.110
So this is a single point to actually
find out how you're allocating memory.

00:20:59.120 --> 00:21:01.520
So let's do a demo here.

00:21:06.230 --> 00:21:09.790
So here's the MallocDebug window.

00:21:09.960 --> 00:21:13.810
So Scott can either select the
application by pressing the Browse

00:21:13.810 --> 00:21:18.450
button and going through a browser,
or can choose it off a drop-down list.

00:21:18.660 --> 00:21:21.520
He can then press Launch to
actually start it up.

00:21:24.000 --> 00:21:26.470
Okay, and why don't we update it to
see what the current status is.

00:21:26.540 --> 00:21:30.890
And we find that in this case
we're launching simple text.

00:21:30.890 --> 00:21:35.100
We find that we're actually allocating
about 700K to get to the point where

00:21:35.100 --> 00:21:37.590
we've actually started the application.

00:21:37.820 --> 00:21:42.480
and what we see in the window
below is basically a call tree.

00:21:42.510 --> 00:21:44.580
It shows us all the
ways that we got down,

00:21:44.610 --> 00:21:47.500
or that we ended up
going to malloc_debug,

00:21:47.500 --> 00:21:48.960
or calling malloc.

00:21:48.960 --> 00:21:51.110
So for example,
from start we called underbar start

00:21:51.170 --> 00:21:56.140
and eventually we got down to main
after going through some system stuff.

00:21:56.140 --> 00:21:58.410
Main called malloc through
about four functions,

00:21:58.410 --> 00:22:02.630
either calling init_cursor or
do_initialize or do_event_loop or

00:22:02.700 --> 00:22:06.020
some strange hexadecimal value there.

00:22:06.020 --> 00:22:07.940
Actually,
let's go through that hexadecimal thing.

00:22:07.940 --> 00:22:12.740
So 4110 is actually
another little secret.

00:22:12.740 --> 00:22:16.180
As you may know,
411 is information and this

00:22:16.190 --> 00:22:19.760
actually is the place where you
go to load dynamic libraries.

00:22:19.770 --> 00:22:22.030
That's where you get information
about how to call other functions.

00:22:22.030 --> 00:22:24.420
Cute, huh?

00:22:24.420 --> 00:22:26.680
So what happens is that when
your application launches,

00:22:26.790 --> 00:22:29.360
it tries to load all these other
libraries and as a result it has

00:22:29.420 --> 00:22:34.340
to call the initialization routines
for each of these libraries.

00:22:34.340 --> 00:22:37.660
So if you look down
inside that call domain,

00:22:37.790 --> 00:22:40.580
that implicit call that you didn't
actually have to make in your code,

00:22:40.760 --> 00:22:43.940
the initialize_highlevel_toolbox,
initialize_quickdraw,

00:22:43.940 --> 00:22:46.620
initialize_carbon_core all
happened automatically.

00:22:46.620 --> 00:22:49.940
We noticed that they actually
allocated about 400K.

00:22:49.940 --> 00:22:52.680
So a good deal of the memory that was
allocated during launch was actually

00:22:52.680 --> 00:22:54.130
in these initialization routines.

00:22:56.220 --> 00:23:01.100
Now going from the top down
is sometimes interesting,

00:23:01.100 --> 00:23:05.810
especially when you know your code,
but sometimes it's interesting to see

00:23:05.810 --> 00:23:10.160
why or how you got down to Malloc and
what was happening down at the other end.

00:23:10.160 --> 00:23:12.100
We can not only show
the tree from this side,

00:23:12.100 --> 00:23:15.740
but we can also invert it,
and we can change the style of the tree.

00:23:15.770 --> 00:23:18.490
And so now what we're doing is,
rather than looking at how we

00:23:18.600 --> 00:23:21.580
got from Maine and called down
through the program down to Malloc,

00:23:21.580 --> 00:23:24.570
we're going to look at Malloc
and we're going to look at the

00:23:24.720 --> 00:23:28.060
ways that Malloc was called.

00:23:28.060 --> 00:23:30.590
So for example, if we select Malloc.

00:23:30.930 --> 00:23:32.860
These are the ways
that Malloc was called.

00:23:32.900 --> 00:23:35.790
AllocateMemory called Malloc,
AddUsage called Malloc,

00:23:35.790 --> 00:23:37.700
GlobalCacheAllocate called Malloc.

00:23:37.750 --> 00:23:39.950
And for each of these,
we can get some idea about

00:23:40.040 --> 00:23:41.610
how Malloc was called.

00:23:42.000 --> 00:23:47.030
Let's go through one little example here.

00:23:47.240 --> 00:23:48.900
Actually, let's do the Valloc one.

00:23:48.900 --> 00:23:54.220
It's too bad we didn't actually
have the better example,

00:23:54.290 --> 00:23:56.380
but this will do.

00:23:56.380 --> 00:23:59.770
We can select Valloc and we see
that we actually have a 65,000 byte

00:23:59.840 --> 00:24:02.850
chunk that was allocated through
one of the calls down that way.

00:24:02.930 --> 00:24:05.870
And Valloc was called by allocate memory,
which was called by

00:24:05.870 --> 00:24:11.000
allocate zeroed memory,
which was called by new handle.

00:24:11.000 --> 00:24:12.790
I would prefer to have a
better example than this,

00:24:12.880 --> 00:24:14.450
but this one will do.

00:24:14.500 --> 00:24:17.190
Here's another little bit of trivia.

00:24:17.190 --> 00:24:20.060
What's happening here
is that in Mac OS X,

00:24:20.080 --> 00:24:23.610
when you create a new handle and
create the memory attached to that,

00:24:23.610 --> 00:24:25.440
handles are actually suballocated.

00:24:25.620 --> 00:24:28.910
There's actually a big block
of space that's been subdivided

00:24:28.910 --> 00:24:30.560
into handle size spaces.

00:24:30.680 --> 00:24:34.160
And somebody's got to create that memory.

00:24:34.160 --> 00:24:36.660
So what happens is the first
time that you call new handle,

00:24:36.660 --> 00:24:39.660
it actually goes and creates
the suballocated field.

00:24:39.660 --> 00:24:42.180
So that 64K chunk is the
place for handles to live.

00:24:44.170 --> 00:24:45.930
May not make sense,
it's a system level idea,

00:24:45.930 --> 00:24:48.720
but the idea is that we can actually
track down from this call graph

00:24:49.120 --> 00:24:52.690
what the point of that memory was,
especially if you're looking at

00:24:52.790 --> 00:24:56.460
your own code and not looking at
the innards of the memory manager.

00:24:56.460 --> 00:25:04.450
What we can also do is we can actually go
to something a little simpler like Calic.

00:25:04.660 --> 00:25:07.700
and why don't you select one
of the buffers down below.

00:25:07.720 --> 00:25:11.030
No, actually, yeah, select a buffer.

00:25:11.180 --> 00:25:13.280
So, you also get a list of
all the allocations.

00:25:13.280 --> 00:25:15.180
So,
not only do you find how you got there,

00:25:15.310 --> 00:25:20.620
but you find a list of the buffers that
were allocated by calling down that way.

00:25:20.620 --> 00:25:23.930
It'll tell you the address
that it was allocated at,

00:25:23.940 --> 00:25:25.580
the size, and so on.

00:25:25.750 --> 00:25:28.270
And if we double click on it,
we get a memory dump.

00:25:28.560 --> 00:25:30.080
So, we can actually look at memory.

00:25:30.110 --> 00:25:30.980
This is really useful.

00:25:31.180 --> 00:25:35.800
If you find that you're allocating 6,000
bytes somewhere and you're curious why,

00:25:35.830 --> 00:25:38.330
now you can double click on it and
take a look and try to understand

00:25:38.330 --> 00:25:41.080
why that memory was allocated.

00:25:41.080 --> 00:25:44.910
Now, one other thing that you can do,
actually press the back button, and

00:25:45.700 --> 00:25:54.750
is, as I said, actually,
can we run the leaked example?

00:25:55.580 --> 00:25:58.080
So one of the other things that
you can do with MallocDebug,

00:25:58.080 --> 00:26:00.310
as I mentioned,
is that you can actually do a bit

00:26:00.310 --> 00:26:04.540
of analysis to find cases where
you overran or underran buffers.

00:26:04.540 --> 00:26:09.020
And these are really nasty bugs
because they tend to be intermittent,

00:26:09.020 --> 00:26:11.610
they tend to be really subtle,
they tend to only occur after the

00:26:11.720 --> 00:26:14.420
program's been running for a while
and then suddenly it crashes.

00:26:14.430 --> 00:26:16.520
And so you'd like to track these down.

00:26:16.590 --> 00:26:19.830
What MallocDebug does,
and you can do update and

00:26:19.830 --> 00:26:23.780
then let's do an inverted,
or actually go to trashed.

00:26:24.110 --> 00:26:27.300
is you can change the mode
from showing all the currently

00:26:27.300 --> 00:26:30.360
allocated regions to only showing
what are called the trashed ones.

00:26:30.390 --> 00:26:32.560
And if Scott actually selects Start,
we see that there's two

00:26:32.560 --> 00:26:34.350
buffers that are trashed.

00:26:34.350 --> 00:26:38.510
That is where the--where we
know we overran or underran it.

00:26:38.700 --> 00:26:42.100
And the way we know that is we
actually have some guard words on each

00:26:42.100 --> 00:26:44.910
side and when those get overwritten
we know that we did something bad.

00:26:44.910 --> 00:26:47.470
If Scott actually double
clicks on one of those,

00:26:47.660 --> 00:26:51.160
you can see the ten bytes,
the ten zeroes.

00:26:51.160 --> 00:26:54.850
Malloc debug then--what it does
is when it allocates space,

00:26:54.910 --> 00:26:57.520
it puts two special
strings at either side.

00:26:57.590 --> 00:27:00.940
It puts the hex value beef dead
at the back end of the buffer

00:27:00.940 --> 00:27:03.870
and then if Scott presses back,
you'll see that it puts

00:27:03.870 --> 00:27:05.220
dead beef at the beginning.

00:27:07.090 --> 00:27:09.540
and the last word down at the bottom.

00:27:09.550 --> 00:27:12.900
And so when those words change,
MallocDebug knows you've

00:27:12.900 --> 00:27:14.000
done something bad.

00:27:14.210 --> 00:27:18.500
It actually also,
in an extremely user-friendly fashion,

00:27:18.500 --> 00:27:20.240
ends up putting a message
out to the console,

00:27:20.240 --> 00:27:22.000
yes, we need to fix this.

00:27:22.000 --> 00:27:25.630
But it will actually give you some
indications of when it actually

00:27:25.630 --> 00:27:28.000
notices that something gets trashed.

00:27:28.010 --> 00:27:31.410
So keep the console window
open if you can when you run

00:27:31.510 --> 00:27:33.840
MallocDebug so you can see this.

00:27:33.840 --> 00:27:37.000
In addition,
let's try leak analysis next.

00:27:38.240 --> 00:27:39.810
There's also the idea of leak analysis.

00:27:39.810 --> 00:27:44.340
For those of you who
have used ZoneRanger,

00:27:44.340 --> 00:27:47.150
ZoneRanger has an idea about leaks.

00:27:47.150 --> 00:27:53.000
What it considers a leak to be is any
memory that you allocate but then don't

00:27:53.000 --> 00:27:56.800
deallocate in some operation that should
have actually been cleaned itself up.

00:27:56.830 --> 00:27:59.760
For example, opening a document and
closing the document.

00:27:59.860 --> 00:28:02.210
If you have more memory than
you--if you have more memory

00:28:02.210 --> 00:28:05.720
allocated than you started out with,
you've probably got a leak.

00:28:05.720 --> 00:28:09.220
MallocDebug goes off the definition
that's more like Purifies,

00:28:09.350 --> 00:28:14.090
that any memory that's not--that cannot
be reached by a pointer probably can't

00:28:14.090 --> 00:28:16.570
be referenced and therefore it is leaked.

00:28:16.570 --> 00:28:18.860
And so we go with that definition.

00:28:18.860 --> 00:28:22.340
And the way you can do leak detection
is that you can start it up and you

00:28:22.450 --> 00:28:25.420
can change the selection mode to leaks.

00:28:25.420 --> 00:28:29.920
And what it does is MallocDebug will
now scan through memory looking for

00:28:29.920 --> 00:28:31.760
anything that looks like a pointer.

00:28:31.890 --> 00:28:33.850
And if it's a pointer,
it goes and it sees whether

00:28:34.030 --> 00:28:34.380
that's a pointer or not.

00:28:34.380 --> 00:28:36.890
And it's a pointer to the
beginning of a malloc region or to

00:28:37.010 --> 00:28:41.910
a handle which points to a memory
region or a couple other options.

00:28:41.920 --> 00:28:46.230
And if it does find a pointer like that,
it marks the block as reachable.

00:28:46.250 --> 00:28:48.340
If it doesn't find it,
then it says it's not reachable

00:28:48.340 --> 00:28:50.570
and it's probably a leak.

00:28:50.600 --> 00:28:53.080
And after a little while,
it comes back and it shows us

00:28:53.250 --> 00:28:56.240
only these--the allocations
that would have been leaked.

00:28:56.240 --> 00:28:58.570
And what we can see here if
Scott goes down a little further or

00:28:58.820 --> 00:29:01.100
actually go from the inverted side.

00:29:02.170 --> 00:29:04.790
is we find about 182,000 bytes.

00:29:04.870 --> 00:29:10.070
This is not completely true unfortunately
because there's a few cases of false

00:29:10.070 --> 00:29:16.740
positives in system routines and let
me just step through a couple of them.

00:29:16.740 --> 00:29:20.290
The calls to malloc from the global
cache allocate which are from

00:29:20.360 --> 00:29:23.190
ATS alloc are cases in the font code
and this is a case where they're

00:29:23.190 --> 00:29:26.470
doing some interesting things with
pointers that this doesn't detect.

00:29:26.590 --> 00:29:28.360
And we can basically ignore those out.

00:29:28.560 --> 00:29:29.450
Yes, this is ugly.

00:29:29.540 --> 00:29:32.920
There's an internal version I hope
to roll out at Apple real soon now,

00:29:33.140 --> 00:29:35.780
like in the next week,
to get around this problem.

00:29:35.960 --> 00:29:37.450
It didn't make it on the CD.

00:29:37.450 --> 00:29:40.940
Hopefully we can put it on the website.

00:29:40.940 --> 00:29:42.370
But for now,
hopefully this will give you some hints

00:29:42.390 --> 00:29:46.400
on how you can actually look at this
stuff and then laugh at me every time.

00:29:46.620 --> 00:29:49.610
So we can select global cache
allocate and we can say,

00:29:49.610 --> 00:29:51.200
okay, this is immaterial.

00:29:51.420 --> 00:29:54.680
Let's select the prune menu,
the path item and we can pull that

00:29:54.680 --> 00:29:56.350
out so we don't have to look at it.

00:29:56.350 --> 00:29:58.920
So we're only looking at the
things we think to be leaks.

00:29:59.030 --> 00:30:02.330
Similarly,
the allocation with new block turns

00:30:02.350 --> 00:30:05.340
out to be a case in icon services.

00:30:05.450 --> 00:30:06.980
Prune that out.

00:30:07.110 --> 00:30:09.170
The case in malloc.

00:30:10.210 --> 00:30:11.230
is the handles.

00:30:11.250 --> 00:30:14.040
We can prune that out.

00:30:14.040 --> 00:30:16.720
It will be better in the future, yes.

00:30:16.720 --> 00:30:19.090
And eventually, you get down to the point
where the only things left are

00:30:19.090 --> 00:30:20.480
things that are probably leaks.

00:30:20.530 --> 00:30:22.630
There's some documentation
on this in the release note.

00:30:22.830 --> 00:30:24.400
Like I said,
we'll have a better version that

00:30:24.400 --> 00:30:25.720
will do this a little better.

00:30:25.910 --> 00:30:30.820
But this is a way to start looking for
memory that might not be reachable.

00:30:30.920 --> 00:30:34.220
And if you find memory that you allocated
in your application that's leaked,

00:30:34.490 --> 00:30:37.000
this is probably a good indication
that you might have a problem.

00:30:38.860 --> 00:30:39.790
Okay, let's move on.

00:30:39.800 --> 00:30:40.770
Hmm?

00:30:40.800 --> 00:30:44.550
Well, that's probably a good idea.

00:30:44.760 --> 00:30:47.310
How are we doing on time?

00:30:47.360 --> 00:30:50.160
So one of the things that's
really nice about ZoneRanger's

00:30:50.160 --> 00:30:54.730
idea is this idea of you allocate
memory or you create an object,

00:30:54.730 --> 00:30:59.900
you destroy it, and hopefully the amount
of memory doesn't change.

00:30:59.900 --> 00:31:02.650
That idea is really nice
for understanding the effect

00:31:02.650 --> 00:31:04.080
of certain operations.

00:31:04.080 --> 00:31:05.860
We can do something similar to that.

00:31:05.860 --> 00:31:08.860
What you can do is you can say,
let's say we're having a slow down

00:31:08.860 --> 00:31:14.680
when we start typing in simple text
and we're curious why that's happening.

00:31:14.680 --> 00:31:18.080
So we can try to see if we're doing a
lot of allocations or a lot of work.

00:31:18.080 --> 00:31:23.340
What we can do is we can select,
go back to all, or actually that's fine.

00:31:23.340 --> 00:31:26.680
What we can do is mark a point
in time by pressing mark.

00:31:27.250 --> 00:31:30.970
We can then go and type into
the buffer to do the event

00:31:31.100 --> 00:31:33.080
that we're trying to watch.

00:31:33.980 --> 00:31:39.150
And then what we can do is go over to
MallocDebug and we can change to the,

00:31:39.710 --> 00:31:43.880
show only the new nodes,
show only the newly allocated memory.

00:31:43.890 --> 00:31:46.890
And we'll find,

00:31:48.200 --> 00:31:48.990
after a moment.

00:31:49.250 --> 00:31:50.850
Did we actually press mark?

00:31:51.030 --> 00:31:52.040
Oh there we go.

00:31:52.040 --> 00:31:55.620
We find that we allocated 400,000 bytes.

00:31:55.640 --> 00:31:57.360
Oh my gosh what are we doing?

00:31:57.360 --> 00:31:59.750
Actually there's a good reason for this.

00:31:59.750 --> 00:32:01.440
But it's a great example.

00:32:01.590 --> 00:32:03.380
If Scott actually,
actually we should go to

00:32:03.380 --> 00:32:05.880
standard for this one because
it's an easier way to see.

00:32:05.880 --> 00:32:07.160
This is why it's exploratory.

00:32:07.270 --> 00:32:09.680
You have to sort of
dash around and explore.

00:32:09.680 --> 00:32:12.680
And it makes it an
interesting thing to demo.

00:32:13.040 --> 00:32:15.310
What we end up finding is
that most of that memory,

00:32:15.310 --> 00:32:19.620
if we descend down the biggest path,
is inside called voices thread.

00:32:19.620 --> 00:32:21.820
And what's actually happening
here is that simple text has

00:32:21.900 --> 00:32:25.680
been voice enabled so that it
actually can do text to speech.

00:32:25.680 --> 00:32:28.010
And so what happens is that
to speed up the load time,

00:32:28.010 --> 00:32:31.670
which is something good that you should
care about for performance of course.

00:32:31.770 --> 00:32:35.200
That was one of those
performance minutes.

00:32:35.200 --> 00:32:37.560
In order to speed that up what
you want to do is you want to

00:32:37.560 --> 00:32:40.760
make sure that you do as little as
possible when you're launching the

00:32:40.850 --> 00:32:42.680
app and maybe do the rest later.

00:32:42.680 --> 00:32:43.810
And this is the case
where they're doing that.

00:32:43.950 --> 00:32:46.680
The they don't bother to actually
load the voices until a few seconds

00:32:46.700 --> 00:32:49.060
after the windows actually appear.

00:32:49.240 --> 00:32:51.330
And one of the things that they have
to do is load in the voices and do

00:32:51.450 --> 00:32:54.400
all the data structures to make sure
that text to speech actually works.

00:32:54.560 --> 00:32:56.520
So that's okay that it's delaying things.

00:32:56.520 --> 00:33:00.420
And that's a cute trick to actually
improve the performance of your apps.

00:33:00.450 --> 00:33:00.930
Thank you very much.

00:33:06.910 --> 00:33:09.800
Let's go through a few little moments.

00:33:09.800 --> 00:33:16.800
One thing I'll point out is the idea of
the call trees may be a little weird.

00:33:16.800 --> 00:33:19.240
As I said,
the idea is that every time you

00:33:19.340 --> 00:33:22.380
do a malloc allocation we get
sort of this call stack of all

00:33:22.380 --> 00:33:23.800
the ways you got down to malloc.

00:33:23.960 --> 00:33:27.770
They can be thought of as those vertical
lines on top or horizontal lines on top.

00:33:27.900 --> 00:33:31.560
When you look at the normal tree,
what it does is it collapses together

00:33:31.690 --> 00:33:36.110
all the things at the main end of the
tree to overlap the similarities so that

00:33:36.110 --> 00:33:37.800
you can see how it starts to diverge.

00:33:37.800 --> 00:33:40.620
And notice this is a tree so
we don't pull it back together

00:33:40.620 --> 00:33:41.930
again at the other end.

00:33:42.400 --> 00:33:44.400
Similarly, when you do the inverted tree,
we do the opposite.

00:33:44.500 --> 00:33:48.680
We start collapsing things together at
the Malloc end to find the ways that

00:33:48.680 --> 00:33:51.680
we called Malloc that were similar,
so we can start seeing where

00:33:51.680 --> 00:33:53.120
things diverge from that end.

00:33:56.080 --> 00:33:58.900
Also, to cover,
as another one of the little

00:33:58.900 --> 00:34:02.500
issues we should probably cover,
just to explain MallocDebug,

00:34:02.650 --> 00:34:05.000
there's also the question
of leak detection.

00:34:05.070 --> 00:34:09.730
As I said,
the way that leak detection works

00:34:09.830 --> 00:34:15.770
is it goes scanning through memory
looking for pointers to buffers

00:34:15.770 --> 00:34:15.770
that are allocated by Malloc.

00:34:16.540 --> 00:34:18.330
There are cases where the
leaks won't be noticed.

00:34:18.400 --> 00:34:21.300
This is just part of the problem
with doing leak detection.

00:34:21.390 --> 00:34:23.200
In some cases,
there may be a value in memory

00:34:23.200 --> 00:34:24.460
that looks like a pointer.

00:34:24.550 --> 00:34:29.220
You may have 5F000000 because
you've got a null terminated string.

00:34:29.380 --> 00:34:33.160
In those cases, the

00:34:33.560 --> 00:34:36.820
A random value and a pointer to
something that's actually a Malloc

00:34:36.820 --> 00:34:38.900
buffer might not be distinguishable.

00:34:38.900 --> 00:34:41.480
You can't tell why that
stuff was put into memory.

00:34:41.480 --> 00:34:44.220
And as a result,
you might get cases where things that

00:34:44.250 --> 00:34:46.260
are actually leaks may not be leaks.

00:34:46.260 --> 00:34:48.490
Similarly,
there's some cases where there may

00:34:48.560 --> 00:34:50.380
be leaks that don't get detected.

00:34:50.380 --> 00:34:53.870
This garbage detection
algorithm is relatively simple.

00:34:53.870 --> 00:34:57.880
Anyone who's played with them
should immediately see some holes.

00:34:58.510 --> 00:35:01.500
One of those is that if you have a
list of circularly linked structures,

00:35:01.520 --> 00:35:05.670
so you've got a big loop of things,
every object points to something else.

00:35:05.670 --> 00:35:07.470
And therefore,
all of them are referenced,

00:35:07.600 --> 00:35:09.300
and so they'll never
be detected as a leak.

00:35:09.380 --> 00:35:11.820
Similarly,
a tree of data structures will only

00:35:11.940 --> 00:35:15.250
have the root of it unreferenced,
and therefore you may only see,

00:35:15.370 --> 00:35:17.910
let's say,
a 20-byte leak when you're actually

00:35:18.060 --> 00:35:19.940
leaking a huge data structure.

00:35:19.940 --> 00:35:23.090
So always pay attention to
even small leaks just in case.

00:35:23.100 --> 00:35:25.750
Now,
I mentioned that there were a number of

00:35:25.750 --> 00:35:28.420
problems with various system routines.

00:35:28.420 --> 00:35:29.920
We're doing clever things with pointers.

00:35:29.920 --> 00:35:32.240
In general,
what was happening is that our

00:35:32.240 --> 00:35:36.400
definition of leak is that there's a
pointer to the beginning of a buffer.

00:35:36.400 --> 00:35:38.040
And if there's a pointer to
the beginning of the buffer,

00:35:38.040 --> 00:35:38.700
it's reachable.

00:35:38.700 --> 00:35:41.100
However, in some cases, in your own code,
in others,

00:35:41.210 --> 00:35:44.290
people will have pointers into the
middle of a buffer for various reasons

00:35:44.290 --> 00:35:47.480
and no pointers to the beginning,
usually because they're trying to hide

00:35:47.540 --> 00:35:49.520
secret information at the beginning.

00:35:49.520 --> 00:35:51.530
In those cases,
MallocDebug is not going to be able

00:35:51.620 --> 00:35:53.100
to do leak detection correctly.

00:35:53.100 --> 00:35:54.750
The next version may help.

00:35:57.590 --> 00:36:01.390
Another issue to keep in mind is
my favorite question or comment.

00:36:01.720 --> 00:36:05.060
People constantly come to me and say,
"This tool is horrible.

00:36:05.190 --> 00:36:09.590
I use it and all my application
ever does is crash."

00:36:10.750 --> 00:36:13.270
This is actually the same problem that
people had with even better bus error.

00:36:13.470 --> 00:36:16.200
You know, gee, every time I use this,
my machine crashes.

00:36:16.230 --> 00:36:17.700
Why don't you write better software?

00:36:17.700 --> 00:36:23.490
I love hearing that story
from the guy who wrote that.

00:36:23.760 --> 00:36:26.780
But what's happening is that MallocDebug
is trying to tell you something.

00:36:26.780 --> 00:36:28.950
It's trying to tell you
something extremely loudly.

00:36:28.950 --> 00:36:30.780
You're doing bad things with pointers.

00:36:30.780 --> 00:36:35.530
There are a number of cases of
operations that can cause subtle

00:36:35.530 --> 00:36:38.110
and intermittent memory bugs.

00:36:38.160 --> 00:36:41.250
Examples of those include overrunning
or underrunning buffers so you

00:36:41.250 --> 00:36:44.620
trash somebody else's buffer,
or freeing memory and then continuing

00:36:44.620 --> 00:36:48.060
to use it and modify the values even
though somebody else has now got that

00:36:48.060 --> 00:36:50.310
memory and you're trashing their values.

00:36:51.140 --> 00:36:53.490
MallocDebug tries to
solve both those problems.

00:36:53.530 --> 00:36:56.920
The first thing it does is that
every time that you free memory,

00:36:56.940 --> 00:36:59.570
it overwrites that memory with 7F
to make sure that there's absolute

00:36:59.730 --> 00:37:03.050
garbage in there and that hopefully
if your app tries to read that,

00:37:03.100 --> 00:37:04.080
you'll notice.

00:37:04.300 --> 00:37:07.940
The second thing is that you saw
that overruns were guarded with dead

00:37:07.940 --> 00:37:09.810
beef and underruns with beef dead.

00:37:09.810 --> 00:37:13.400
And so if you end up
trying to access beyond,

00:37:13.480 --> 00:37:14.870
you're going to get a bogus value also.

00:37:14.870 --> 00:37:17.990
As a result, you may see your program
behaving strangely,

00:37:17.990 --> 00:37:21.160
you may see odd values in
variables that shouldn't be there,

00:37:21.270 --> 00:37:25.270
or you may find your application crashing
when trying to access address 7f,

00:37:25.270 --> 00:37:26.590
7f, 7f, 7f.

00:37:27.460 --> 00:37:32.470
When you get crashes on your app in
MallocDebug that don't happen normally,

00:37:32.610 --> 00:37:36.330
the first thing to do is that
there is a preferences panel that

00:37:36.330 --> 00:37:39.180
has the clear freed memory option.

00:37:39.340 --> 00:37:40.590
Turn that off and try it again.

00:37:40.800 --> 00:37:43.940
If your app runs, then you're doing bad
things with freed memory.

00:37:44.000 --> 00:37:46.830
What you can then do is run
the program inside GDB using

00:37:46.830 --> 00:37:48.570
MallocDebug's special library.

00:37:48.890 --> 00:37:52.340
There's documentation on this
in the release notes and the

00:37:52.340 --> 00:37:56.160
debugger will drop you off exactly
where you should pay attention.

00:37:57.230 --> 00:38:01.680
And the final bit of information
about MallocDebug is questions

00:38:01.780 --> 00:38:04.100
about taking its advice.

00:38:04.100 --> 00:38:07.890
Once again, MallocDebug is primarily a
tool for exploring your data.

00:38:07.940 --> 00:38:11.400
It's a really good tool for the
writer to actually look because

00:38:11.500 --> 00:38:14.770
the writer understands their
own code and may be able to say,

00:38:14.920 --> 00:38:18.330
"Gee, that's odd." There's still
uses for this in testing.

00:38:18.380 --> 00:38:20.060
If you have cases where
you're leaking memory,

00:38:20.150 --> 00:38:21.780
if you've got block
underruns or overruns,

00:38:21.780 --> 00:38:25.380
or you're referencing freed memory,
that's a red flag that

00:38:25.490 --> 00:38:27.500
there's something to be fixed.

00:38:27.500 --> 00:38:30.150
In terms of exploring,
I can't give you very good

00:38:30.150 --> 00:38:31.980
details about how to explore.

00:38:32.000 --> 00:38:34.340
Basically,
go off and see what's out there.

00:38:34.340 --> 00:38:36.420
See if you've got any
really big allocations.

00:38:36.420 --> 00:38:39.700
See if you're allocating a lot of really
small things that you didn't expect.

00:38:39.700 --> 00:38:40.740
Look for odd cases.

00:38:40.800 --> 00:38:42.620
Look for patterns.

00:38:42.620 --> 00:38:45.820
The best advice I can give you that's
really concrete is I tend to find it

00:38:45.890 --> 00:38:49.100
much more useful to use the inverted
graph rather than the standard one.

00:38:49.260 --> 00:38:51.840
But that may be because I tend to look
at the system libraries a lot more.

00:38:52.340 --> 00:38:55.910
So, hopefully you'll find this useful.

00:38:56.660 --> 00:39:00.700
The second tool that I'd like to
show is a tool called Sampler.

00:39:00.700 --> 00:39:04.750
And you can think of this
as a really cheap profiler.

00:39:04.760 --> 00:39:09.770
What Sampler does is every 20
milliseconds or every 50 milliseconds

00:39:09.860 --> 00:39:13.150
it stops the program and it says,
hey, where are you running?

00:39:13.160 --> 00:39:16.020
And it actually gets the call stack
for all of the threads that are

00:39:16.020 --> 00:39:18.660
currently running so it knows the
current point that it's executing.

00:39:18.660 --> 00:39:21.900
Like malloc.debug,
it provides basically the call

00:39:21.900 --> 00:39:25.020
stack so that you can browse
through those and try to find out

00:39:25.020 --> 00:39:26.120
exactly how things are running.

00:39:27.520 --> 00:39:27.820
Um,

00:39:29.590 --> 00:39:34.160
Now, the reason why Sampler is good is
that it's extremely easy to perform.

00:39:34.160 --> 00:39:35.820
You use it, it works.

00:39:35.820 --> 00:39:38.810
You don't need to recompile
your application like

00:39:38.880 --> 00:39:40.700
you would with profiling.

00:39:40.700 --> 00:39:43.240
You don't need to have special
profiled versions of libraries.

00:39:43.240 --> 00:39:46.620
You don't need to make
any changes to the code.

00:39:46.620 --> 00:39:47.890
It just works.

00:39:47.890 --> 00:39:51.040
You can run this on any of the
applications on the system.

00:39:51.040 --> 00:39:52.710
And in fact,
all these tools are on the CD,

00:39:52.710 --> 00:39:54.310
so please go out and play with them.

00:39:56.630 --> 00:39:59.350
And in addition,
because it's only stopping the program

00:39:59.350 --> 00:40:03.530
every 20 milliseconds or 50 milliseconds,
hopefully it will be doing very little

00:40:03.530 --> 00:40:06.820
to the application's running behavior,
as opposed to, let's say,

00:40:06.880 --> 00:40:08.200
doing full profiling.

00:40:08.210 --> 00:40:12.460
And so this may be a way to get really
cheap data to find performance problems

00:40:12.830 --> 00:40:14.930
that should be explored in more depth.

00:40:16.160 --> 00:40:18.520
I'll also point out, just in passing,
there's also a command

00:40:18.520 --> 00:40:22.020
line tool called Sample,
where you just type sample and the

00:40:22.020 --> 00:40:25.640
process ID or the application name
and how many seconds of sample.

00:40:25.800 --> 00:40:26.740
And so this is a really
good tool for that.

00:40:26.740 --> 00:40:26.740
And it's really good for
basically cutting and pasting

00:40:26.740 --> 00:40:26.740
and putting into a bug report.

00:40:27.740 --> 00:40:27.740
And it's really good for
basically cutting and pasting

00:40:27.740 --> 00:40:27.740
and putting into a bug report.

00:40:27.740 --> 00:40:30.760
And it will put out a text-based
report saying where it found

00:40:30.910 --> 00:40:32.440
the program's execution.

00:40:32.440 --> 00:40:35.570
This is really good if your
application hangs or if it seems slow,

00:40:35.570 --> 00:40:38.580
so that you can actually track down
what the performance problem is.

00:40:38.610 --> 00:40:40.670
And it's really good for
basically cutting and pasting

00:40:40.670 --> 00:40:42.960
and putting into a bug report.

00:40:42.960 --> 00:40:45.010
So let's do a demo.

00:40:48.930 --> 00:40:50.580
Okay, so here's the Sampler UI.

00:40:50.600 --> 00:40:53.760
Once again, we can select an application,
we can launch it,

00:40:53.760 --> 00:40:57.530
and let's actually change the
sampling rate to 20 milliseconds.

00:40:57.970 --> 00:41:02.400
And then we can launch and sample it
and we can see how simple text launches

00:41:02.400 --> 00:41:04.610
and what's going on during that.

00:41:05.300 --> 00:41:08.200
And so eventually the
window will come up.

00:41:08.200 --> 00:41:09.800
There we go.

00:41:09.800 --> 00:41:11.100
And we can stop sampling.

00:41:11.370 --> 00:41:15.640
And now we get a set of call
stacks showing what's going on.

00:41:15.640 --> 00:41:17.910
We'll start off with the extra threads.

00:41:18.010 --> 00:41:23.600
So thread two, so there were 155 samples,
155 times where it stopped the program

00:41:23.710 --> 00:41:25.780
that it found execution in thread two.

00:41:25.780 --> 00:41:28.920
And all those were basically
in Mach message overwrite trap.

00:41:29.020 --> 00:41:31.740
Okay, so it's basically sitting
there waiting for a message.

00:41:31.760 --> 00:41:32.420
We can ignore that.

00:41:32.490 --> 00:41:35.240
So we can actually add that
to excluded stacks down at the

00:41:35.410 --> 00:41:37.280
bottom to get it out of our view.

00:41:37.280 --> 00:41:40.140
Thread one's pretty much the same way,
except for about eight samples it's

00:41:40.440 --> 00:41:42.200
basically sitting there doing nothing.

00:41:42.220 --> 00:41:43.790
So we can ignore that one also.

00:41:45.350 --> 00:41:49.460
And then in thread zero,
if we click on the 1000 and

00:41:49.550 --> 00:41:52.850
start and start and main,
now we can start finding

00:41:52.850 --> 00:41:54.430
out what was going on.

00:41:54.500 --> 00:41:57.570
So we had 158 samples at 20 milliseconds.

00:41:57.660 --> 00:42:02.200
That's, what, 10, 3 seconds.

00:42:02.520 --> 00:42:06.180
Most of the time was being
spent in do event loop.

00:42:06.210 --> 00:42:08.150
The wait next event is pretty trivial.

00:42:08.150 --> 00:42:13.480
That's just when it's spinning,
so we can ignore that.

00:42:13.480 --> 00:42:13.480
And the last six,

00:42:14.320 --> 00:42:17.570
Samples were actually-- you
want to go down to that?

00:42:17.570 --> 00:42:19.840
Actually,
you can see the call stack on the

00:42:19.840 --> 00:42:21.580
far side showing the entire tree.

00:42:21.730 --> 00:42:23.960
So you can see that we
were in doEventLoop,

00:42:23.960 --> 00:42:26.630
which called handleEvent,
which called eventually

00:42:26.910 --> 00:42:28.260
resume the current event.

00:42:28.730 --> 00:42:31.220
So that was doing the setup for the app.

00:42:31.220 --> 00:42:34.230
This is a relatively
uninteresting example.

00:42:34.230 --> 00:42:36.800
Feel free to go off
and try your own code,

00:42:36.800 --> 00:42:39.540
and hopefully you'll find some very
interesting things about how your app

00:42:39.540 --> 00:42:39.540
is running and where it's finding it.

00:42:40.450 --> 00:42:43.240
There's also a way that you can invert
the call graph so you can look from the

00:42:43.240 --> 00:42:48.270
bottom up and you can find the common
functions that it found it running in.

00:42:48.340 --> 00:42:50.990
If you find that your
functions are listed down here,

00:42:51.070 --> 00:42:54.900
that probably means you have a tight loop
and you're spending all your time there.

00:42:54.900 --> 00:42:58.410
Often you'll find that the application
is stopped in system calls when it

00:42:58.410 --> 00:43:02.540
was sampled and that's why you're
seeing calls to string compare or to

00:43:02.540 --> 00:43:05.350
Mach message overwrite trap and the like.

00:43:07.430 --> 00:43:11.540
Okay,
there's one big caveat I should mention.

00:43:11.540 --> 00:43:13.480
Although I've said that this
is a cheap method of profiling,

00:43:13.480 --> 00:43:16.340
remember,
Sampler is not providing comprehensive,

00:43:16.360 --> 00:43:17.220
accurate data.

00:43:17.220 --> 00:43:18.300
It's sampling.

00:43:18.300 --> 00:43:19.580
It's a statistical approach.

00:43:19.580 --> 00:43:21.880
That means that it's not going
to show you all the calls

00:43:21.880 --> 00:43:24.630
that are actually happening,
just the ones that were happening

00:43:24.630 --> 00:43:26.190
when it decided to stop the app.

00:43:27.030 --> 00:43:31.260
Second, the numbers refer to how many
times it found it in that function,

00:43:31.330 --> 00:43:33.660
not how many times that
function was called.

00:43:33.680 --> 00:43:38.380
If we found 150 samples in
some arbitrary function,

00:43:38.380 --> 00:43:41.940
that could mean it was called 150 times,
it could mean it was called once,

00:43:42.110 --> 00:43:45.170
but every time it looked it was in that,
or that function could have been

00:43:45.170 --> 00:43:48.500
called 150,000 times and we just
happened to see it when it was in that.

00:43:48.520 --> 00:43:52.670
If you're trying to get...

00:43:52.810 --> 00:43:55.100
If you have small,
quick executing functions,

00:43:55.100 --> 00:43:57.890
those are going to appear statistically
based on what percentage of the

00:43:57.950 --> 00:43:59.440
time they actually take to execute.

00:43:59.440 --> 00:44:01.440
So with longer sample runs
and smaller sample times,

00:44:01.490 --> 00:44:03.540
you'll start getting better
data and you'll start seeing

00:44:03.670 --> 00:44:05.080
the smaller functions appear.

00:44:05.080 --> 00:44:10.280
In addition, because this is sampling,
there's the question of sampling error.

00:44:10.280 --> 00:44:14.050
What are we going to see
when we stop the program?

00:44:14.060 --> 00:44:18.080
Well, because the way Sampler works
is it takes control of the

00:44:18.080 --> 00:44:22.650
CPU and the other process stops,
that means that the other application

00:44:22.720 --> 00:44:24.620
is going to be at preemption points.

00:44:24.620 --> 00:44:28.830
And so wherever the operating
system decides is a good time to

00:44:28.850 --> 00:44:31.840
stop the thing is going to be where
you're going to see it in Sampler.

00:44:31.840 --> 00:44:35.550
That could either be because it
ran out of time and the operating

00:44:35.550 --> 00:44:38.350
system took control away,
or it could be because the

00:44:38.430 --> 00:44:41.900
application made a system call
and the operating system said,

00:44:41.900 --> 00:44:43.520
you're never going to
finish this in time.

00:44:43.520 --> 00:44:45.280
I'm just going to give control
to someone else while you're

00:44:45.280 --> 00:44:46.350
waiting for this disk access.

00:44:46.360 --> 00:44:48.920
And so you may see disk accesses.

00:44:49.020 --> 00:44:52.120
you may see some of the system calls much
more frequently than they really appear.

00:44:55.350 --> 00:44:57.490
Okay, let's not worry about ObjectAlloc.

00:44:57.740 --> 00:45:00.990
Actually, let's just demo it quickly.

00:45:02.810 --> 00:45:05.140
So another tool that's
available is ObjectAlloc.

00:45:05.140 --> 00:45:09.360
This is a tool that was originally
intended for Objective-C,

00:45:09.480 --> 00:45:12.380
but still can be useful
for programming in Carbon,

00:45:12.470 --> 00:45:15.100
for programming in Basics, in just C.

00:45:15.140 --> 00:45:18.240
The idea is that this is trying
to be a lot more like ZoneRanger.

00:45:18.460 --> 00:45:22.880
That it's trying to give you ideas
about how fast you're adding data,

00:45:23.050 --> 00:45:25.840
how much data you're using,
how quickly it's increasing.

00:45:25.920 --> 00:45:29.510
And what it does is it
shows you a histogram.

00:45:29.730 --> 00:45:32.750
What it does is it divides
up all the allocations based

00:45:32.750 --> 00:45:35.960
on the class of the object.

00:45:35.960 --> 00:45:39.150
You can see how many
CFDictionary's you had,

00:45:39.270 --> 00:45:42.850
how many NSStrings,
and in the case of just

00:45:42.980 --> 00:45:46.280
plain malloc allocations,
it just says malloc-46 for

00:45:46.280 --> 00:45:48.600
46 byte malloc allocations.

00:45:48.620 --> 00:45:52.740
What the histograms show you is,
first for the darkest bar,

00:45:52.740 --> 00:45:55.590
it shows you the current
number of objects of that

00:45:55.790 --> 00:45:57.600
type existing in the system.

00:45:57.790 --> 00:46:02.370
The next lighter represents the
maximum number of objects of this

00:46:02.370 --> 00:46:05.730
type that ever existed at once,
and the final bar shows you how many

00:46:05.730 --> 00:46:07.590
objects of that type have been allocated.

00:46:07.600 --> 00:46:09.720
Watching this run can
give you an idea about,

00:46:09.720 --> 00:46:11.910
in general,
how your app might be behaving and

00:46:11.910 --> 00:46:15.380
might give you some hints about objects
that you're creating a huge number

00:46:15.380 --> 00:46:17.600
of that might be performance problems.

00:46:18.600 --> 00:46:24.600
There are some other features in this
and other features in the other tools.

00:46:24.600 --> 00:46:26.600
Please go play with them.

00:46:33.140 --> 00:46:36.070
Let's do one example here.

00:46:36.210 --> 00:46:38.920
Let's talk about how we'd
actually debug something for real.

00:46:38.920 --> 00:46:42.430
The example I'm going to
use is one of my own things,

00:46:42.440 --> 00:46:45.000
so that I can be very embarrassed.

00:46:45.310 --> 00:46:48.950
Specifically,
it's the MallocDebug leak detection.

00:46:48.960 --> 00:46:51.600
What happened was that when
I actually implemented or added

00:46:51.690 --> 00:46:55.570
support for Carbon memory,
I found that leak detection got much,

00:46:55.630 --> 00:46:58.410
much, much, much, much slower,
about a 10 times slowdown.

00:46:58.410 --> 00:46:59.720
This was very bad.

00:46:59.740 --> 00:47:02.690
However, I'd only changed the
algorithm in small ways,

00:47:02.740 --> 00:47:05.300
so I was extremely confused
about what was going on.

00:47:05.300 --> 00:47:09.480
What I needed to do was I needed
to use multiple tools to understand

00:47:09.480 --> 00:47:11.350
exactly what was going on.

00:47:12.300 --> 00:47:14.060
This is something you're
probably going to find,

00:47:14.060 --> 00:47:16.000
that you really need to play
around and look from different

00:47:16.000 --> 00:47:20.320
angles to find out why something's
behaving less than optimally.

00:47:20.320 --> 00:47:23.170
The first thing I did was I ran Sampler.

00:47:23.180 --> 00:47:26.120
I had Sampler look at my
process when it was doing,

00:47:26.230 --> 00:47:29.910
or at the application,
when it was doing the leak detection.

00:47:29.920 --> 00:47:33.930
What I found was that most of
the time it was actually spending

00:47:33.990 --> 00:47:38.090
in a call known as VM region,
which is a system-level call

00:47:38.300 --> 00:47:42.220
that will tell you about
what parts of virtual memory,

00:47:42.420 --> 00:47:45.230
for a specific process,
are actually mapped in,

00:47:45.360 --> 00:47:48.360
which ones don't exist,
whether they're readable, writable.

00:47:48.470 --> 00:47:50.840
This was important for
being able to identify,

00:47:50.840 --> 00:47:53.470
when I was checking a pointer,
figuring out whether there

00:47:53.470 --> 00:47:55.870
was anything at the other end,
so that I could read that

00:47:55.870 --> 00:47:57.920
data without knowing that the
system was going to crash.

00:47:58.380 --> 00:48:00.290
Actually,
the application was going to crash,

00:48:00.290 --> 00:48:02.300
because the system won't crash in OS X.

00:48:03.230 --> 00:48:05.880
Thank God.

00:48:06.360 --> 00:48:09.480
The solution was that this
data didn't change during the

00:48:09.480 --> 00:48:11.220
time I was doing analysis.

00:48:11.340 --> 00:48:14.650
And so I found I could actually cache
that and I increased the speed by

00:48:14.840 --> 00:48:19.380
about a third better than it was.

00:48:19.380 --> 00:48:21.410
The second thing was that
I started listening to my machine.

00:48:21.420 --> 00:48:23.180
I used my ears, another tool.

00:48:23.180 --> 00:48:25.620
And I found that the
disk was chattering away.

00:48:25.620 --> 00:48:27.550
Using Top,
I looked and I found that I was

00:48:27.550 --> 00:48:30.120
swapping about 2,000 pages a minute.

00:48:30.120 --> 00:48:32.490
So my machine was basically spending
all its time throwing pages out

00:48:32.490 --> 00:48:35.380
to disk and bringing them back in.

00:48:35.380 --> 00:48:40.230
This is not very efficient unless
you happen to be a disk drive.

00:48:40.240 --> 00:48:45.720
And what I found was that although it was
spending all that time swapping around,

00:48:45.720 --> 00:48:47.710
all the execution was
being spent in my code.

00:48:47.740 --> 00:48:48.740
It wasn't doing other I/O.

00:48:48.740 --> 00:48:52.020
It was just trying to swap.

00:48:52.120 --> 00:48:55.640
And what it turned out to be after
commenting certain parts of the code

00:48:55.640 --> 00:48:58.960
out was that I was checking for pointers
in places I shouldn't have been,

00:48:58.960 --> 00:49:01.900
in places that were only readable
memory that you couldn't have

00:49:01.900 --> 00:49:03.400
had reasonable pointers in.

00:49:03.460 --> 00:49:04.460
And as a result,
I was searching for pointers in

00:49:04.460 --> 00:49:05.540
places that I shouldn't have been,
in places that were only readable.

00:49:05.540 --> 00:49:07.230
And as a result, I was searching around
in a lot of places.

00:49:07.310 --> 00:49:10.330
And because of the change in algorithm,
suddenly I was looking at a lot

00:49:10.330 --> 00:49:12.280
more pages in random places.

00:49:12.380 --> 00:49:14.330
And instead of sort of linearly
passing through memory and

00:49:14.330 --> 00:49:17.890
looking at only a few places,
I was looking everywhere randomly and

00:49:17.890 --> 00:49:20.480
causing huge performance problems.

00:49:20.480 --> 00:49:23.000
As a result,
what I was able to do was minimize

00:49:23.000 --> 00:49:27.260
the number of out-of-order checks and
tighten up the checks on what I was

00:49:27.260 --> 00:49:29.480
going to look at other pages for.

00:49:29.480 --> 00:49:33.360
And as a result, got the speed up down to
about a factor of two.

00:49:33.390 --> 00:49:34.300
And since then,
I've looked at my algorithm.

00:49:34.300 --> 00:49:38.620
And I've gotten it down to
like only 10 seconds from 30,

00:49:38.620 --> 00:49:40.340
which was pretty cool.

00:49:40.340 --> 00:49:44.180
So the take-home lesson here
is plan to use lots of tools,

00:49:44.210 --> 00:49:46.670
plan to explore lots
of parts of the system,

00:49:46.670 --> 00:49:47.740
and plan to learn a lot of trivia.

00:49:50.770 --> 00:49:51.990
Welcome to the world of performance.

00:49:52.030 --> 00:49:57.670
For those of you that are
planning on porting from Carbon,

00:49:57.760 --> 00:50:02.980
yes, we've heard great testimonials about
people who went off for lunch and

00:50:03.350 --> 00:50:05.680
converted their app over to Carbon.

00:50:05.690 --> 00:50:07.150
That's really good.

00:50:07.150 --> 00:50:10.670
In fact, in a lot of cases that's
probably good enough.

00:50:10.890 --> 00:50:13.480
However, as I gave you examples,
there may be places in your

00:50:13.560 --> 00:50:17.840
code where there actually are
mismatches in the algorithms that

00:50:18.010 --> 00:50:20.590
don't quite match the new world.

00:50:20.660 --> 00:50:22.460
Porting is only going
to be half the work.

00:50:22.520 --> 00:50:24.570
You're going to have to look at the app,
you're going to have to

00:50:24.690 --> 00:50:26.930
understand how it works,
and see if you can find

00:50:26.960 --> 00:50:28.560
any performance problems.

00:50:28.560 --> 00:50:30.390
Plan on using these performance tools.

00:50:30.580 --> 00:50:32.380
Plan on using multiple
tools and exploring,

00:50:32.380 --> 00:50:35.100
as I mentioned before.

00:50:35.100 --> 00:50:37.290
In addition,
remember that one of the things that

00:50:37.290 --> 00:50:41.550
we're getting with Mac OS X is a huge
number of pieces of infrastructure

00:50:41.550 --> 00:50:44.020
that are really going to help us out.

00:50:44.290 --> 00:50:48.680
Plan on looking at them and
deciding what you can actually use.

00:50:48.680 --> 00:50:50.520
Examples include memory mapped files.

00:50:50.520 --> 00:50:55.480
You don't actually have to read
stuff from disk into a buffer.

00:50:55.480 --> 00:50:58.730
The operating system will kindly just
map that file into virtual memory.

00:50:58.730 --> 00:51:01.270
When you try to touch that page,
it will actually map

00:51:01.330 --> 00:51:02.370
it into memory for you.

00:51:02.460 --> 00:51:05.500
You don't actually need to
keep multiple buffers around.

00:51:05.500 --> 00:51:07.740
In fact,
if you try to keep those buffers around,

00:51:07.740 --> 00:51:09.910
you may be being too clever
because the operating system may

00:51:10.010 --> 00:51:13.900
be keeping a copy of the memory
mapped file in your address space.

00:51:13.900 --> 00:51:13.900
Suddenly, you've got two copies.

00:51:13.900 --> 00:51:19.470
Similarly, we now have Pthreads,
a really nice thread implementation.

00:51:19.480 --> 00:51:22.070
These are threads at the
level of the operating system.

00:51:22.070 --> 00:51:25.620
They don't have a lot of overhead
because they're part of the OS.

00:51:25.620 --> 00:51:28.840
Plan on looking at Pthreads and
seeing if you can exploit those.

00:51:28.950 --> 00:51:31.080
Finally,
we also now have the POSIX file I/O.

00:51:31.080 --> 00:51:34.370
There may be cases where that's
much more useful to you than

00:51:34.370 --> 00:51:36.640
the standard Mac OS toolkit.

00:51:36.660 --> 00:51:39.880
Take a look at that and see if that
will actually help you in some cases.

00:51:40.430 --> 00:51:43.230
In addition,
a certain vice president who shall

00:51:43.330 --> 00:51:46.050
remain nameless hacked on a few weekends.

00:51:46.090 --> 00:51:49.080
Excuse me, let me rephrase this.

00:51:49.500 --> 00:51:51.800
Certain people high in the company
happened to be very interested

00:51:51.800 --> 00:51:54.120
in algorithms and happened to
be very interested in Malloc.

00:51:54.160 --> 00:52:00.060
One of the problems on many
Mac OS compiler implementations

00:52:00.060 --> 00:52:02.520
was that the Malloc implementations
used to be really bad.

00:52:02.820 --> 00:52:06.140
And a lot of people have used
sub-allocators instead of going through

00:52:06.140 --> 00:52:09.990
native memory management because they
want extra efficiency or they don't think

00:52:10.060 --> 00:52:11.470
the performance is going to be that hot.

00:52:11.580 --> 00:52:13.770
We have a really nice
implementation of Malloc thanks

00:52:13.770 --> 00:52:15.430
to someone's nights and weekends.

00:52:15.440 --> 00:52:18.750
So think twice about
using sub-allocators.

00:52:18.760 --> 00:52:19.740
Try the new Malloc.

00:52:19.850 --> 00:52:20.640
It's really efficient.

00:52:20.640 --> 00:52:22.550
There's some really cool
new little features in it.

00:52:22.800 --> 00:52:23.600
Go play.

00:52:23.600 --> 00:52:26.960
And finally,
I will repeat again and again and again,

00:52:27.360 --> 00:52:29.020
polling bad, blocking good.

00:52:29.040 --> 00:52:31.220
Don't sit and wait for
something to happen.

00:52:31.220 --> 00:52:32.710
Have the OS.

00:52:32.880 --> 00:52:35.190
Go off and tell you when it's done.

00:52:35.200 --> 00:52:38.250
Have the OS take control away from
you and give it to someone else so

00:52:38.250 --> 00:52:42.340
that other processes can actually
run and you'll have a nice feeling

00:52:42.340 --> 00:52:47.740
of smoothness all through the app
instead of having yours take up CPU.

00:52:49.590 --> 00:52:53.700
And as a final warning,
in a horrible place,

00:52:54.200 --> 00:52:58.370
some of these tools do
work with PEF binaries.

00:52:58.380 --> 00:53:00.470
Some of them don't
necessarily work so well,

00:53:00.470 --> 00:53:01.880
and we want to improve that.

00:53:02.090 --> 00:53:05.520
With MallocDebug and Sampler,
they currently do not

00:53:05.520 --> 00:53:08.360
identify the PEF symbols,
and so you're not going to see the

00:53:08.360 --> 00:53:09.490
symbols in your own application.

00:53:09.500 --> 00:53:13.080
If you're running Mac OS native binaries,
this isn't a problem.

00:53:13.080 --> 00:53:16.380
This is something that
didn't get onto the CD.

00:53:16.580 --> 00:53:21.960
Hopefully, we can actually put it out
on the developer website so

00:53:21.970 --> 00:53:23.370
that everybody can use this.

00:53:23.570 --> 00:53:26.960
But plan that the version on the CD may
not do a good job with PEF binaries,

00:53:26.960 --> 00:53:28.990
that you may not be able to
see much about your program.

00:53:30.790 --> 00:53:36.090
So, to conclude, Mac OS X is really cool,
but the differences between

00:53:36.350 --> 00:53:40.840
it and how you use it to work
has a lot of differences.

00:53:40.840 --> 00:53:43.100
The algorithms you use are
probably going to need to change,

00:53:43.100 --> 00:53:47.230
so take a look at them,
use the performance

00:53:47.230 --> 00:53:51.950
tools to analyze them,
and have a great time with

00:53:52.440 --> 00:53:55.170
native Mac OS X applications.

00:54:00.070 --> 00:54:04.570
If you have questions,
or actually if you want to use the tools,

00:54:04.570 --> 00:54:07.400
they're in /usr/bin for
the command line tools.

00:54:07.400 --> 00:54:11.000
The graphical tools are in
System Developer Applications.

00:54:11.000 --> 00:54:15.000
Documentation is available as man
pages for the command line tools.

00:54:15.000 --> 00:54:18.240
MallocDebug and Sampler
have documentation in them,

00:54:18.380 --> 00:54:21.660
and there's also a nice release
note on MallocDebug explaining

00:54:21.660 --> 00:54:25.000
some of its idiosyncrasies
for this particular release.

00:54:25.000 --> 00:54:28.750
If you've got questions or feedback,
if you send mail to

00:54:28.970 --> 00:54:34.220
macos10/tools/feedback,
it goes to, I believe, the entire group.

00:54:34.220 --> 00:54:36.330
We'd love to hear your comments,
suggestions about other tools

00:54:36.330 --> 00:54:38.750
that are really necessary,
because we're all going to

00:54:38.760 --> 00:54:42.760
learn what's really needed
when moving over to Mac OS X.

00:54:42.760 --> 00:54:46.070
And if you have any other issues,
Godfrey DiGiorgi is our technology

00:54:46.140 --> 00:54:50.230
manager for the Development Tools Group,
and I will bring him up so that he

00:54:50.230 --> 00:54:51.980
can tell you about the other forums.

00:54:51.980 --> 00:54:53.980
Male #1: Oops.

00:54:53.980 --> 00:54:58.230
Male #1: Group.apple.com.

00:55:02.000 --> 00:55:10.510
Okay, we have about 12 minutes for Q&A,
so... Oops.

00:55:12.500 --> 00:55:32.000
[Transcript missing]