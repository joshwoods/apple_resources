WEBVTT

00:00:15.530 --> 00:00:19.910
Ladies and gentlemen,
please welcome the manager-- OK, now.

00:00:19.910 --> 00:00:26.090
As I was saying,
please welcome the manager of

00:00:26.090 --> 00:00:33.160
Mac OS X file systems engineering,
Clark Warner.

00:00:37.620 --> 00:00:40.530
Thank you.

00:00:40.600 --> 00:00:43.160
The stage police are watching me.

00:00:44.150 --> 00:00:50.510
And I've heard from my spies that when
I turn around to jump on the stage,

00:00:50.590 --> 00:00:51.670
something bad's going to happen to me.

00:00:51.830 --> 00:00:53.100
So I have to be careful.

00:00:53.120 --> 00:00:55.070
I've noticed that the stage
is higher this year than last,

00:00:55.100 --> 00:00:57.660
so I think I'm ready.

00:01:08.540 --> 00:01:08.960
Thank you.

00:01:08.960 --> 00:01:10.840
Welcome.

00:01:10.890 --> 00:01:13.450
So we have, we hope,
some really good information

00:01:13.450 --> 00:01:16.840
for you folks whether you're
just a user of Mac OS X,

00:01:16.870 --> 00:01:19.760
whether you're a developer
on top of Mac OS X,

00:01:19.780 --> 00:01:23.240
or whether or not you actually want
to implement a file system or some

00:01:23.530 --> 00:01:25.540
kernel extension of the file system.

00:01:25.550 --> 00:01:29.420
And we hope that by the end of the
session you'll have some vague ideas of

00:01:29.420 --> 00:01:32.740
what you need to do and whether or not
you need to do some of these things.

00:01:32.800 --> 00:01:35.180
So we've got information
on the interfaces,

00:01:35.190 --> 00:01:39.180
we've got some information on
differences between Mac OS X and

00:01:39.180 --> 00:01:43.720
Mac OS 9 that come from our BSD heritage,
and we'll also have a little

00:01:43.780 --> 00:01:46.800
description of some of the differences
between the different volume

00:01:46.800 --> 00:01:49.450
formats that we support in Mac OS X.

00:01:49.730 --> 00:01:54.520
We have a relatively significant
architectural change to tell you about

00:01:54.910 --> 00:01:57.320
in Mac OS X involving our buffer cache.

00:01:57.460 --> 00:02:00.540
And so we're going to spend some time
on that at the end of the session,

00:02:00.580 --> 00:02:03.840
hopefully enough to give you a feel for
what we've done and why we've done it.

00:02:03.990 --> 00:02:08.040
And for those folks who have actually
been to a file system session before,

00:02:08.140 --> 00:02:10.080
then you're going to see
a fair amount of review.

00:02:10.080 --> 00:02:12.510
And we think that's needed
for a couple of reasons.

00:02:12.600 --> 00:02:16.830
One is, as Steve said in the
whole keynote on Monday,

00:02:17.020 --> 00:02:19.510
Mac OS X is becoming very real.

00:02:19.620 --> 00:02:22.400
And there are probably, we expect,
a lot more folks who have not actually

00:02:22.460 --> 00:02:26.320
been introduced to any of the technology
in Mac OS X who are going to need to

00:02:26.320 --> 00:02:29.520
hear some of the stuff we've actually
talked about in previous sessions.

00:02:29.590 --> 00:02:31.240
And also,
given that it's much more real now,

00:02:31.240 --> 00:02:35.320
I imagine there may be a few of you
who perhaps weren't as concerned

00:02:35.320 --> 00:02:38.140
about what you were hearing in
years past and will want to hear

00:02:38.190 --> 00:02:39.760
some of that stuff over again.

00:02:39.910 --> 00:02:42.710
With that having been said,
there will be some new stuff.

00:02:43.280 --> 00:02:45.760
Let me start by giving you a
brief overview of the Core OS.

00:02:45.760 --> 00:02:48.170
If you went to the
Darwin overview session,

00:02:48.170 --> 00:02:49.760
then you've seen this picture already.

00:02:49.790 --> 00:02:51.510
In fact,

00:02:52.570 --> 00:02:55.670
My tech lead has supplied me
with this really handy laser

00:02:55.670 --> 00:03:00.880
pointer so I can point out the
coolest part of the Darwin system.

00:03:01.870 --> 00:03:04.300
And it goes like this.

00:03:04.300 --> 00:03:06.480
We want to tell you a little bit
about the internal architecture

00:03:06.480 --> 00:03:08.350
of the file system as well.

00:03:08.350 --> 00:03:11.720
And this diagram is something you're
going to want to keep in your head

00:03:11.730 --> 00:03:13.880
as we move along in the presentation.

00:03:13.950 --> 00:03:17.240
So, several key things
I need you to remember.

00:03:17.300 --> 00:03:20.360
The application environments, Classic,
Carbon, Cocoa,

00:03:20.410 --> 00:03:23.160
they all have their own way of
interfacing with the file system.

00:03:23.160 --> 00:03:26.020
And we're going to talk about
those interfaces a little bit at

00:03:26.020 --> 00:03:27.800
a later time in the presentation.

00:03:27.860 --> 00:03:32.040
The actual stuff that my group
works on is here inside of Darwin,

00:03:32.040 --> 00:03:33.370
inside of the BSD kernel.

00:03:33.400 --> 00:03:36.560
And we have two interfaces
of particular significance,

00:03:36.560 --> 00:03:39.360
which we'll talk about in
some detail today also,

00:03:39.500 --> 00:03:42.080
as well as a collection of
volume formats that we support.

00:03:42.120 --> 00:03:45.920
So, our basic interface is
a BSD Unix interface.

00:03:45.940 --> 00:03:49.770
But BSD traditionally doesn't
support in its volume formats some

00:03:49.950 --> 00:03:53.120
of the sophisticated features that
have been available in Mac OS X.

00:03:53.140 --> 00:03:56.700
So, namely things like resource forks,
catalog information,

00:03:56.700 --> 00:03:58.120
and that sort of stuff.

00:03:58.170 --> 00:04:01.020
And so we've extended the
BSD interfaces to allow for

00:04:01.250 --> 00:04:04.110
retrieval of that and establishment
of that sort of information.

00:04:04.670 --> 00:04:07.550
Likewise, inside of BSD,
there is a relatively clean

00:04:07.550 --> 00:04:11.120
abstract layer called the
virtual file system layer.

00:04:11.120 --> 00:04:14.370
And that basically separates
the file system dependent from

00:04:14.370 --> 00:04:18.120
file system independent layer
inside of the BSD kernel.

00:04:18.120 --> 00:04:21.600
And if you wanted to develop
a file system yourself because

00:04:21.600 --> 00:04:23.080
you had a new device type,
or if you wanted to develop

00:04:23.080 --> 00:04:23.080
a new system yourself because
you had a new device type,

00:04:23.080 --> 00:04:23.080
or if you wanted to develop
a new system yourself because

00:04:23.080 --> 00:04:23.240
you had a new device type,
or if you wanted to develop a

00:04:23.240 --> 00:04:23.680
new system yourself because you
had a new system yourself because

00:04:23.680 --> 00:04:25.340
you had a new system yourself,
or if you wanted to develop a file

00:04:25.340 --> 00:04:28.470
system plug-in which will intercept all
the file system calls and then deliver

00:04:28.470 --> 00:04:30.080
information to file systems beneath,
this is the interface that

00:04:30.080 --> 00:04:30.080
you would actually write to.

00:04:30.080 --> 00:04:30.710
or if you wanted to develop a file
system plug-in which will intercept all

00:04:30.710 --> 00:04:31.400
the file system calls and then deliver
information to file systems beneath,

00:04:31.400 --> 00:04:32.040
this is the interface that
you would actually write to.

00:04:32.080 --> 00:04:34.990
We're not going to talk in great
detail about the volume formats,

00:04:35.080 --> 00:04:38.070
but suffice it to say that we
support a number of them in Mac OS X.

00:04:38.080 --> 00:04:40.300
We've got support for
the BSD fast file system,

00:04:40.340 --> 00:04:43.070
which was our implementation
of the Unix file system,

00:04:43.160 --> 00:04:46.530
HFS and HFS+, the network file system,
AFS by virtue of the

00:04:46.530 --> 00:04:49.710
Apple share server team,
they've done an

00:04:49.710 --> 00:04:52.930
AFS client implementation,
and more to come.

00:04:53.040 --> 00:04:53.200
Come.

00:04:54.850 --> 00:04:55.820
So here's how we're going to do it.

00:04:55.900 --> 00:04:57.620
Here's our session outline.

00:04:57.640 --> 00:04:59.960
We're going to do a little
status update to let you know

00:04:59.960 --> 00:05:02.320
what's changed in Mac OS X.

00:05:02.350 --> 00:05:05.080
We're going to talk about some of
the new features of the system since

00:05:05.080 --> 00:05:07.700
last we got together at last year's
Worldwide Developers Conference,

00:05:07.720 --> 00:05:11.600
focusing on some of the stuff
we've done for DP4 and for DP3.

00:05:11.620 --> 00:05:13.840
And we're going to, as I mentioned,
talk about some of the interfaces,

00:05:13.910 --> 00:05:18.280
the various ways of getting at file
system functionality in Mac OS X.

00:05:18.300 --> 00:05:20.010
And we're going to talk about some
of those differences I mentioned

00:05:20.050 --> 00:05:23.760
between different volume formats and
arising out of the fact that we have

00:05:23.770 --> 00:05:28.520
a BSD system and a Mac OS X system,
a Mac OS system coexisting.

00:05:28.540 --> 00:05:30.970
And we'll talk,
we'll do a review of the virtual file

00:05:31.070 --> 00:05:34.540
system interfaces themselves to give
you an idea of what they look like

00:05:34.890 --> 00:05:36.860
and how you might make use of them.

00:05:36.880 --> 00:05:39.450
And we're going to talk in some
detail about the changes in our

00:05:39.450 --> 00:05:41.300
buffer caching architecture so that,
again,

00:05:41.300 --> 00:05:44.520
if you're one of those Darwin folks
that is building a new file system,

00:05:44.530 --> 00:05:47.440
you'll have an idea of what sort of
things you'll need to do to change to

00:05:47.520 --> 00:05:50.300
work with our new unified buffer cache.

00:05:50.320 --> 00:05:51.620
The next new feature could be yours.

00:05:51.700 --> 00:05:55.060
We're going to talk a little bit
about Darwin and maybe some ideas

00:05:55.060 --> 00:05:57.580
about what things could happen next.

00:05:59.320 --> 00:06:01.530
So here's our status update.

00:06:01.530 --> 00:06:04.270
Back at this time last year,
we demonstrated to you,

00:06:04.310 --> 00:06:08.520
for the first time ever in public,
a system, a Unix-based system,

00:06:08.690 --> 00:06:12.130
in that case Mac OS X,
booted off of an HFS+ volume.

00:06:12.200 --> 00:06:14.200
And it was the first time ever in public.

00:06:14.200 --> 00:06:16.220
And this year,
we're happy to report that this

00:06:16.250 --> 00:06:19.170
is just a standard humdrum,
boring old part of the system now.

00:06:19.210 --> 00:06:24.200
At DP4, and even back in DP3,
the primary volume format is HFS+.

00:06:24.200 --> 00:06:27.980
The server team has added support
for the Apple FileShare protocol,

00:06:28.090 --> 00:06:30.990
as I've mentioned,
and it's actually in DP4.

00:06:31.320 --> 00:06:34.780
If you look in your demos folder,
you'll find an application

00:06:34.780 --> 00:06:37.110
called TestAFPClient,
and you can use that application

00:06:37.220 --> 00:06:42.200
to mount an Apple Share IP-based
volume on your Mac OS X system.

00:06:42.200 --> 00:06:44.200
Now, it's still a little early
and still a little new.

00:06:44.200 --> 00:06:46.310
You may run into some bugs,
but it does work.

00:06:46.320 --> 00:06:47.190
I have used it.

00:06:47.200 --> 00:06:47.800
We also are well underway with
the development of a new version

00:06:47.840 --> 00:06:48.160
of the Unix-based system.

00:06:48.220 --> 00:06:48.820
We also are well underway with
the development of a new version

00:06:48.820 --> 00:06:49.200
of the Unix-based system.

00:06:49.200 --> 00:06:51.540
with our universal disk
format implementation.

00:06:51.540 --> 00:06:52.940
It's not in the release yet.

00:06:52.940 --> 00:06:56.500
It turns out that UDF is very much
a kitchen sink of file systems.

00:06:56.500 --> 00:06:59.990
It has just about every feature any
file system person could ever imagine

00:06:59.990 --> 00:07:03.140
and so it's quite a challenge to
build support for into the system.

00:07:03.140 --> 00:07:04.520
But we're well underway with that.

00:07:04.520 --> 00:07:08.380
We also have added
support for large files.

00:07:08.460 --> 00:07:12.530
It's not exposed yet for your use
because as a result of the new

00:07:12.590 --> 00:07:15.640
caching architecture we're actually
relying on the virtual memory

00:07:15.640 --> 00:07:19.180
system for IO in some cases and
the VM interfaces are still 32 bit.

00:07:19.180 --> 00:07:22.630
But all of the file system interfaces
now are 64 bit so that we'll be

00:07:22.630 --> 00:07:26.890
able to support very large files in
Mac OS X and the virtual memory system

00:07:26.970 --> 00:07:29.820
is going to change to accommodate that.

00:07:30.210 --> 00:07:35.030
Also important to note is that all
of the interfaces into the core file

00:07:35.030 --> 00:07:39.710
system at the BSD level are UTF-8,
take UTF-8 strings.

00:07:39.830 --> 00:07:43.080
The important thing about UTF-8,
it is a single byte character

00:07:43.180 --> 00:07:44.960
encoding of Unicode characters.

00:07:44.960 --> 00:07:48.120
Unicode, for those who don't know,
is an international standard that allows

00:07:48.310 --> 00:07:51.940
for just about every character in any
language that's in popular use today.

00:07:52.010 --> 00:07:55.450
Up to 65,000 character spaces
are reserved in the Unicode set,

00:07:55.470 --> 00:07:56.640
generally two bytes.

00:07:56.720 --> 00:08:00.170
But a single byte encoding of Unicode
was invented and it's called UTF-8.

00:08:00.480 --> 00:08:03.240
The key feature of UTF-8 is
that you can pass it through

00:08:03.240 --> 00:08:04.740
normal C string interfaces.

00:08:04.750 --> 00:08:07.880
It guarantees that there won't
be any null termination where

00:08:07.880 --> 00:08:09.490
there isn't supposed to be.

00:08:09.590 --> 00:08:13.970
And the first 128 bits, sorry,
the first 128 characters in UTF-8

00:08:14.290 --> 00:08:16.200
are just regular ASCII characters.

00:08:16.270 --> 00:08:19.340
And so for people that are just
dealing with a regular ASCII system,

00:08:19.400 --> 00:08:22.370
it's going to look the same to
you even though we've dictated

00:08:22.480 --> 00:08:25.070
throughout the kernel that all
file names are going to be UTF-8.

00:08:25.080 --> 00:08:25.080
It's not until you start seeing
shifting of the file names that

00:08:25.080 --> 00:08:25.080
you can see that it's going to look
the same to you even though we've

00:08:25.080 --> 00:08:25.080
dictated throughout the kernel that
all file names are going to be UTF-8.

00:08:25.200 --> 00:08:28.130
It's not until you start seeing
shift gist and so forth kanji

00:08:28.300 --> 00:08:31.340
representations that the byte
stream will start to look different.

00:08:31.400 --> 00:08:33.610
But the beauty is, again,
you can pass it through

00:08:33.610 --> 00:08:34.660
car star interfaces.

00:08:34.860 --> 00:08:37.810
We also have a number of new features
besides what we've given you on the

00:08:37.810 --> 00:08:40.330
status update and we're going to
talk about those in more detail.

00:08:40.340 --> 00:08:42.940
Actually,
I'm going to have my tech lead talk about

00:08:43.400 --> 00:08:45.290
those in more detail as we move along.

00:08:46.160 --> 00:08:47.150
So I have to apologize a little bit.

00:08:47.160 --> 00:08:49.200
Our radar screen hasn't changed much.

00:08:49.260 --> 00:08:51.870
If you don't remember,
our definition of the radar screen

00:08:52.200 --> 00:08:55.800
is things that I think we're
going to have at some point,

00:08:55.860 --> 00:08:58.190
but I'm not sure and I'm
unwilling to tell you.

00:08:58.280 --> 00:09:01.460
So, you know, you can ask the question,
but that's the answer you'll get.

00:09:01.530 --> 00:09:04.520
But we do recognize that a lot of
people are really interested in a

00:09:04.520 --> 00:09:06.760
directory change notification feature.

00:09:06.800 --> 00:09:09.100
And quite frankly,
we had sort of hoped to have it by now,

00:09:09.150 --> 00:09:10.780
but there were other
things that took priority.

00:09:11.000 --> 00:09:13.440
It is, however,
something we're still considering.

00:09:13.490 --> 00:09:18.530
Another thing we'd like to do is
allow NFS access to HFS+ volumes.

00:09:18.760 --> 00:09:21.610
It turns out this is actually
quite a bit trickier than it

00:09:21.790 --> 00:09:23.080
would seem on the surface.

00:09:23.370 --> 00:09:27.900
And HFS after all, HFS+ after all rather,
has pretty much all the features now that

00:09:27.990 --> 00:09:30.400
a UFS or a Berkeley Fast File System has.

00:09:30.420 --> 00:09:34.060
And so it seems like it would
be trivial to do the same things

00:09:34.210 --> 00:09:36.270
that UFS does to allow NFS access.

00:09:36.490 --> 00:09:39.170
But the problem is there are
a number of architectural

00:09:39.170 --> 00:09:42.300
issues with Mac OS X overall
that make this rather tricky.

00:09:42.300 --> 00:09:45.380
And in a nutshell,
the Carbon application environment,

00:09:45.380 --> 00:09:48.510
when it runs across a volume that
doesn't support some of the features

00:09:48.600 --> 00:09:52.780
that are associated with HFS+, like
resource forks and catalog information,

00:09:52.780 --> 00:09:55.530
access by ID and so forth, well,
it mimics that for you.

00:09:55.640 --> 00:09:58.550
It actually builds its own
environment for creating its own

00:09:58.550 --> 00:10:01.220
separate resource fork and its
own separate catalog information,

00:10:01.510 --> 00:10:04.650
does its own ID to path name mapping,
so that as an application,

00:10:04.890 --> 00:10:08.260
you don't have to care if you're on a
UFS volume or if you're talking over NFS.

00:10:08.380 --> 00:10:11.230
You can make the same Carbon calls
you've always made and they'll just work.

00:10:11.870 --> 00:10:15.340
Well, the problem with that is,
on HFS+, we have local support

00:10:15.460 --> 00:10:16.740
for all of those features.

00:10:16.760 --> 00:10:19.980
And so if you were to export
an HFS+ volume over NFS,

00:10:20.000 --> 00:10:24.540
locally, the clients would use the native
resource fork and catalog information.

00:10:24.570 --> 00:10:26.560
Remotely,
because the NFS protocol doesn't

00:10:26.560 --> 00:10:29.350
have support for this sort of thing,
Carbon is going to see that

00:10:29.460 --> 00:10:31.000
it's NFS and start mimicking it.

00:10:31.020 --> 00:10:34.930
And so the remote clients will have
a different view of the resource fork

00:10:34.940 --> 00:10:38.040
data and of the catalog information
than the local clients will do.

00:10:38.090 --> 00:10:40.340
So that's actually
quite a problem for us.

00:10:40.350 --> 00:10:42.640
We're still figuring out
how exactly to solve that.

00:10:42.650 --> 00:10:43.240
In the short term,
we're going to have to figure

00:10:43.240 --> 00:10:43.240
out how exactly to solve that.

00:10:43.240 --> 00:10:44.930
In the long term,
we'll probably have some configurations

00:10:44.940 --> 00:10:49.340
where you can NFS export HFS+ volumes,
even though those problems aren't solved.

00:10:49.380 --> 00:10:51.340
And for people that just
want to use it in a server

00:10:51.340 --> 00:10:54.010
configuration and don't worry,
aren't so concerned about how

00:10:54.020 --> 00:10:57.600
the local access works and are
able to step around the issues.

00:10:57.630 --> 00:10:59.830
We also would like to add
some new file systems,

00:10:59.920 --> 00:11:02.200
specifically the Joliet
extensions to ISO.

00:11:02.380 --> 00:11:04.840
At some point,
we'd like to have an NFS implementation

00:11:05.180 --> 00:11:07.740
and even more importantly,
an implementation that will allow us

00:11:07.740 --> 00:11:09.480
to read files from DOS and Windows.

00:11:09.530 --> 00:11:13.140
So the FAT16 or FAT32, some,
one of the Windows format file systems.

00:11:13.140 --> 00:11:18.020
So now, we actually do have a demo.

00:11:18.040 --> 00:11:21.130
And I'm going to call Conrad
Minchel up to bring it to you.

00:11:21.140 --> 00:11:25.230
I'm always a little bit jealous
of the QuickTime guys because

00:11:25.310 --> 00:11:27.840
they get to do these big,
fancy multimedia demos.

00:11:27.840 --> 00:11:29.640
And they have movies and
it's really exciting.

00:11:29.640 --> 00:11:31.740
And file systems,
we never get to do that stuff.

00:11:31.740 --> 00:11:33.920
And they just won't let us do that stuff.

00:11:34.010 --> 00:11:36.610
So we're going to sort of skirt
the edge a little bit and try to

00:11:36.610 --> 00:11:38.980
get in at least a picture for you.

00:11:39.170 --> 00:11:41.030
And with that,
I'm going to introduce Conrad Minchel,

00:11:41.030 --> 00:11:45.170
who is one of the file system
engineers engineers on my team.

00:11:50.770 --> 00:11:54.660
So this actually got working
about 20 minutes ago.

00:11:54.670 --> 00:11:59.080
And hopefully it's still working.

00:12:00.200 --> 00:12:07.660
A DVD RAM disk installed in this system
down below and it's already mounted.

00:12:07.660 --> 00:12:10.970
And it's mounted on /data here.

00:12:23.760 --> 00:12:26.300
So I can see, oops, yeah.

00:12:26.300 --> 00:12:31.580
So I can see I have one
movie already there.

00:12:31.700 --> 00:12:36.190
So last year at this time we gave you
the first ever public demonstration of

00:12:36.450 --> 00:12:40.500
booting off of HFS and HFS+. You can
imagine what might happen next year

00:12:40.500 --> 00:12:44.410
when you all pay your money again and
come back to see us at the Worldwide

00:12:44.520 --> 00:12:46.670
Developers Conference in 2001.

00:12:48.420 --> 00:12:50.530
So with that,
I'd like to bring up Pat Dirks,

00:12:50.600 --> 00:12:52.490
my tech lead,
and he's going to walk you through

00:12:52.830 --> 00:12:54.820
some of the cool and awesome
new features that we've added

00:12:54.920 --> 00:12:57.380
to the file system in Mac OS X.

00:13:00.850 --> 00:13:04.300
I'll do without that.

00:13:04.390 --> 00:13:05.460
Yeah,
it's pretty amazing to think that last

00:13:05.560 --> 00:13:09.890
year around this time we just very
carefully demoed the first booting

00:13:09.890 --> 00:13:12.650
and rooting off an HFS+ file system.

00:13:12.650 --> 00:13:14.560
It's been quite a year.

00:13:14.590 --> 00:13:19.560
I could take a moment and talk about some
of the new features that we've added.

00:13:19.990 --> 00:13:24.500
Prime amongst them probably,
once you realize that you are

00:13:24.500 --> 00:13:29.040
running on HFS+ file systems is
Mac OS X is a multi-user system

00:13:29.180 --> 00:13:31.880
and at its core is a BSD base.

00:13:31.880 --> 00:13:36.840
Every file and every directory
in the system is going to have

00:13:36.840 --> 00:13:40.420
permissions assigned to it.

00:13:40.420 --> 00:13:45.040
Those permissions are a bit like what
you may have seen on AFP file servers,

00:13:45.090 --> 00:13:46.410
but they're not quite like it.

00:13:46.630 --> 00:13:51.910
There's a similar division between
owners of files and directories and

00:13:51.910 --> 00:13:55.690
sort of everyone or guest access class
of people and then there's a group

00:13:55.690 --> 00:13:58.320
that you can assign to every object.

00:13:58.320 --> 00:14:00.770
You can assign separate
read/write or execute permissions

00:14:00.770 --> 00:14:03.400
to each of these groups.

00:14:03.400 --> 00:14:05.930
Underneath, although you'll see names
in most of the interfaces,

00:14:05.970 --> 00:14:10.650
these are mapped to numeric identifiers
that are mapped out of a database.

00:14:10.730 --> 00:14:14.410
What access you have to any given
file or directory is determined by

00:14:14.410 --> 00:14:16.680
the user ID of the running process.

00:14:16.690 --> 00:14:19.640
That's all fine and well if you're
running on a single big Unix

00:14:19.700 --> 00:14:22.750
system where things get mounted
and they stay mounted all the time,

00:14:22.830 --> 00:14:25.420
but it's a different story if
you're running in a Mac OS type

00:14:25.500 --> 00:14:26.660
environment where you've got zip
disks that you're carrying around

00:14:26.660 --> 00:14:31.140
that you're popping up and dropping
in and out of machines all the time.

00:14:31.140 --> 00:14:35.330
You have a problem where the numeric
IDs that are assigned on a disk that

00:14:35.330 --> 00:14:39.100
may be created on your neighbor's
system may not mean anything in the

00:14:39.100 --> 00:14:43.680
system that you have created or they
may map to entirely different names.

00:14:43.680 --> 00:14:46.520
Simply plugging in a disk and
using the permissions on there,

00:14:46.520 --> 00:14:49.670
even though HFS+ has provisions
for storing the user ID and

00:14:49.680 --> 00:14:52.410
group ID and permissions
information is not sufficient,

00:14:52.410 --> 00:14:54.730
you have to worry about what to
do when there are permissions

00:14:54.730 --> 00:14:55.650
on there that make no sense.

00:14:57.080 --> 00:14:59.450
We wanted to make that as
transparent as possible,

00:14:59.450 --> 00:15:03.200
and to that end we decided to
tag every volume with a 64-bit

00:15:03.200 --> 00:15:07.360
ID that we make up the first
time that volume is initialized.

00:15:07.360 --> 00:15:11.190
The system actually maintains a list
of IDs of volumes that it knows about,

00:15:11.350 --> 00:15:17.840
and it only uses the privileges on
those disks whose ID is in the list.

00:15:17.910 --> 00:15:21.440
If the ID is not in the list,
we fall back to a mode where we ignore

00:15:21.530 --> 00:15:25.470
all the privileges on the volume and
we make the person who's logged in

00:15:25.470 --> 00:15:26.200
on the machine the owner of the disk.

00:15:26.200 --> 00:15:26.200
That's the first step.

00:15:26.200 --> 00:15:26.200
The second step is to make sure
that the disk is in the list.

00:15:26.200 --> 00:15:26.300
The third step is to make sure
that the disk is in the list.

00:15:26.300 --> 00:15:26.320
The fourth step is to make sure
that the disk is in the list.

00:15:26.380 --> 00:15:26.430
The fifth step is to make sure
that the disk is in the list.

00:15:26.440 --> 00:15:26.480
The sixth step is to make sure
that the disk is in the list.

00:15:26.480 --> 00:15:31.560
be the owner of all the files and
all the directories on that system.

00:15:31.560 --> 00:15:34.520
The only thing to be aware of is
that if a disk is used in that mode

00:15:34.520 --> 00:15:38.880
and you create new things on there,
those will also be tagged with unknown.

00:15:38.880 --> 00:15:42.040
When the disk comes back to
the originating system where

00:15:42.040 --> 00:15:45.420
the permissions are used,
they'll see unknown on there.

00:15:45.420 --> 00:15:47.540
They'll get the same treatment.

00:15:47.540 --> 00:15:50.360
The person logged in on that machine
will be the full owner of it,

00:15:50.360 --> 00:15:54.380
but that's kind of a little
subtlety to be aware of.

00:15:54.380 --> 00:15:57.540
Basically, what we strove to do was to
approach as much as possible

00:15:57.540 --> 00:16:00.750
the regular Mac OS experience
of being able to take a disk,

00:16:00.750 --> 00:16:03.890
put it in your machine,
and use it in any way you see fit.

00:16:03.890 --> 00:16:06.470
We don't want permissions
to get in the way of that.

00:16:08.800 --> 00:16:14.180
Something else we did is an extension
of something that is an API that's

00:16:14.180 --> 00:16:19.050
already provided for AFP URLs,
allowing you to manipulate server

00:16:19.050 --> 00:16:22.800
references as URLs and have
the system mount those for you.

00:16:22.880 --> 00:16:25.930
So this is currently
supported only for AFP,

00:16:25.930 --> 00:16:28.800
but the idea is that
we'll extend it over time.

00:16:28.800 --> 00:16:32.790
And it's the perfect implementation,
in fact it is the implementation,

00:16:32.800 --> 00:16:37.130
for the connect to dialog in the finder,
you just type in a URL and

00:16:37.140 --> 00:16:41.800
the system takes that URL,
figures out what file system uses that,

00:16:41.880 --> 00:16:43.800
finds a place in the file
system to make that appear,

00:16:43.800 --> 00:16:47.430
creates that directory, mounts it,
makes the server connection

00:16:47.430 --> 00:16:48.800
and makes it available to you.

00:16:48.970 --> 00:16:50.800
So it's a very nice API.

00:16:50.800 --> 00:16:52.740
We're working on that,
it's still evolving.

00:16:52.840 --> 00:16:58.800
If you think you have URL schemes that
would make sense to tie into that,

00:16:58.800 --> 00:17:01.350
we're still discussing whether
we want to make this dynamically

00:17:01.350 --> 00:17:04.800
pluggable or a small static list,
come talk to us afterwards.

00:17:04.800 --> 00:17:05.800
But that's how we envision it.

00:17:05.800 --> 00:17:10.110
providing access to URLs over
time through the file system.

00:17:10.650 --> 00:17:19.190
We've added a number of features to the
system to help support the HFS+ APIs.

00:17:19.360 --> 00:17:21.570
Long file names are prime amongst them.

00:17:21.780 --> 00:17:25.760
On Mac OS, using the Unicode APIs,
you can now create and manipulate

00:17:25.760 --> 00:17:28.600
files with 255 Unicode characters.

00:17:28.680 --> 00:17:33.840
And we've extended the
BSD APIs to do the same thing.

00:17:33.840 --> 00:17:36.500
As Clark mentioned,
UTF-8 is now the standard

00:17:36.880 --> 00:17:41.120
interface for all files and
directories in the file system.

00:17:41.120 --> 00:17:47.630
And so we allow you to pass a UTF-8
encoding of a string of up to 255 Unicode

00:17:47.630 --> 00:17:49.890
characters into the kernel that way.

00:17:49.900 --> 00:17:53.780
So everything is done
through UTF-8 in the kernel.

00:17:53.860 --> 00:17:57.110
And last but not least,
we've supplied new system

00:17:57.110 --> 00:18:00.230
calls that allow us to support
the bulk enumeration calls.

00:18:00.230 --> 00:18:02.840
And we'll get into some more
detail about that in a moment.

00:18:04.770 --> 00:18:08.700
I want to talk both a little bit
in review and a little bit to bring

00:18:08.750 --> 00:18:12.570
you up to date on the interfaces
and talk about the differences

00:18:12.660 --> 00:18:19.800
between the behavior of possibly the
same code on Mac OS X and Mac OS.

00:18:20.090 --> 00:18:23.650
So when you're writing your application,
you've basically got your choice

00:18:23.650 --> 00:18:27.360
of one of three environments,
and they all make sense at

00:18:27.400 --> 00:18:29.710
sort of different times.

00:18:29.750 --> 00:18:32.260
They're all layered on
top of a common BSD layer,

00:18:32.260 --> 00:18:36.640
though, and as Clark mentioned,
that's where we really do our work.

00:18:36.640 --> 00:18:39.450
And inside BSD,
there is a virtual file system

00:18:39.460 --> 00:18:43.220
or VFS layer that all file
system calls get translated to,

00:18:43.220 --> 00:18:46.680
and that takes care of doing the file
system specific implementation of

00:18:46.680 --> 00:18:49.310
some particular feature or operation.

00:18:49.330 --> 00:18:55.840
So Carbon is the natural way
to port your application.

00:18:55.840 --> 00:19:00.220
If you have a Mac OS application,
it should be a cinch to Carbonize it.

00:19:00.220 --> 00:19:06.640
And the Carbon interfaces are,
in most ways, exactly like the existing

00:19:06.640 --> 00:19:08.120
Mac OS interfaces.

00:19:08.120 --> 00:19:10.000
If you're writing a
brand new application,

00:19:10.000 --> 00:19:13.620
you may want to consider
writing to the Cocoa APIs.

00:19:13.620 --> 00:19:17.880
They're a completely different
object-oriented set of APIs.

00:19:17.880 --> 00:19:22.660
They were designed-- they were designed
from the outset to be very portable

00:19:22.740 --> 00:19:24.400
to a number of different environments.

00:19:24.400 --> 00:19:27.410
And so they don't support some
of the features that you may

00:19:27.410 --> 00:19:29.400
expect on HFS or on Mac OS.

00:19:29.460 --> 00:19:30.880
So there are no resource forks.

00:19:30.880 --> 00:19:33.800
There's no support for Finder info,
those sorts of things.

00:19:33.800 --> 00:19:38.880
But you can have a very uniform
API to general file storage.

00:19:38.880 --> 00:19:40.400
Finally, of course, there's Classic.

00:19:40.400 --> 00:19:43.460
If you do nothing to your application,
it will run in Classic.

00:19:43.570 --> 00:19:46.200
And obviously,
all the calls are supported there, too.

00:19:46.200 --> 00:19:48.070
And as I said,
all of those are implemented in

00:19:48.150 --> 00:19:51.690
the PSD layer that we've extended
with an actually surprisingly

00:19:51.690 --> 00:19:54.910
small number of new system calls.

00:19:56.120 --> 00:20:02.840
I guess the first thing to be aware of,
when Mac OS 9 came out,

00:20:02.840 --> 00:20:08.000
we introduced a number of new APIs to
let you operate on files and directories

00:20:08.000 --> 00:20:13.390
with Unicode and with FSRefs,
a new form of file reference.

00:20:13.500 --> 00:20:15.760
And all those calls are
supported in Carbon.

00:20:15.930 --> 00:20:19.040
So I think the message is clear.

00:20:19.160 --> 00:20:21.240
Think Unicode, think FSRefs.

00:20:21.300 --> 00:20:23.240
It'll work great in Carbon.

00:20:23.300 --> 00:20:25.970
The full range of calls is supported.

00:20:26.070 --> 00:20:27.610
Cat search is supported.

00:20:27.610 --> 00:20:29.440
It's not something that's
in traditional BSD,

00:20:29.440 --> 00:20:32.540
but we've added system
calls to support that.

00:20:32.600 --> 00:20:37.530
And so anything you can do in 9.0 Carbon,
it's fully supported.

00:20:37.620 --> 00:20:45.340
The new Get Catalog Info Bulk
Call that I refer to is a great call.

00:20:45.410 --> 00:20:48.930
It's a bit like the AFP Enumerate call,
which lets you get directory

00:20:48.940 --> 00:20:53.190
information on a number of objects
in the directory in one single call.

00:20:53.320 --> 00:20:57.430
And it has a very flexible method of
specifying what it is you're interested

00:20:57.430 --> 00:20:59.480
in about each particular object.

00:20:59.670 --> 00:21:02.720
So you can say, I want to get just the
name and the finder info,

00:21:02.720 --> 00:21:05.430
for instance,
for all the objects in the system.

00:21:05.440 --> 00:21:08.110
And it will return just that to you.

00:21:08.200 --> 00:21:10.930
You'll see that actually appear in
a number of different calls and some

00:21:10.930 --> 00:21:12.350
of the new system calls as well.

00:21:12.440 --> 00:21:17.020
There are methods using a bitmap
type of specification to let you

00:21:17.060 --> 00:21:20.630
specify which attributes or which
pieces of information about files and

00:21:20.630 --> 00:21:22.350
directories it is you're interested in.

00:21:22.420 --> 00:21:23.160
And it's great because the other
thing that we've done is we've

00:21:23.160 --> 00:21:23.160
added a bitmap to the system.

00:21:23.160 --> 00:21:25.920
So that way,
the underlying file systems don't have

00:21:26.030 --> 00:21:30.120
to go and possibly make up information
that you're not even interested in.

00:21:30.120 --> 00:21:35.460
I mean, for the longest time on Mac OS,
we've carried around this number of

00:21:35.460 --> 00:21:40.300
directories in the root field just
because it was there in MFS when

00:21:40.300 --> 00:21:41.420
the Mac was first introduced.

00:21:41.480 --> 00:21:44.320
And we had no way to tell whether
somebody who called GetVolInfo

00:21:44.520 --> 00:21:46.150
actually needed that field or not.

00:21:46.160 --> 00:21:50.120
And so this will help
us move away from that.

00:21:50.190 --> 00:21:53.120
So the BSD extensions.

00:21:53.150 --> 00:21:55.100
Um...

00:21:55.350 --> 00:22:00.890
Get Adder List and Set Adder List are
new ways of getting information out

00:22:01.050 --> 00:22:03.200
about individual files or directories.

00:22:03.460 --> 00:22:07.520
They basically let you specify a buffer
that your information will be filled into

00:22:07.900 --> 00:22:12.020
and a bitmap with a couple of fields,
very much like the AFP Get Fault

00:22:12.020 --> 00:22:13.660
Airpods call again.

00:22:13.660 --> 00:22:16.470
You can say,
"I want the finder info about files.

00:22:16.470 --> 00:22:21.350
I want the fork links," and that's it,
and you'll get exactly that returned.

00:22:21.470 --> 00:22:25.660
SearchFS is a very direct implementation
of the CatSearch functionality,

00:22:25.680 --> 00:22:30.710
and all the things you can do with
CatSearch you can do in this system,

00:22:30.790 --> 00:22:32.920
and it's all implemented
through SearchFS.

00:22:32.990 --> 00:22:37.280
ExchangeData is an implementation
of the file exchange functionality

00:22:37.280 --> 00:22:39.820
that lets you do save saves of data.

00:22:39.900 --> 00:22:44.050
These are all things that we needed to
do to support Carbon's functionality,

00:22:44.060 --> 00:22:46.640
but there were no equivalent
BSD calls to do it.

00:22:46.710 --> 00:22:50.540
Most other things actually map pretty
straightforwardly to existing BSD calls.

00:22:50.710 --> 00:22:54.580
All the various variants of open
and opendf and whatnot all just map

00:22:54.580 --> 00:22:56.720
to the straight old BSD open call.

00:22:56.720 --> 00:23:00.770
For those areas where we didn't have
any system calls to support them,

00:23:00.770 --> 00:23:02.460
we made up these new ones.

00:23:02.460 --> 00:23:06.910
F-control is an existing
BSD system call that lets you do

00:23:06.910 --> 00:23:10.870
various operations on open files,
and we added an option to

00:23:10.870 --> 00:23:12.850
let you do file allocation,
for instance.

00:23:12.860 --> 00:23:17.050
The allocate call is not something
that is known in the BSD system.

00:23:17.730 --> 00:23:20.840
Get-Dir Entries Adder is the
system call that was added

00:23:20.890 --> 00:23:23.200
to do the bulk enumeration.

00:23:23.200 --> 00:23:25.810
And if you are at all familiar
with the attribute list,

00:23:25.850 --> 00:23:29.700
get-adder list, and set-adder list API,
you'll see this is very familiar.

00:23:29.730 --> 00:23:32.880
There is an attribute list that is
passed in and a buffer and a size

00:23:32.930 --> 00:23:37.070
that you specify and a few parameters
that let you say which entries in

00:23:37.070 --> 00:23:40.850
the directory you're interested in,
how many items you are

00:23:40.860 --> 00:23:44.080
prepared to take back,
and various options that you can pass.

00:23:44.100 --> 00:23:47.060
And with that one call,
you can get information on a whole

00:23:47.060 --> 00:23:50.970
number of directory entries all at once,
possibly the whole directory

00:23:50.970 --> 00:23:52.420
in one single call.

00:23:52.610 --> 00:23:56.760
So it's a very efficient
way to enumerate file data.

00:23:57.160 --> 00:24:01.680
The VFS layer is something we'll get
into in a little bit more detail later,

00:24:01.700 --> 00:24:05.670
but as I said,
that is the layer inside the kernel that

00:24:05.810 --> 00:24:11.440
separates the file system independent
system call layer from the file system

00:24:11.440 --> 00:24:15.330
specific implementation of certain calls.

00:24:15.510 --> 00:24:19.850
and it basically manipulates these
abstract data structures called V nodes

00:24:20.250 --> 00:24:23.690
that are specific to each file system
and there's a dispatch table that's

00:24:23.700 --> 00:24:28.370
associated with each V node that lets
the system dispatch to the file system

00:24:28.460 --> 00:24:30.900
specific implementation of some call.

00:24:31.000 --> 00:24:34.000
If you're writing file systems,
you'll get very familiar

00:24:34.000 --> 00:24:35.810
with that layer of interface.

00:24:35.920 --> 00:24:40.140
Now, there are some differences in the
way that code works even if the

00:24:40.200 --> 00:24:44.000
code looks the same or similar.

00:24:44.000 --> 00:24:46.940
Unix for instance,
various flavors of Unix

00:24:46.940 --> 00:24:50.300
and BSD among them,
lets you delete open files which

00:24:50.300 --> 00:24:55.540
was always a big no-no under Mac OS,
you get FBC error if you try to do that.

00:24:55.570 --> 00:24:57.960
We support both semantics.

00:24:57.960 --> 00:25:01.700
If you're calling the
delete call in Carbon,

00:25:01.720 --> 00:25:05.250
you'll call a variant of
unlink that actually will

00:25:05.250 --> 00:25:07.520
return an error if it's open.

00:25:07.530 --> 00:25:11.640
But if you are a BSD or a
Cocoa application calling unlink to

00:25:11.640 --> 00:25:15.760
remove a file from the directory,
that will succeed even if it's open.

00:25:15.770 --> 00:25:19.780
So it's possible that files you have
open will get deleted from the catalog.

00:25:19.790 --> 00:25:21.930
That's not as wild of an
adjustment as you might think

00:25:22.000 --> 00:25:25.060
because even under Mac OS 9,
it's always possible that something

00:25:25.450 --> 00:25:28.420
would get deleted or renamed out
from under you and no longer appear

00:25:28.420 --> 00:25:30.000
where you thought it appeared.

00:25:30.020 --> 00:25:31.640
And Don may get an interview.

00:25:31.640 --> 00:25:31.680
Okay.

00:25:31.680 --> 00:25:32.860
So I'm going to go ahead and jump
in a moment into the kinds of

00:25:32.910 --> 00:25:34.700
games we play to make this happen.

00:25:34.700 --> 00:25:37.490
But it's something to be mindful of.

00:25:37.490 --> 00:25:41.020
Hard links are a strictly Unixism.

00:25:41.020 --> 00:25:48.760
It's a way for two entries in the catalog
to refer to the same exact data on disk.

00:25:48.760 --> 00:25:52.300
And this is not an alias that
you resolve to get to the data.

00:25:52.300 --> 00:25:55.760
These are two equivalent names to
get to the same data on the disk.

00:25:55.800 --> 00:25:59.940
So again,
that may be something to be mindful of.

00:26:00.280 --> 00:26:01.620
One thing in the Unix file,
there's a lot of things that you can

00:26:01.620 --> 00:26:02.550
do to get to the same data on disk.

00:26:02.620 --> 00:26:04.000
So you can do that by
using the file system APIs.

00:26:04.000 --> 00:26:08.640
It's very careful not to return you
data that you have not yourself written.

00:26:08.660 --> 00:26:11.940
And if you're just streaming out a file,
that's no issue.

00:26:11.940 --> 00:26:14.500
You're writing everything
before you'll ever read it.

00:26:14.520 --> 00:26:16.750
But you should be careful of
some things that may have made

00:26:16.840 --> 00:26:20.870
perfect sense at times on Mac OS,
like calling setEof to extend

00:26:21.000 --> 00:26:25.060
the file to its maximum length
before you start writing the data.

00:26:25.070 --> 00:26:27.050
Now,
to prepare for the possibility that you

00:26:27.050 --> 00:26:31.520
might actually read that data since the
logical end of file has been moved out.

00:26:31.550 --> 00:26:31.560
That whole interview is a very
important part of the process.

00:26:31.560 --> 00:26:34.100
And the intervening gap between
the current EOF and where you're

00:26:34.110 --> 00:26:35.250
setting it is zero-filled.

00:26:35.500 --> 00:26:41.300
So you may be doing gigantic IOs where
before you were doing absolutely none.

00:26:41.340 --> 00:26:44.330
Calls like allocate are fully supported
and they're a much better way of making

00:26:44.330 --> 00:26:49.040
sure that the data is available on
disk before you make your write calls,

00:26:49.040 --> 00:26:51.500
for instance, than setEof is.

00:26:51.500 --> 00:26:56.910
One sort of quirk in the BSD APIs is
that there is no notion of a create

00:26:56.910 --> 00:27:01.460
date the way there is on HFS+. HFS+
volumes have them and we support them.

00:27:01.550 --> 00:27:04.920
But if somebody uses the
BSD APIs to manipulate the

00:27:04.920 --> 00:27:09.960
modification dates of files,
the create dates are actually untouched.

00:27:09.980 --> 00:27:12.990
And until we decide to
do something about this,

00:27:13.000 --> 00:27:16.820
you may actually see modification
dates that are prior to the

00:27:16.820 --> 00:27:18.540
creation date of a file.

00:27:18.540 --> 00:27:21.460
It's a little weird, but again,
something to be mindful of.

00:27:21.470 --> 00:27:26.010
And then, of course, the path separators,
Unix, BSD,

00:27:26.130 --> 00:27:31.430
all uses slash to separate path names.

00:27:31.430 --> 00:27:31.440
And HFS, Mac OS uses colons.

00:27:31.440 --> 00:27:35.690
And then, if you have a path
that's not a path name,

00:27:35.760 --> 00:27:41.930
you should be aware that the path
names that you use in your Carbon calls

00:27:41.930 --> 00:27:46.180
are not the same path names that you
should use if you make BSD calls.

00:27:46.190 --> 00:27:51.620
The system underneath actually translates
between colon and slash for you.

00:27:51.630 --> 00:27:54.040
And the results may be surprising.

00:27:54.040 --> 00:27:57.170
If you call BSD APIs to
enumerate a directory,

00:27:57.340 --> 00:28:01.320
you may see files with slashes in them,
but they will appear in the BSD APIs.

00:28:01.430 --> 00:28:04.370
So they will appear to you
with colons inside the name,

00:28:04.640 --> 00:28:07.660
because we can't pass up a
path name through the BSD layer

00:28:07.660 --> 00:28:08.880
that has a slash in it.

00:28:08.890 --> 00:28:11.360
We can safely pass one
with a colon in it.

00:28:11.360 --> 00:28:14.450
So between Carbon and BSD in the system,
there's all kinds of colon and

00:28:14.460 --> 00:28:16.320
slash translation going on.

00:28:16.320 --> 00:28:19.670
And if you are spanning these two layers,
if you're doing some BSD calls

00:28:19.670 --> 00:28:21.680
and some Carbon calls,
you should be careful

00:28:21.680 --> 00:28:23.100
about path name separators.

00:28:25.060 --> 00:28:27.640
Finally, we support a range of
different file systems,

00:28:27.640 --> 00:28:29.420
and they all have their
own implementation,

00:28:29.420 --> 00:28:31.150
and they all have their own quirks.

00:28:31.200 --> 00:28:34.800
Unix systems, BSD, no exception.

00:28:34.800 --> 00:28:38.260
The UFS file system has
long been case sensitive.

00:28:38.260 --> 00:28:44.800
We decided not to make a case sensitive
implementation of HFS or HFS+.

00:28:44.800 --> 00:28:50.830
So HFS is the same case insensitive
but case preserving file system.

00:28:50.840 --> 00:28:55.130
But you may be running into other
file systems unbeknownst to you

00:28:55.140 --> 00:28:58.730
because you're just traversing
the catalog hierarchy and run into

00:28:58.740 --> 00:29:01.900
UFS volumes that are mounted there,
which are case sensitive.

00:29:01.900 --> 00:29:07.680
So if your application counts on being
able to find the same name under two

00:29:07.680 --> 00:29:10.590
different case representations of it,
you're going to be surprised.

00:29:10.640 --> 00:29:16.590
And that's actually very common in code,
where you refer to a header

00:29:16.590 --> 00:29:18.660
file first by one name,
then by another.

00:29:18.660 --> 00:29:20.000
If you do it on UFS.

00:29:20.060 --> 00:29:20.820
UFS.

00:29:20.840 --> 00:29:22.050
You may run into trouble like that.

00:29:22.150 --> 00:29:26.100
Sparse files are something that UFS has
long supported that there's been no

00:29:26.100 --> 00:29:30.140
precedent for on HFS+, although we've
had some requests for it on occasion.

00:29:30.140 --> 00:29:33.140
Basically,
sparse files are a way for the file

00:29:33.140 --> 00:29:37.870
system to store your data files
without storing the gaps in the

00:29:37.870 --> 00:29:39.820
data blocks that you've written.

00:29:39.870 --> 00:29:45.080
So you can seek out to a location
two gigabytes into your file and

00:29:45.080 --> 00:29:48.060
write 10 bytes of data there,
and on UFS that will actually

00:29:48.060 --> 00:29:49.960
only take about 10 bytes on disk.

00:29:52.150 --> 00:29:55.620
That's not something that's supported on
HFS+. So if you try that because you're

00:29:55.720 --> 00:29:59.500
porting a BSD application or something,
you may be surprised to find that step

00:29:59.500 --> 00:30:04.390
one is to allocate two gig of data and
then go write your ten bytes of data.

00:30:04.450 --> 00:30:10.160
Hard links are supported
on HFS+, but not on HFS.

00:30:10.160 --> 00:30:12.820
They're supported on UFS and NFS as well.

00:30:12.860 --> 00:30:15.160
But, again,
if this is something you count on,

00:30:15.160 --> 00:30:18.700
you better make sure you know what
file system it is you're talking about.

00:30:18.700 --> 00:30:21.800
And so there are a
number of subtle gotchas,

00:30:21.840 --> 00:30:24.330
even though the basic
application that you have,

00:30:24.330 --> 00:30:27.360
if you ported the Carbon and
run it on HFS+ volume,

00:30:27.360 --> 00:30:30.320
will run remarkably well unchanged.

00:30:30.340 --> 00:30:32.120
And you may be unaware of
the difference altogether.

00:30:33.830 --> 00:30:36.260
At this point,
I'd like to bring up Don to talk about

00:30:36.410 --> 00:30:41.050
some of the differences between the way
HFS+ volumes are used in the system.

00:30:41.120 --> 00:30:44.290
Don?

00:30:44.290 --> 00:30:44.290
Thank you.

00:30:50.050 --> 00:30:51.760
Okay, as Pat said,
I'm going to talk about

00:30:51.840 --> 00:30:56.320
some of the changes we made
to the HFS+ volume format.

00:30:56.320 --> 00:30:58.580
In general,
almost all these changes should be

00:30:58.580 --> 00:31:02.750
transparent to all of you out there,
but some people like to hard code things,

00:31:02.880 --> 00:31:05.960
and so for that reason,
you're going to have to suffer

00:31:06.070 --> 00:31:08.680
through all these boring details here.

00:31:09.670 --> 00:31:13.930
The first set of changes involve
what we do during initialization in

00:31:14.330 --> 00:31:17.500
Mac OS X for HFS+ volume formats.

00:31:17.670 --> 00:31:22.670
Primarily we made some changes for
performance and for some of the

00:31:22.690 --> 00:31:25.060
features like hard links that we added.

00:31:25.330 --> 00:31:29.010
The allocation block size,
which used to be

00:31:29.100 --> 00:32:32.000
[Transcript missing]

00:32:34.300 --> 00:32:36.410
And like I said,
we have the startup file now where

00:32:36.490 --> 00:32:38.940
we use that to boot for Mac OS X.

00:32:39.020 --> 00:32:42.610
However, I should note that that startup
file is not in the DP4 release.

00:32:42.720 --> 00:32:45.060
That will be in a follow-on release.

00:32:45.160 --> 00:32:49.680
The last mounted version up
until now had been set at 8.1.

00:32:49.680 --> 00:32:51.440
Now that we're using
a different code base,

00:32:51.480 --> 00:32:54.510
we've upped that to 10.0.

00:32:54.600 --> 00:32:58.600
As Pat mentioned, there's some BSD fields
that we now are used.

00:32:58.650 --> 00:33:01.720
The permissions data and
the last access date.

00:33:01.720 --> 00:33:06.320
We now enforce the permissions and
we initialize them under Mac OS X.

00:33:06.390 --> 00:33:10.290
And on every read and write access,
we set the last access date.

00:33:10.740 --> 00:33:11.960
Finder info is mentioned here.

00:33:11.960 --> 00:33:15.240
It's not really Finder,
it's not really file system data,

00:33:15.250 --> 00:33:17.490
but people tended to use
it in file system calls.

00:33:17.500 --> 00:33:19.480
So I mentioned it in here.

00:33:19.730 --> 00:33:24.990
Typically, like the type and creator and
the invisible bit are often used.

00:33:25.230 --> 00:33:27.640
However,
you should be aware that in Mac OS X,

00:33:27.790 --> 00:33:29.760
often they'll be uninitialized.

00:33:29.830 --> 00:33:35.980
So if you're doing searches for
criteria on type and creator,

00:33:35.980 --> 00:33:36.930
be aware that they might
not be initialized.

00:33:38.260 --> 00:33:43.910
One interesting thing we discovered
when we were debugging is that there's

00:33:43.960 --> 00:33:49.960
occasionally in Mac OS 9 you'll
get file names with embedded nulls,

00:33:49.960 --> 00:33:52.210
mainly due because they
had a Pascal interface and

00:33:52.210 --> 00:33:53.180
you could actually do that.

00:33:53.230 --> 00:33:56.620
It's not a good thing to do,
but so to support those and

00:33:56.620 --> 00:34:00.440
not trip up some of the stacks
and the file system layers,

00:34:00.440 --> 00:34:06.160
we allow them to go through,
but you can no longer create them.

00:34:06.790 --> 00:34:11.930
In the past, we really didn't have
any reserved file names,

00:34:12.050 --> 00:34:17.470
but a long time ago, pre-System 7,
we discouraged the use

00:34:17.650 --> 00:34:22.480
of dot prefixes in files,
mainly because the file system and the

00:34:22.480 --> 00:34:25.700
device manager shared a common open call,
and we needed to differentiate

00:34:25.700 --> 00:34:27.890
between opening a file and a device.

00:34:28.220 --> 00:34:31.080
That support is no
longer there or needed,

00:34:31.080 --> 00:34:33.380
so we no longer discourage it.

00:34:33.380 --> 00:34:37.550
In fact, given our VSD underpinnings,
we have lots of files

00:34:37.560 --> 00:34:40.830
that start with a dot,
but that's not a bad thing anymore.

00:34:40.930 --> 00:34:44.890
Of course, you should avoid naming
any file dot or dot dot,

00:34:45.210 --> 00:34:47.720
because we use those in the VSD layer.

00:34:47.990 --> 00:34:50.840
There is some pseudo metadata,
additional.

00:34:50.840 --> 00:34:55.180
In the past,
we had Apple Share and the desktop DB.

00:34:55.350 --> 00:34:59.620
Now we have symbolic links and the
hard link and the deleted files.

00:34:59.650 --> 00:35:02.960
All this stuff,
in terms of the pseudo metadata,

00:35:02.960 --> 00:35:04.760
should be completely transparent.

00:35:04.890 --> 00:35:07.720
In Carbon,
you'll see some links as aliases.

00:35:07.730 --> 00:35:12.660
And in the POSIX and the Cocoa layers,
they'll actually automatically resolve,

00:35:12.660 --> 00:35:14.180
so you won't even see them.

00:35:14.460 --> 00:35:17.720
But we just tell you about
that so you know about them.

00:35:18.780 --> 00:35:25.060
We updated the disk first aid and the
FSDK to be able to fix hard links and

00:35:25.060 --> 00:35:28.420
we came any of the files that were

00:35:28.720 --> 00:35:31.770
We're hidden for the delete on busy.

00:35:31.770 --> 00:35:35.310
And if you're doing any
type of repair utility,

00:35:35.310 --> 00:35:41.850
we encourage you to also add that
minimal support into your application.

00:35:41.980 --> 00:35:43.700
Back to you, Clark.

00:35:48.200 --> 00:35:53.920
Thanks, John.

00:35:54.410 --> 00:35:57.160
If you attended Steve's
overview on Monday,

00:35:57.190 --> 00:36:00.430
you know that there are a
million different ways that

00:36:00.550 --> 00:36:05.210
QuickTime technology is distributed
on CD-ROMs of famous artists,

00:36:05.260 --> 00:36:07.420
sorry, audio CDs of famous artists.

00:36:07.450 --> 00:36:13.200
He also actually had a cereal box with a
CD in it that had QuickTime technology.

00:36:13.220 --> 00:36:14.540
Well, I have great news.

00:36:14.560 --> 00:36:18.700
As part of the whole new era of
respect for file systems that got us

00:36:18.700 --> 00:36:24.040
this large hall to get together in,
my boss just informs me that we have made

00:36:24.100 --> 00:36:27.690
a deal with the makers of generic cereal.

00:36:29.710 --> 00:36:34.970
for the distribution of file system
technology in the lesser stores near you.

00:36:40.000 --> 00:36:43.740
Alright, let's talk about the
virtual file system layer.

00:36:43.760 --> 00:36:46.470
So, to begin,
this is the layer in the file system

00:36:46.500 --> 00:36:49.940
that you would implement to if you
wanted to develop a new system.

00:36:50.000 --> 00:36:53.250
There are two fundamental uses of it:
a new file system for a new

00:36:53.260 --> 00:36:57.000
device type or a new protocol,
or what we call a file system stack.

00:36:57.000 --> 00:37:01.490
The way file system stacks work is they
basically implement a set of functions

00:37:01.630 --> 00:37:06.390
to the VFS interfaces and they sit on
top of an existing file system and at the

00:37:06.720 --> 00:37:10.980
bottom they call that same VFS interface
for a lower file system to process.

00:37:11.090 --> 00:37:14.850
So, for example, if you wanted to build
an encryption system,

00:37:15.200 --> 00:37:19.000
you could build a file system stack,
you would support the VFS interfaces,

00:37:19.000 --> 00:37:23.950
most of your VFS calls would simply
call the file system underneath you.

00:37:24.000 --> 00:37:26.570
But if you wanted to do I/O,
then you would look at the buffer,

00:37:26.640 --> 00:37:30.000
encrypt it if it was coming in,
decrypt it if it was coming out,

00:37:30.020 --> 00:37:32.930
and so you would call the file
system below you to get the data,

00:37:33.180 --> 00:37:35.000
make your manipulation,
and then return to the caller.

00:37:35.000 --> 00:37:36.990
Or you would get the
information from the caller,

00:37:36.990 --> 00:37:39.480
do your manipulation,
and then call the file system

00:37:39.480 --> 00:37:40.970
below you to finally do the write.

00:37:41.010 --> 00:37:44.000
So that's roughly what the VFS is for.

00:37:45.190 --> 00:37:48.900
Two key data structures you need
to know about if you are really

00:37:48.900 --> 00:37:52.610
seriously thinking about implementing
a file system or a file system stack.

00:37:52.720 --> 00:37:55.670
And the first is the VNode,
which stands for virtual node.

00:37:55.810 --> 00:37:59.200
And this is the key data structure
that represents a file in our system.

00:37:59.260 --> 00:38:02.540
If you programmed in Mac OS,
it's similar to a file control block,

00:38:02.620 --> 00:38:04.900
an FCB, but there is one key difference.

00:38:04.940 --> 00:38:08.100
VNodes are created whenever a
file is referenced for any reason,

00:38:08.160 --> 00:38:09.960
not just when a file is opened.

00:38:10.040 --> 00:38:12.200
So if you just wanted to
get file system statistics,

00:38:12.260 --> 00:38:15.190
if you did the equivalent
of a getcatinfo,

00:38:15.190 --> 00:38:17.720
we're going to allocate a
VNode inside of our system and

00:38:17.750 --> 00:38:19.190
we're going to initialize it.

00:38:19.300 --> 00:38:20.710
And it's going to stay
around for a while.

00:38:20.720 --> 00:38:23.600
We have a cache of these in the
system and up to thousands of

00:38:23.600 --> 00:38:26.100
them that just stay in case you
want to reference that file again

00:38:26.100 --> 00:38:27.490
so we don't have to recreate it.

00:38:27.710 --> 00:38:30.400
And we also have another
key data structure,

00:38:30.500 --> 00:38:33.120
a mount structure,
which is kind of similar

00:38:33.140 --> 00:38:34.970
to a volume control block.

00:38:35.100 --> 00:38:38.270
So these are the two key data structures
that you'll need to work with if you're

00:38:38.270 --> 00:38:39.920
wanting to implement a file system.

00:38:39.920 --> 00:38:44.440
There are also two key
segments of the VFS interface.

00:38:44.480 --> 00:38:47.580
There are what we call VFS calls,
and I apologize for the name confusion,

00:38:47.580 --> 00:38:48.630
but we inherited that.

00:38:48.680 --> 00:38:52.010
And there are also VNode
operations or VOP calls.

00:38:52.020 --> 00:38:55.220
And the reason they're called this is
because if you actually look in our code,

00:38:55.220 --> 00:38:57.600
if you look in the file
system independent layer

00:38:57.620 --> 00:39:00.080
of system call processing,
you'll see some work go on and

00:39:00.080 --> 00:39:03.070
then you'll see a call to something
called VFS underbar something

00:39:03.070 --> 00:39:04.880
or VOP underbar something.

00:39:04.920 --> 00:39:08.310
And what those are are macros
that look for the dispatch table

00:39:08.490 --> 00:39:09.880
for the right operation to call.

00:39:09.980 --> 00:39:13.360
Given the type of a VNode that it's about
to make the call on or the type of file

00:39:13.360 --> 00:39:15.280
system it's about to make the call on.

00:39:15.340 --> 00:39:17.940
If it's a VFS operation,
you'll see VFS underbar

00:39:18.060 --> 00:39:21.580
operation and it will call the
VFS routine for that file system.

00:39:21.640 --> 00:39:26.280
If it's a file based operation,
you'll see a VOP underbar operation,

00:39:26.370 --> 00:39:29.840
VOP remove, VOP open,
VOP close and so forth.

00:39:29.840 --> 00:39:33.770
And it will make that call to the
underlying routine in that file system.

00:39:33.830 --> 00:39:36.590
So while you see all these
VOP macros scattered about,

00:39:36.590 --> 00:39:39.820
if you look at the implementation
for any given file system,

00:39:39.840 --> 00:39:41.940
you'll see that they have
routines called in the HFS,

00:39:42.000 --> 00:39:44.840
for example, HFS close, HFS open,
HFS remove.

00:39:44.910 --> 00:39:47.980
Those are the routines that will
actually be called when you see

00:39:47.980 --> 00:39:49.570
VOP calls inside of the code.

00:39:49.780 --> 00:39:52.160
So let me give you a walkthrough
of some of the VFS operations.

00:39:52.160 --> 00:39:54.910
Actually,
I think we have the full list of these.

00:39:55.160 --> 00:39:58.870
One quick note:
to instantiate a file system in BSD,

00:39:58.870 --> 00:40:01.200
and this is the model we've
inherited in Mac OS X,

00:40:01.270 --> 00:40:04.770
you first create a directory
and then you mount the device,

00:40:04.770 --> 00:40:08.240
which corresponds basically to
our petition on a hard disk drive

00:40:08.390 --> 00:40:11.700
or some protocol that alludes you
to a hard disk over on a server.

00:40:11.730 --> 00:40:14.970
You actually mount the device on
top of the existing directory.

00:40:15.030 --> 00:40:18.550
If the directory was not empty,
all the files and anything that was

00:40:18.550 --> 00:40:23.100
in that directory disappears from your
system until you unmount it again.

00:40:23.140 --> 00:40:25.200
So usually we do this
on empty directories.

00:40:25.250 --> 00:40:27.240
So you'll create a directory,
you'll mount the device

00:40:27.320 --> 00:40:29.400
on top of the directory,
and from that point on,

00:40:29.420 --> 00:40:33.360
that directory will be a window into
all of the files on that partition that

00:40:33.440 --> 00:40:35.600
you've mounted there in that place.

00:40:35.600 --> 00:40:39.700
VFS mount and VFS unmount are the
calls you use to manipulate that.

00:40:39.740 --> 00:40:42.700
So VFS mount is going to create
and fill in the root V node,

00:40:42.720 --> 00:40:46.900
the V node for the root directory that
you've just mounted on that directory,

00:40:46.960 --> 00:40:49.570
and it'll initialize the mount structure.

00:40:49.680 --> 00:40:55.200
VFS unmount is going to clean up the
mount structure and remove that device

00:40:55.200 --> 00:40:58.300
from that file system you create,
from the directory that you've created,

00:40:58.320 --> 00:41:00.280
and from that point on,
you'll no longer be able

00:41:00.280 --> 00:41:02.900
to access the files on that
volume or on that partition.

00:41:02.910 --> 00:41:05.120
But you will be able to access any
files that were left in that directory

00:41:05.200 --> 00:41:07.390
when you mount it on top of it.

00:41:07.640 --> 00:41:12.000
VFS Root's job is to give you back the
root V node given a mount structure.

00:41:12.000 --> 00:41:14.000
So if you're trying to
do path name processing,

00:41:14.000 --> 00:41:17.030
you're starting at a mount point,
this will get you to the root V node.

00:41:17.120 --> 00:41:20.340
StatFS is just a get statistics call.

00:41:20.370 --> 00:41:22.080
It's kind of like a get ball info.

00:41:22.080 --> 00:41:25.080
It is give me some information
about this overall volume,

00:41:25.170 --> 00:41:29.750
things like the overall size or
the amount of free space available,

00:41:29.750 --> 00:41:31.100
that sort of thing.

00:41:31.150 --> 00:41:34.100
VFS VGet, we won't get into detail,
but this is the call we use to

00:41:34.100 --> 00:41:35.740
support access by identifier.

00:41:35.890 --> 00:41:38.820
So if you want to implement a
file system that allows access by

00:41:38.910 --> 00:41:43.690
directory ID name the way HFS+ does,
then you'll need to support VFS VGet.

00:41:44.160 --> 00:41:47.540
VFS init is just your basic file
system initialization routine.

00:41:47.540 --> 00:41:49.860
You may or may not need
to support VFS init.

00:41:49.860 --> 00:41:53.370
It depends on whether or not you have
some global file system-wide data that

00:41:53.370 --> 00:41:56.710
you need initialized at mount time,
or I mean when your system

00:41:56.710 --> 00:41:57.910
is first initialized.

00:41:58.100 --> 00:42:03.100
VFS FH to VP, this stands for File Handle
to VNode Pointer,

00:42:03.100 --> 00:42:07.100
and VP to FH stands for
VNode Pointer to File Handle,

00:42:07.100 --> 00:42:11.000
and these calls are specifically for
supporting NFS access to your volume.

00:42:11.470 --> 00:42:14.810
These are the two key guys
for getting file handles sent

00:42:14.810 --> 00:42:17.100
across NFS for remote access.

00:42:17.100 --> 00:42:19.100
And sys control and quota apple,
we won't go into detail about those,

00:42:19.100 --> 00:42:22.560
but those are for supporting
system-level controls of your

00:42:22.670 --> 00:42:24.410
file system and for quotas.

00:42:25.560 --> 00:42:29.090
We're not going to give you an exhaustive
list of all of the VNode operations,

00:42:29.100 --> 00:42:31.360
but I will tell you about
some of the important ones.

00:42:31.390 --> 00:42:34.080
And these are the ones you'll
actually be tested on later when we

00:42:34.080 --> 00:42:35.890
talk about our caching architecture.

00:42:36.170 --> 00:42:37.240
So, VOP lookup.

00:42:37.270 --> 00:42:40.500
This is the most important and most
complicated of all the VNode operations.

00:42:40.520 --> 00:42:43.600
And its job is to handle
path name resolution.

00:42:43.660 --> 00:42:46.100
So basically,
you give it a path name that

00:42:46.160 --> 00:42:48.760
starts from a directory that
belongs to your file system.

00:42:48.820 --> 00:42:51.890
And you give it a string that
contains a bunch of path name

00:42:51.890 --> 00:42:53.660
elements separated by a slash.

00:42:53.690 --> 00:42:55.860
And then this routine will
give you back a VNode.

00:42:55.880 --> 00:42:58.850
And it's generally called iteratively as
we do the lookup process in the system.

00:42:58.900 --> 00:43:03.560
So, you'll get a component pointer,
you'll get a VNode that corresponds to

00:43:03.610 --> 00:43:05.290
a directory through your lookup call.

00:43:05.490 --> 00:43:07.700
You'll return the VNode for
the next thing in the path.

00:43:07.710 --> 00:43:10.900
You'll get called again if that's one
of yours and it will go on and on and on

00:43:10.920 --> 00:43:15.020
until lookup finally reaches the end of
the path and gives you back the VNode for

00:43:15.020 --> 00:43:17.360
the end file that you wished to target.

00:43:17.400 --> 00:43:20.310
Probably worth mentioning that
Mac OS X has a singly rooted

00:43:20.310 --> 00:43:23.640
hierarchical file system layout,
much like BSD.

00:43:23.710 --> 00:43:25.920
The Carbon interfaces will
abstract that for you.

00:43:25.920 --> 00:43:27.950
They'll know where all the
various mount points are.

00:43:28.050 --> 00:43:31.070
And you can ask the Carbon interfaces
for a mount list and work

00:43:31.070 --> 00:43:32.300
like you did in Mac OS 9.

00:43:32.610 --> 00:43:35.050
But in Mac OS X,
there will be actually a complete

00:43:35.050 --> 00:43:36.720
hierarchical file system list.

00:43:36.730 --> 00:43:38.190
And so BSD calls will work.

00:43:38.350 --> 00:43:40.790
And the Cocoa environment which
relies on that will also work.

00:43:40.850 --> 00:43:45.160
And so lookup becomes a very
central routine in Mac OS X.

00:43:45.210 --> 00:43:46.610
Open and close are what you're expecting.

00:43:46.620 --> 00:43:50.100
They're the two routines that enable
you to start doing access to a file

00:43:50.190 --> 00:43:52.460
and rip out that ability later.

00:43:52.490 --> 00:43:53.260
Read and write.

00:43:53.290 --> 00:43:53.660
Basic read and write.

00:43:53.660 --> 00:43:54.920
Read and write routines.

00:43:54.920 --> 00:43:58.080
Truncate, which is used to both
grow and shrink a file.

00:43:58.130 --> 00:43:59.620
The name is a little unfortunate.

00:43:59.650 --> 00:44:03.800
But you can truncate a file and send
its valid space way out to the end,

00:44:03.800 --> 00:44:04.760
move its EOF.

00:44:04.770 --> 00:44:08.170
Or you can also do a truncate
and reduce the size of a file,

00:44:08.270 --> 00:44:10.360
cut it all the way to zero if you like.

00:44:10.380 --> 00:44:11.990
Fsync is file synchronization.

00:44:12.050 --> 00:44:15.650
This is the call that takes all of the
dirty buffers you may have associated

00:44:15.660 --> 00:44:19.160
with your file in that system and
starts IO to the disk on those so that

00:44:19.240 --> 00:44:22.860
we can get all the data flushed that we
have cached in memory out to the disk.

00:44:22.890 --> 00:44:23.260
And you can make sure that
your file is in the disk.

00:44:23.310 --> 00:44:24.800
You can make sure that your
file data is consistent.

00:44:24.940 --> 00:44:27.820
It's a lot like flush files in Mac OS 9.

00:44:27.850 --> 00:44:30.800
And vop remove,
that's basically the delete call.

00:44:30.820 --> 00:44:33.460
As we mentioned before,
in order to give the full

00:44:33.460 --> 00:44:36.660
range of abilities of HFS,
HFS+, things traditionally

00:44:36.660 --> 00:44:40.100
associated with Mac OS,
we've extended the BSD interfaces to

00:44:40.110 --> 00:44:43.250
add things like get adder list and
set adder list that will get you to

00:44:43.250 --> 00:44:46.360
things like the catalog information,
allow you to do searching.

00:44:46.400 --> 00:44:48.430
Well,
all of those calls are also supported

00:44:48.430 --> 00:44:50.000
by underlying VNode operations.

00:44:50.000 --> 00:44:52.810
And so if you wanted to
implement the file system that's

00:44:52.860 --> 00:44:55.500
supported by the get adder list,
you implement vop get adder list the way

00:44:55.500 --> 00:45:00.310
we did in HFS+, and then those abilities
will be available to your callers.

00:45:01.260 --> 00:45:04.100
So now we're at the real hard stuff.

00:45:04.130 --> 00:45:06.800
We want to talk to you about
this big change that we've just

00:45:06.800 --> 00:45:09.200
made in our caching architecture.

00:45:09.200 --> 00:45:11.120
And anybody who's actually
thinking of implementing a file

00:45:11.120 --> 00:45:12.590
system will need to know this.

00:45:12.720 --> 00:45:13.990
And we'll also tell you
about why we did it.

00:45:14.100 --> 00:45:16.990
The changes in the caching
architecture were actually done by

00:45:16.990 --> 00:45:20.100
the BSD team with some assistance
from the file systems group.

00:45:20.100 --> 00:45:23.140
But this is something that spans
both file system development

00:45:23.500 --> 00:45:25.060
and the virtual memory system.

00:45:25.150 --> 00:45:25.900
And so we're fortunate.

00:45:25.900 --> 00:45:27.870
I'm going to talk a little bit about
the beginnings of this and then

00:45:27.870 --> 00:45:31.100
later I'm going to bring up Ramesh,
one of the BSD engineers,

00:45:31.100 --> 00:45:33.990
who will tell you the details
about how that system worked.

00:45:34.550 --> 00:45:38.400
This is a rough picture of
what our old cache looked like.

00:45:38.490 --> 00:45:42.800
And up on the top, top of your right,
that big box that says VFS there,

00:45:42.840 --> 00:45:45.560
that's the VFS interfaces we've
been talking to you about.

00:45:45.560 --> 00:45:49.260
And then that's our underlying support
for all of the different file systems.

00:45:49.300 --> 00:45:51.020
And this is,
I'm going to walk you through how

00:45:51.020 --> 00:45:53.660
the buffering used to work when
a read call would come through.

00:45:53.730 --> 00:45:57.030
Basically,
the overall BSD call read would come

00:45:57.230 --> 00:46:00.610
in and it would go to the VFS layer,
it would dispatch to the appropriate

00:46:00.610 --> 00:46:02.380
file system that was going to service it.

00:46:02.430 --> 00:46:05.940
And then that file system would most
likely call the buffer cache underneath.

00:46:05.990 --> 00:46:09.470
The buffer cache might have that data
already read in memory because you've

00:46:09.560 --> 00:46:12.890
read it before or because you were doing
sequential access and we anticipated

00:46:12.990 --> 00:46:16.860
that that page was going to be read and
we put it in memory for you in advance.

00:46:16.920 --> 00:46:18.940
And it will satisfy the request
out of the buffer cache.

00:46:18.940 --> 00:46:20.870
And if not,
then the buffer cache will call

00:46:20.950 --> 00:46:23.650
back to one of your routines to
actually read the data off of the

00:46:23.770 --> 00:46:25.310
device or get it off the network.

00:46:25.500 --> 00:46:27.630
So that's the basic,
that's the basic deal.

00:46:27.940 --> 00:46:30.090
Come in with a read,
go to the file system that supports it,

00:46:30.170 --> 00:46:32.190
talk to the buffer cache,
try to satisfy the page.

00:46:32.440 --> 00:46:35.160
And it generally works pretty
well and the buffer cache can

00:46:35.260 --> 00:46:36.960
be used to optimize performance.

00:46:37.060 --> 00:46:38.920
Well,
there's this other way of doing I/O in

00:46:38.920 --> 00:46:40.600
our system that we call file mapping.

00:46:40.770 --> 00:46:43.540
And some of you, if you've worked on BSD,
may be familiar with this.

00:46:43.570 --> 00:46:46.120
But the idea is you can open up
a file and then you can decide

00:46:46.490 --> 00:46:48.640
that a certain range of address,
by the way,

00:46:48.640 --> 00:46:50.780
I appreciate the applause over there.

00:46:51.150 --> 00:46:54.340
A certain range of address will
be backed by that particular file.

00:46:54.340 --> 00:46:56.970
So you'll say, "Hey,
I'd like you to make addresses

00:46:56.990 --> 00:47:01.100
1000 through 5000 valid for me,
and I want you to use as their backing

00:47:01.100 --> 00:47:04.710
store this particular file that I'm
telling you about." And then if you read

00:47:05.020 --> 00:47:09.040
anything in that array of addresses,
you'll get the data that's actually

00:47:09.040 --> 00:47:12.650
in that file that you've decided is
mapped into that array of address.

00:47:12.760 --> 00:47:16.060
So it's a very handy way of doing I/O,
but it does cause some interesting

00:47:16.060 --> 00:47:17.630
problems for file systems.

00:47:17.740 --> 00:47:21.040
The first one being the cache for the
virtual memory system is different

00:47:21.140 --> 00:47:24.350
from the cache for the file system,
or at least it was in

00:47:24.350 --> 00:47:26.200
earlier versions of Mac OS X.

00:47:26.280 --> 00:47:28.590
And so to walk you through this,
you have a process.

00:47:28.690 --> 00:47:32.070
He comes along, maps a range of data,
says he would like it backed

00:47:32.070 --> 00:47:35.070
by this particular file,
and then starts reading the memory.

00:47:35.170 --> 00:47:37.990
But what happens is the VM cache
will start doing I/O via new

00:47:38.120 --> 00:47:40.020
calls called Page In and Page Out.

00:47:40.090 --> 00:47:42.400
They'll eventually call
the file system underneath,

00:47:42.490 --> 00:47:44.240
they'll get the data,
and then they'll put

00:47:44.240 --> 00:47:45.310
it into the VM cache.

00:47:45.400 --> 00:47:47.520
And the VM cache will then
satisfy those requests.

00:47:47.660 --> 00:47:51.910
But what can also happen is one process
can open the file via read and write,

00:47:52.400 --> 00:47:55.610
another process can map the file,
and either of those processes

00:47:55.610 --> 00:47:56.780
can now write to it.

00:47:56.800 --> 00:47:59.750
The read and write process is
going to write to it via the

00:47:59.750 --> 00:48:02.840
normal file system read/write APIs,
and the data is going to be

00:48:02.840 --> 00:48:04.020
cached in the buffer cache.

00:48:04.040 --> 00:48:06.830
The mapped file is just going to
write into an array of memory,

00:48:06.850 --> 00:48:10.570
and it's going to believe
that that maps into that file,

00:48:10.570 --> 00:48:13.340
but in the meantime it's going to
be stored in the VM cache until

00:48:13.340 --> 00:48:14.850
the VM system decides to flush it.

00:48:14.920 --> 00:48:17.660
So if two processes
open up the same file,

00:48:17.660 --> 00:48:17.660
it's going to be cached
in the buffer cache.

00:48:17.730 --> 00:48:19.860
So if two processes open up the same
file via these different mechanisms,

00:48:19.920 --> 00:48:23.700
they can actually wind up with
inconsistent data that corresponds

00:48:23.700 --> 00:48:26.790
to the same address in the
file for that page of memory.

00:48:26.820 --> 00:48:28.900
That's kind of actually diagrammed here.

00:48:28.900 --> 00:48:32.310
You'll see a page of memory here in
our buffer cache that's supposed to

00:48:32.420 --> 00:48:35.880
correspond to a page of data in the file,
another page in the VM cache,

00:48:35.880 --> 00:48:39.380
and you can see how these
guys can become inconsistent.

00:48:39.680 --> 00:48:40.600
So that's the big problem.

00:48:40.600 --> 00:48:43.520
We have these two caches and the data
can become inconsistent in these two

00:48:43.520 --> 00:48:47.360
caches because ReadWrite is using
the file system buffer cache and the

00:48:47.360 --> 00:48:49.350
VM system is using a separate cache.

00:48:49.380 --> 00:48:51.540
And as long as these things
were totally distinct,

00:48:51.570 --> 00:48:53.300
you could get yourself into trouble.

00:48:53.370 --> 00:48:55.500
Now to try to avoid some
of those problems in DP3,

00:48:55.500 --> 00:48:58.830
there were a number of synchronization
points in the file system where we

00:48:58.830 --> 00:49:01.690
would check with the VM system to
see if it had data and try to get

00:49:01.690 --> 00:49:03.140
flushing to happen and so forth.

00:49:03.210 --> 00:49:05.500
But there were still some holes in
there and while it was unlikely,

00:49:05.500 --> 00:49:08.880
you might have stumbled across
some cases where your file system

00:49:08.920 --> 00:49:10.830
data looks sort of inconsistent.

00:49:11.620 --> 00:49:16.440
So what we've done-- let me tell
you what we did in Mac OS X Server.

00:49:16.440 --> 00:49:18.540
We recognized that this
was a potential problem,

00:49:18.540 --> 00:49:21.320
so we had a solution for this
issue in Mac OS X Server.

00:49:21.390 --> 00:49:24.700
What we did was we implemented
something called the map file system.

00:49:24.700 --> 00:49:28.560
And what it did was turn all
regular I/O into mapped I/O.

00:49:28.560 --> 00:49:31.860
And so the VM cache became supreme
in a Mac OS X Server system.

00:49:31.860 --> 00:49:33.400
You would do a read and
you would do a write,

00:49:33.530 --> 00:49:35.650
and actually ultimately it
would turn into a map call,

00:49:35.650 --> 00:49:38.950
and we would map it into a spot in
the kernel's address space as opposed

00:49:39.030 --> 00:49:40.680
to the process's address space.

00:49:40.920 --> 00:49:43.520
Well, that worked pretty well,
and it did get rid of the

00:49:43.520 --> 00:49:47.200
potential data inconsistencies,
but the buffer cache was a fixed

00:49:47.250 --> 00:49:49.120
size and rather tiny underneath.

00:49:49.120 --> 00:49:50.300
That was a bit of an issue.

00:49:50.300 --> 00:49:51.990
The VM system, again,
was reigning supreme,

00:49:51.990 --> 00:49:55.000
and so some of that read ahead
stuff might not be happening.

00:49:55.000 --> 00:49:57.160
There still were two copies
of data in the system.

00:49:57.160 --> 00:50:00.400
Now while the VM cache was sort
of superimposed on the buffer

00:50:00.400 --> 00:50:03.110
cache so that you would never
get the data inconsistent,

00:50:03.120 --> 00:50:05.950
you could still replicate the data,
because the VM system would ask the file

00:50:05.950 --> 00:50:09.580
system-- the file system would get out of
the buffer cache and it would go back up.

00:50:09.580 --> 00:50:09.860
They were never different data.

00:50:09.860 --> 00:50:11.760
for different data,
but there were still two copies,

00:50:11.760 --> 00:50:13.570
and that was inefficient.

00:50:13.980 --> 00:50:16.140
Only part of the file could
actually be mapped in the kernel.

00:50:16.140 --> 00:50:18.300
We don't have the full range
of process address spaces.

00:50:18.300 --> 00:50:20.560
We have one kernel address space.

00:50:20.560 --> 00:50:23.530
And so that caused us to have to do
some funny things like potentially

00:50:23.660 --> 00:50:26.800
throw out mappings for different
processes as new processes would come

00:50:26.800 --> 00:50:30.710
and go and wish to do I/O because
we only had a limited address space.

00:50:30.830 --> 00:50:35.190
That meant that not all currently
mapped files could necessarily be

00:50:35.190 --> 00:50:39.350
mapped at the same time and you
might not be able to map the entire

00:50:39.450 --> 00:50:42.040
file at once because we had to deal
with this limited address space.

00:50:42.140 --> 00:50:44.600
So we're throwing mappings in and
out of the kernel all the time

00:50:44.600 --> 00:50:46.440
causing potential performance issues.

00:50:46.510 --> 00:50:49.720
And it also wasn't very friendly to
the file system because reads and

00:50:49.720 --> 00:50:52.440
writes would come in and the file
system wouldn't get them right away.

00:50:52.440 --> 00:50:54.580
They would be redirected
to the VM system.

00:50:54.630 --> 00:50:57.520
The VM system had its caching going
and it would only call the file

00:50:57.520 --> 00:51:00.440
system when it decided to flush
its cache or bring in new data.

00:51:00.540 --> 00:51:03.240
And so if you wanted to have a counter
in your file system of all the reads

00:51:03.300 --> 00:51:06.190
and writes that were taking place,
you wouldn't have them at

00:51:06.190 --> 00:51:07.500
the time the user asked.

00:51:07.670 --> 00:51:10.480
You'd only see them when the
VM system was ready to communicate.

00:51:10.640 --> 00:51:12.040
So that was an issue too.

00:51:12.040 --> 00:51:15.160
So what we've done is we've
just unified these caches.

00:51:15.240 --> 00:51:18.830
Sounds simple, it was a lot of work,
but it's into DP4 and so we

00:51:18.830 --> 00:51:20.750
got rid of all of these issues.

00:51:20.830 --> 00:51:25.400
And now there's just one cache for both
the VM system and the file system and all

00:51:25.400 --> 00:51:29.570
the pages that are associated with the
file are satisfied out of the same cache.

00:51:29.640 --> 00:51:31.470
So to talk about some
of the details of that,

00:51:31.510 --> 00:51:34.640
we've brought up Ramesh,
who's one of the BSD engineers,

00:51:34.680 --> 00:51:37.250
and he'll walk you through
some of the implementation

00:51:37.250 --> 00:51:38.890
of the unified buffer cache.

00:51:45.290 --> 00:51:46.490
Hi, good afternoon.

00:51:46.500 --> 00:51:50.380
My name is Ramesh.

00:51:50.520 --> 00:51:53.180
Basically,
as you saw that Clark mentioned that we

00:51:53.180 --> 00:51:55.420
had two kinds of caches in the system.

00:51:55.450 --> 00:51:58.340
When we had the VM page cache,
which was a big thing,

00:51:58.340 --> 00:52:02.380
and we had a tiny buffer cache,
which is much smaller,

00:52:02.380 --> 00:52:06.120
which we were using to buffer cache,
we had a data inconsistency.

00:52:07.130 --> 00:52:10.620
The way we used to construct
buffer cache was when a

00:52:10.970 --> 00:52:16.570
Mac OS machine boots very early on,
the secondary loader then transfers

00:52:16.570 --> 00:52:18.100
the control to the kernel.

00:52:18.250 --> 00:52:23.760
When the kernel comes up,
initially it has a pool of pages.

00:52:24.000 --> 00:52:27.640
Actually, this pool consists of all the
physical pages in the system except

00:52:28.300 --> 00:52:32.600
for the pages that are consumed
by the kernel text and data itself.

00:52:32.660 --> 00:52:37.140
What we used to do is carve out
a small portion of the pages from

00:52:37.220 --> 00:52:42.510
there and then reserve them to
use exclusively for buffer cache.

00:52:42.770 --> 00:52:49.540
And this is where the fixed size of the
buffer cache is coming because we used

00:52:49.540 --> 00:52:54.750
to take a number of pages and we were
trying to use that for the buffer cache.

00:52:54.970 --> 00:53:00.170
As Clark mentioned before,
we have one huge honking VM page

00:53:00.170 --> 00:53:04.300
cache on one side and we had a
small buffer cache caching the two.

00:53:04.360 --> 00:53:06.950
What we did basically was

00:53:08.320 --> 00:53:13.790
To stop using the buffer
cache for the file data,

00:53:13.900 --> 00:53:19.710
what we did instead was to turn all
the file data to the VM cache itself.

00:53:20.740 --> 00:53:26.350
Now that we turn all the file
pages into the VM page cache,

00:53:26.400 --> 00:53:32.110
we had to implement a mechanism to
track all the pages corresponding to

00:53:32.180 --> 00:53:36.800
a particular file or a single page
corresponding to a particular file or a

00:53:36.890 --> 00:53:40.090
particular offset in the VM page cache.

00:53:40.190 --> 00:53:43.680
For this, at Apple,
we implemented a mechanism

00:53:43.680 --> 00:53:46.150
called Universal Page Lists.

00:53:46.260 --> 00:53:51.600
What Universal Page Lists does
for us is to get at a page,

00:53:51.880 --> 00:53:54.260
a specific page in the system.

00:53:54.370 --> 00:54:01.060
Universal Page Lists is a mechanism
to get at a page in a memory object.

00:54:01.150 --> 00:54:05.630
And it actually also allows us
the physical access to the page

00:54:05.960 --> 00:54:09.520
so that we can use that to do
DMA directly without ever mapping

00:54:09.520 --> 00:54:11.420
inside the kernel address space.

00:54:11.530 --> 00:54:15.020
If you want to know more
about the page lists,

00:54:15.100 --> 00:54:16.160
the kernel session on the
Mac OS X is a very simple process.

00:54:16.160 --> 00:54:21.600
On 106 on Thursday morning, 9:00,
they will discuss about this.

00:54:21.600 --> 00:54:25.960
If you want to know more about this,
they will talk about it.

00:54:25.960 --> 00:54:31.560
What we did was to use these UPLs,
Universal Page Lists,

00:54:31.560 --> 00:54:38.120
and develop an infrastructure
underneath the buffer cache APIs.

00:54:38.130 --> 00:54:42.610
The buffer cache APIs were like B-read,
B-write, get block, B-rails,

00:54:42.610 --> 00:54:44.160
and things like that.

00:54:44.260 --> 00:54:47.840
So not all the colors of the-- The colors
of the buffer cache APIs never have to

00:54:47.940 --> 00:54:50.260
change because we maintain the same APIs.

00:54:50.260 --> 00:54:56.260
We changed all the plumbing underneath
Thursday to use unified buffer cache.

00:54:56.310 --> 00:54:59.650
We also wrote a set of routines
that we need to sprinkle all along

00:54:59.940 --> 00:55:03.860
the different file systems to work
with the unified buffer cache.

00:55:03.880 --> 00:55:09.060
The best way for me to explain all
this is to take different scenarios

00:55:09.060 --> 00:55:15.090
in a lifecycle of a file and then see
how unified buffer cache works there.

00:55:18.870 --> 00:55:22.170
First, let me take up the open.

00:55:22.600 --> 00:55:26.920
Let's assume that we were
opening a file in HFS volume,

00:55:26.950 --> 00:55:30.220
so that all the vNode operations that
I'm going to be talking about are

00:55:30.220 --> 00:55:33.560
basically HFS plus vNode operations.

00:55:33.560 --> 00:55:36.930
And let's also assume that
we're opening the file for

00:55:36.940 --> 00:55:39.080
the first time in the system.

00:55:39.700 --> 00:55:48.530
As part of the open system called
implementation in the kernel,

00:55:48.550 --> 00:55:51.430
the first thing it tries to do,
as Clark mentioned before,

00:55:51.540 --> 00:55:56.710
is to convert the path name from
path name to a file representation

00:55:56.710 --> 00:55:58.440
inside the file system.

00:55:58.540 --> 00:56:06.350
So basically that, for our example,
is going to be HFS lookup.

00:56:06.690 --> 00:56:10.440
This HFS lookup is a vNodeOperation
lookup vNodeOperation HFS.

00:56:10.470 --> 00:56:12.680
One of the things that it will
do is going to see whether

00:56:12.760 --> 00:56:14.980
the file already exists.

00:56:15.620 --> 00:56:19.120
And see whether it needs to
create a file if you pass the

00:56:19.120 --> 00:56:22.270
correct flags for the open.

00:56:22.660 --> 00:56:26.560
And then, what it does,
whether the file exists or not,

00:56:26.680 --> 00:56:29.470
whether it created or the
file was already there,

00:56:29.470 --> 00:56:31.170
it creates a VNode.

00:56:31.670 --> 00:56:35.500
It creates a VNode for
that particular file.

00:56:35.680 --> 00:56:39.280
All future references to the file,
all manipulations to the file from

00:56:39.280 --> 00:56:42.730
now onwards goes through this VNode.

00:56:43.340 --> 00:56:48.790
In the lookup,
we added our first support routines

00:56:48.870 --> 00:56:51.900
that was talking about file system
support routines in the unified

00:56:52.330 --> 00:56:54.620
buffer cache called UBC-Infinite.

00:56:54.990 --> 00:57:00.540
What this UBC info does basically
is create a memory object and

00:57:00.550 --> 00:57:03.940
create an association between
that memory object we just created

00:57:04.220 --> 00:57:06.140
and the vNode we just created.

00:57:06.220 --> 00:57:08.080
It's a one-to-one association.

00:57:08.200 --> 00:57:12.550
For example, if we're looking at a 2000
offset inside this vNode,

00:57:12.550 --> 00:57:19.560
it translates to a 2000 offset inside
the memory object that we created.

00:57:19.630 --> 00:57:22.540
Now let me take another scenario where

00:57:22.890 --> 00:57:27.750
We're just going to map the file.

00:57:27.850 --> 00:57:33.890
Let's say we take first 32K of the file
and we map in the current address space.

00:57:34.000 --> 00:57:36.530
Suppose a user,
this usually basically comes to

00:57:36.620 --> 00:57:39.460
the kernel as an M-Map system call.

00:57:39.570 --> 00:57:42.890
What we do in the M-Map system
call is basically set up a

00:57:42.890 --> 00:57:49.040
pager and then allocate a 32K
range because my example is 32K.

00:57:49.130 --> 00:57:57.100
We set a 32K address range in the user
process for this particular file mapping.

00:57:57.210 --> 00:58:01.120
And then we return the address
of this where we allocated in

00:58:01.120 --> 00:58:06.370
the user space back to the user
through the M-Map system call.

00:58:06.500 --> 00:58:11.090
Note that we haven't done any page I/O,
any file I/O, nothing yet.

00:58:11.440 --> 00:58:13.370
Nothing has happened yet.

00:58:13.460 --> 00:58:17.780
When the user first
accesses this address space,

00:58:17.870 --> 00:58:22.310
let's take it as, you know, for example,
he's reading the first byte

00:58:22.530 --> 00:58:24.810
of this allocated range.

00:58:25.820 --> 00:58:28.090
What happens at that point of time,
since there's no page

00:58:28.300 --> 00:58:32.700
backing this address range,
it's going to generate a page fault.

00:58:32.750 --> 00:58:36.120
The VM system goes and tries
to resolve the page fault.

00:58:36.310 --> 00:58:40.330
The page is not there in the system,
so then it allocates a clean page

00:58:40.690 --> 00:58:46.360
and then it tries to fill the data
from the file system for this page.

00:58:46.600 --> 00:58:49.800
In our case,
since we're taking an example of HFS+,

00:58:49.850 --> 00:58:54.390
it ultimately goes to the VNodePager
and falls into HFS Paging Call,

00:58:54.390 --> 00:58:58.580
which's responsibility is
to copy the data from the

00:58:58.580 --> 00:59:04.670
disk to this particular page,
which VM gave us, and then it returns the

00:59:04.670 --> 00:59:04.670
page back to the VM.

00:59:05.030 --> 00:59:10.160
And this page stays in the VM page
cache and all future modifications that

00:59:10.230 --> 00:59:15.750
users would play with this particular
page would happen without any knowledge

00:59:15.750 --> 00:59:18.370
in the file system or anywhere else.

00:59:20.000 --> 00:59:22.850
Let's say for our example
we access some more pages.

00:59:22.970 --> 00:59:25.500
I just put like sparse,
it's not always contiguous,

00:59:25.500 --> 00:59:32.150
so 0 to 12K and then 20 to 32K
range of files we have touched.

00:59:32.960 --> 00:59:37.100
Then, now I'm going to take a scenario
where we just want to do a write.

00:59:37.100 --> 00:59:40.230
On, at an offset 15,
just to make it a little

00:59:40.250 --> 00:59:42.490
bit more interesting,
like offset 15,

00:59:42.490 --> 00:59:46.490
and then we'll just write only
10 bytes of data into the,

00:59:46.490 --> 00:59:47.290
we want it to the file.

00:59:47.300 --> 00:59:50.300
So we use a write system call to do that.

00:59:50.320 --> 00:59:53.970
As part of the write system call,
implementations are the kernel,

00:59:54.030 --> 00:59:55.800
which will then translate
to the particular BNode

00:59:55.800 --> 01:00:01.060
operation for that file system,
which happens to be HFS write for us.

01:00:01.070 --> 01:00:05.240
What we use,
we then use a universal page list

01:00:05.240 --> 01:00:10.880
routine called VMFault List Request,
and we ask the VMFault List Request

01:00:10.980 --> 01:00:15.460
to return us the page to
starting at an offset zero,

01:00:15.460 --> 01:00:19.290
and the size which is
equal to the page size.

01:00:19.560 --> 01:00:23.150
What it does is it gives us the
exclusive access to that page which

01:00:23.150 --> 01:00:25.570
was residing in the VM page cache.

01:00:26.710 --> 01:00:29.840
Once the page has been obtained,
we then copy the data from the user

01:00:29.840 --> 01:00:33.570
buffer which he had to this page.

01:00:33.800 --> 01:00:37.230
Once we copy the data
back into the system,

01:00:37.340 --> 01:00:39.760
then depending on what
file systems want to,

01:00:39.810 --> 01:00:42.810
they either can write up
the data immediately to

01:00:42.810 --> 01:00:46.960
the disk or they can delay,
postpone writing to the disk.

01:00:47.000 --> 01:00:50.970
In HFS we actually delay the writes.

01:00:51.410 --> 01:00:55.810
Then, all we do then is basically
written the page back to the

01:00:55.810 --> 01:01:01.620
VM page cache that we used to by
using a kernel UPL commit call,

01:01:01.620 --> 01:01:05.200
which is also coming from
the universal page list.

01:01:05.270 --> 01:01:10.080
So basically,
we did a copy of 10 bars into the system,

01:01:10.410 --> 01:01:15.070
which the user who has mapped this
can also see the modifications

01:01:15.080 --> 01:01:16.700
that were already done.

01:01:17.280 --> 01:01:19.740
Okay, let's say I just took
another example where he just

01:01:20.050 --> 01:01:22.100
dirtied another page as well.

01:01:22.180 --> 01:01:27.160
I'm going to now move to take another
scenario where we're going to talk

01:01:27.300 --> 01:01:30.990
about the WinRAR operation called Fsync,
WAP Fsync.

01:01:31.080 --> 01:01:33.840
In our example,
it's going to be HFS Fsync.

01:01:33.890 --> 01:01:35.750
Basically, the

01:01:36.060 --> 01:01:41.720
HFS Fsync's responsibility is to
push all the modified data from

01:01:41.720 --> 01:01:45.940
the buffers back to the disk.

01:01:46.030 --> 01:01:49.650
Since we have modified
data in the VM page cache,

01:01:49.650 --> 01:01:53.060
we have a new routine which
the Fsync also has to do,

01:01:53.060 --> 01:01:58.240
apart from whatever it used to do before,
called UBC Push30.

01:01:58.370 --> 01:02:02.370
What UBC Push30 does is goes and
looks at all the modified data

01:02:02.530 --> 01:02:06.350
corresponding to this particular
file that was there in the VM page

01:02:06.350 --> 01:02:08.270
cache and writes it back to the disk.

01:02:08.470 --> 01:02:12.570
That's how all the data
goes to the system.

01:02:12.680 --> 01:02:17.490
I forgot to mention that this Fsync,
you know, operation can come in two ways.

01:02:17.590 --> 01:02:23.210
Once the user program can call a Fsync
system call to make sure that all the

01:02:23.320 --> 01:02:25.840
modified data is pushed back to the disk.

01:02:25.930 --> 01:02:29.950
Or it can also come from the
system which periodically runs

01:02:29.950 --> 01:02:32.710
a system called called sync,
which goes through and looks at all

01:02:32.720 --> 01:02:36.440
the modified data in the system for
all files and pushes them to disk.

01:02:36.540 --> 01:02:39.640
So even if you don't
explicitly call a Fsync,

01:02:39.760 --> 01:02:42.550
the data is synced back to the disk.

01:02:43.200 --> 01:02:45.320
Now I'm going to take an
example of the truncate.

01:02:45.360 --> 01:02:49.170
We just want to truncate
the file back to 16K.

01:02:49.740 --> 01:02:54.360
Basically what I mean is the logical
end of the file is set to 16K.

01:02:54.390 --> 01:02:59.420
So we have another support
routine called UBC set size.

01:02:59.670 --> 01:03:04.540
Basically what it does is it invalidates
all the pages in the VM page cache which

01:03:04.560 --> 01:03:08.960
are beyond the logical end of the file.

01:03:09.530 --> 01:03:14.600
So as you can see in this diagram,
all the pages that were marked 20K, 24K,

01:03:14.630 --> 01:03:16.100
38K, they're all gone.

01:03:16.160 --> 01:03:20.800
They're all invalidated,
so they don't exist in the VM page cache.

01:03:21.960 --> 01:03:24.950
Now, I'm going to go to the last
scenario that I want to talk

01:03:24.950 --> 01:03:27.940
about where the file gets deleted.

01:03:28.020 --> 01:03:30.840
So basically,
everything that we set up for so far,

01:03:30.960 --> 01:03:32.910
we just have to tear everything down.

01:03:33.000 --> 01:03:35.260
So we do that in three steps.

01:03:35.390 --> 01:03:40.200
First, we call the UBC set size routine,
which I mentioned as

01:03:40.200 --> 01:03:45.720
part of the truncate,
which basically takes away all the pages

01:03:45.840 --> 01:03:48.510
that were there in the VM page cache.

01:03:48.640 --> 01:03:52.030
Basically, we set it to zero as the
logical end of the size.

01:03:52.150 --> 01:03:56.300
And then we also call another
function called UBC release,

01:03:56.350 --> 01:04:01.310
which basically disassociates the memory
object association with the VNode.

01:04:01.390 --> 01:04:05.400
And then we also need to call
another routine called UBC NCache,

01:04:05.450 --> 01:04:09.360
which basically tells VM systems
don't ever cache that memory object,

01:04:09.430 --> 01:04:13.800
because VM does cache all the
memory objects that it has.

01:04:14.480 --> 01:04:18.890
So this basically gives us a whole
lot of overview like how we have

01:04:19.050 --> 01:04:23.850
implemented the unified buffer cache
and what kind of set of proteins

01:04:23.850 --> 01:04:27.580
that we did and things like that.

01:04:27.670 --> 01:04:33.210
Now I'm just going to talk next two
slides about like what each file system

01:04:33.470 --> 01:04:36.750
has to do and what kind of changes
each file system had to go through

01:04:37.150 --> 01:04:40.140
to work with unified buffer cache.

01:04:40.320 --> 01:04:43.110
While implementing the
unified buffer cache,

01:04:43.110 --> 01:04:45.650
we introduced five new vNode operations.

01:04:45.960 --> 01:04:48.980
Actually two of them are already there,
three of them are new.

01:04:49.060 --> 01:04:51.730
And in this file, four are mandatory.

01:04:51.810 --> 01:04:57.240
You need to have them to work with
UBC and the last one is optional.

01:04:57.380 --> 01:05:01.440
The first part is a vNode
operation called block to offset.

01:05:01.520 --> 01:05:04.470
What it basically does
is given a logical block,

01:05:04.500 --> 01:05:08.030
it converts and gives
us back the file offset.

01:05:08.150 --> 01:05:10.140
If you know already that all the
buffer cache is in the file set,

01:05:10.190 --> 01:05:14.140
buffer cache routines like get block,
bread, everything works in

01:05:14.140 --> 01:05:15.410
logical block numbers.

01:05:15.660 --> 01:05:21.340
You tell bread, logical number, blah,
and then it returns the data back.

01:05:21.400 --> 01:05:24.880
So since all the memory
objects work with the offsets,

01:05:24.960 --> 01:05:29.900
this basically tells us what the offset
conversion for that block number is.

01:05:29.970 --> 01:05:35.610
Because every file system can
have their own logical block size.

01:05:35.900 --> 01:05:38.540
and the next one,
it does exactly opposite.

01:05:38.690 --> 01:05:43.230
It actually translates from the
offset to a logical block number,

01:05:43.230 --> 01:05:50.370
which we use it in the page to shoot
any buffers that are there in the cache.

01:05:50.510 --> 01:05:57.560
We also have two next two on operations,
page and page.

01:05:57.650 --> 01:06:00.240
They actually existed before
Unified Buffer Cache 2.

01:06:00.310 --> 01:06:05.160
There's slight differences in there,
like we passed Unified Page List to that,

01:06:05.230 --> 01:06:07.460
Universal Page List to that,
other than that there's

01:06:07.460 --> 01:06:09.220
nothing changed there.

01:06:09.280 --> 01:06:13.930
And then the last one is used
for the file system clustering.

01:06:14.140 --> 01:06:15.800
It's called Cmap.

01:06:15.830 --> 01:06:17.320
It's very similar to BMap.

01:06:17.360 --> 01:06:21.690
What the BMap feed-on operation does
is given a logical block number,

01:06:21.780 --> 01:06:25.400
it goes and computes the
actual disk block number so

01:06:25.400 --> 01:06:27.460
that we can do an I/O on it.

01:06:27.550 --> 01:06:30.710
What Cmap does is,

01:06:30.740 --> 01:06:34.320
It takes a file offset instead
of the logical block number,

01:06:34.390 --> 01:06:40.940
and it also returns the disk block number
corresponding so that we can do an I/O.

01:06:40.970 --> 01:06:45.060
And it also tells us how much
continuous blocks are there from

01:06:45.130 --> 01:06:48.540
that disk block onward so that
we can do a big chunk of I/O.

01:06:48.690 --> 01:06:53.480
If anybody wants to use
file system clustering,

01:06:53.480 --> 01:06:53.480
then they need to implement that too.

01:06:53.970 --> 01:06:56.750
And as I already mentioned
in all my examples,

01:06:56.750 --> 01:07:00.330
like the all file systems
have to sprinkle these five,

01:07:00.410 --> 01:07:06.830
one, two, three, four, five, yes,
five support routines that we wrote.

01:07:06.960 --> 01:07:11.060
So for example, just to recap them,
UBC in point eight,

01:07:11.130 --> 01:07:13.900
we need to insert them
whenever we create a VNode.

01:07:13.980 --> 01:07:18.900
And then we need to call UBC push dirty,
which basically whenever you want

01:07:18.900 --> 01:07:22.640
to send the modified data back
from the VM page cache to the disk,

01:07:22.770 --> 01:07:23.900
you need to implement that.

01:07:23.900 --> 01:07:29.900
And then you need to call UBC set
size whenever your file size changes.

01:07:29.920 --> 01:07:33.300
The file size can change, for example,
when you're growing a

01:07:33.380 --> 01:07:35.870
file by write system call,
it changes the size.

01:07:35.900 --> 01:07:37.870
That's the time when
you need to call them.

01:07:37.900 --> 01:07:43.640
And also when you're shrinking the file,
we're using a truncate or making

01:07:43.710 --> 01:07:45.770
a truncate to the far end.

01:07:46.060 --> 01:07:48.900
In any case,
you need to call UBC set size.

01:07:49.150 --> 01:07:53.900
And then when you actually delete a file,
most of the file system actually does,

01:07:54.050 --> 01:07:57.760
but they don't do a truncate because they
need to get rid of all the disk blocks

01:07:57.850 --> 01:07:59.890
that are allocated and things like that.

01:07:59.900 --> 01:08:04.900
If you already don't do that,
then you need to do a UBC set size.

01:08:04.900 --> 01:08:08.880
Otherwise, you need to do UBC release
and UBC uncache.

01:08:08.900 --> 01:08:15.180
We're going to be having a -- we're
going to be putting a document saying

01:08:15.180 --> 01:08:17.600
how to write file systems to UBC.

01:08:17.920 --> 01:08:19.860
It will be there in
Darwin sometime in the near future.

01:08:20.090 --> 01:08:21.900
That's a big resource.

01:08:21.900 --> 01:08:24.530
And all the code for the unified
buffer cache and all the file

01:08:24.550 --> 01:08:25.900
systems are there on the system.

01:08:25.900 --> 01:08:29.200
It's already in the Darwin,
and it's already there

01:08:29.280 --> 01:08:30.740
in the Darwin server now.

01:08:31.020 --> 01:08:35.040
If you have any specific questions on
which corresponds to your file system,

01:08:35.270 --> 01:08:36.780
then we can talk about it.

01:08:36.970 --> 01:08:37.900
Come see me.

01:08:37.900 --> 01:08:39.890
We can talk about it in the end.

01:08:40.070 --> 01:08:42.890
Now I'll get Clark back on the stage.

01:08:44.900 --> 01:08:45.900
Thanks.

01:08:45.900 --> 01:08:49.900
Thanks, Ramesh.

01:08:49.900 --> 01:08:51.540
All right, so that's it.

01:08:51.540 --> 01:08:53.800
The only key thing left to tell
you about is that the next new

01:08:54.080 --> 01:08:55.810
file system feature could be yours.

01:08:55.960 --> 01:08:56.900
This is our Darwin slide.

01:08:56.900 --> 01:09:04.860
If you're really interested in the
stuff we've talked about with UBC,

01:09:04.860 --> 01:09:05.120
VNodeOps, and so forth, check out Darwin.

01:09:05.120 --> 01:09:05.120
Take a look.

01:09:05.120 --> 01:09:05.120
Take our code, please.

01:09:07.300 --> 01:09:08.490
some of the new things,
some of the ideas we've

01:09:08.520 --> 01:09:09.580
just sort of tossed around.

01:09:09.580 --> 01:09:12.150
An NTFS implementation,
because we haven't gotten to it yet.

01:09:12.290 --> 01:09:15.620
ProDOS, the stuff that used to ship
with Apple II machines.

01:09:15.620 --> 01:09:18.330
We don't have a logical volume manager,
so Venom might be an

01:09:18.330 --> 01:09:19.500
interesting thing to port.

01:09:19.500 --> 01:09:21.860
We don't have a Linux file
system support yet,

01:09:21.860 --> 01:09:23.910
so EX2FS might be an
interesting thing to port.

01:09:23.920 --> 01:09:25.640
And, you know, of course,
there's always URFS,

01:09:25.680 --> 01:09:27.060
something we haven't thought of yet.

01:09:27.100 --> 01:09:30.460
Some stuff we'd like to
have you take away with you.

01:09:30.480 --> 01:09:32.520
Choose your interface carefully.

01:09:32.520 --> 01:09:35.330
Remember that there are some
subtle differences that happen

01:09:35.330 --> 01:09:38.350
because of the volume format,
and there are some subtle differences in

01:09:38.580 --> 01:09:40.100
what the interface is doing to not allow.

01:09:40.100 --> 01:09:42.350
If you're doing a new
object-oriented application,

01:09:42.350 --> 01:09:43.360
you want to use Cocoa.

01:09:43.360 --> 01:09:44.850
If you need access to
the resource for it,

01:09:44.850 --> 01:09:46.400
you may need a new Carbon, et cetera.

01:09:46.400 --> 01:09:50.460
Be aware of those behavior differences
that Pat talked about between

01:09:50.460 --> 01:09:53.800
Mac OS X and Mac OS 9 and also among
the different volume formats we support.

01:09:53.800 --> 01:09:58.760
If you do want to implement a new
file system or a file system stack,

01:09:58.910 --> 01:10:01.780
be sure it's necessary because you are,
as Dean Reese mentioned

01:10:01.780 --> 01:10:03.700
in the IOCAD session,
going to be in the kernel,

01:10:03.820 --> 01:10:06.220
and if there's something that goes wrong,
you'll crash the whole system.

01:10:06.670 --> 01:10:07.560
Be sure you need to.

01:10:07.560 --> 01:10:10.100
PhoneFS is probably not the
best way to do modem access,

01:10:10.230 --> 01:10:10.900
for example.

01:10:10.900 --> 01:10:14.340
And be aware of the unified buffer
cache because it has changed

01:10:14.340 --> 01:10:16.710
the way file systems interact
with caching in the system.

01:10:16.720 --> 01:10:19.060
And finally,
make sure you use Darwin as a resource.

01:10:19.060 --> 01:10:21.580
And I'll mention now,
even before the roadmap slide,

01:10:21.580 --> 01:10:24.780
that today at 6.30 in Hall J1,
there'll be a birds of a

01:10:24.880 --> 01:10:27.120
feather session for the folks
that are interested in Darwin.

01:10:27.120 --> 01:10:29.990
And if you have questions about Darwin,
you'll want to come to that Hall J1,

01:10:30.050 --> 01:10:31.130
6.30 tonight.

01:10:31.140 --> 01:10:33.910
Other sessions you
might be interested in,

01:10:33.910 --> 01:10:37.000
there is a Mac OS X kernel
session tomorrow at 9

01:10:37.100 --> 01:10:38.860
o'clock in the Civic Center.

01:10:38.860 --> 01:10:40.980
And if you're curious
about universal page list,

01:10:41.070 --> 01:10:42.860
there'll be some talk about that there.

01:10:42.860 --> 01:10:47.060
There's also a Mac OS X application
packaging and document typing session

01:10:47.060 --> 01:10:49.820
that may be important to go to if you
want to find out about how we're doing,

01:10:49.820 --> 01:10:54.520
like, mapping launch bindings and icon
decisions and stuff like that.

01:10:54.600 --> 01:10:57.100
That's going to be tomorrow
at 2 here in the big hall.

01:10:57.100 --> 01:11:01.600
Mac OS X BSD support, it's going to be in
Hall A2 Friday at 3.30,

01:11:01.600 --> 01:11:04.600
a crucial session if you're curious
about the BSD infrastructure

01:11:04.920 --> 01:11:05.940
inside of Mac OS X.

01:11:05.940 --> 01:11:06.020
Thank you.

01:11:06.060 --> 01:11:06.090
Thank you.

01:11:06.120 --> 01:11:06.220
Thank you.

01:11:06.430 --> 01:11:08.540
And the feedback form
for Mac OS X overall,

01:11:08.540 --> 01:11:10.800
I'll be there as well as all
the other Core OS managers.

01:11:10.800 --> 01:11:14.030
That'll be in Room J1
Friday at 2 o'clock p.m.

01:11:14.080 --> 01:11:17.830
If you have questions,
you want to get in contact with us about,

01:11:17.900 --> 01:11:20.900
you know, closer relationships or get
on a mailing list or whatever,

01:11:20.900 --> 01:11:23.800
give a shout to John Signa at
Cigna at Apple.com.

01:11:23.800 --> 01:11:26.560
He's the technology manager
for Mac OS X in Apple Worldwide

01:11:26.560 --> 01:11:27.830
Developer Relations.

01:11:27.840 --> 01:11:31.110
I see we only have a couple minutes left,
and so we're not going to be able

01:11:31.160 --> 01:11:33.910
to do questions here on the stage,
but my team will be in the back of the

01:11:33.910 --> 01:11:35.680
hall over there if you want to come by.

01:11:36.050 --> 01:11:37.950
And ask us questions,
and we'll also be at the Birds,

01:11:38.070 --> 01:11:41.160
I will also be at the Birds of a Feather,
and I'll also be on campus tomorrow

01:11:41.230 --> 01:11:43.730
from 7 to 9.30 for the big campus event.

01:11:43.740 --> 01:11:45.420
So thanks a lot for your time.

01:11:45.420 --> 01:11:46.220
We appreciate your coming.

01:11:46.220 --> 01:11:46.300
Thank you.